{
    "Name": "Paul Iusztin",
    "Posts": {
        "Post_0": {
            "text": "When 𝗰𝗼𝗺𝗽𝗮𝗿𝗶𝗻𝗴 𝟭𝟬𝟬+ 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗲𝘅𝗽𝗲𝗿𝗶𝗺𝗲𝗻𝘁𝘀, I often got 𝗼𝘃𝗲𝗿𝘄𝗵𝗲𝗹𝗺𝗲𝗱 by which one to 𝗽𝗶𝗰𝗸.\nUntil I started 𝘂𝘀𝗶𝗻𝗴 these 𝟯 𝘀𝗶𝗺𝗽𝗹𝗲 𝘁𝗿𝗶𝗰𝗸𝘀 ↓\n#𝟭. 𝗕𝗮𝘀𝗲 𝗠𝗼𝗱𝗲𝗹\nYou need a reference point to compare your model results with.\nOtherwise, the computed metrics are hard to interpret.\n-> For example, you are training a time series forecaster.\nThe base model always predicts the last value. If your \"smart\" model can't outperform that, you are better off without it.\n#𝟮. 𝗦𝗹𝗶𝗰𝗶𝗻𝗴\nAggregated metrics are often misleading (the mean over all the testing samples).\nSlicing your testing dataset by features of interest such as gender, age, demographics, etc., can bring to the surface issues such as:\n- bias\n- weakness points\n- relationships between inputs & outputs (aka explainability), etc.\n-> For example, your model can have extraordinary results in the [18, 30] age range but terrible ones in [30+, inf].\nThe aggregated metrics look great because most data samples are within the [18, 30] range. But in reality, your model fails the minority groups.\nThus, even though the aggregated metrics look great, you may deploy a broken model.\nTools: Snorkel\n#𝟯. 𝗘𝘅𝗽𝗲𝗿𝗶𝗺𝗲𝗻𝘁 𝗧𝗿𝗮𝗰𝗸𝗲𝗿\nYou just run 100+ experiments using different models and hyperparameters.\nYou already have your base model and slicing techniques set in place.\nHow can you easily compare these experiments?\nYou can quickly aggregate the results using an experiment tracker in a single graph(s).\nThus, you have the big picture to pick the best experiment and its metadata easily.\nTools: Comet ML, W&B, MLFlow, Neptune\n.\nTo conclude...\nTo quickly compare many experiments, you need:\n- a base model\n- to slice your testing split\n- an experiment tracker\nDo you recommend other tricks to improve your evaluation process?\n.\nIf you are curious to read more, check out my hands-on article:\n↳🔗 𝘎𝘶𝘪𝘥𝘦 𝘵𝘰 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘌𝘧𝘧𝘦𝘤𝘵𝘪𝘷𝘦 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨 𝘗𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘔𝘢𝘹𝘪𝘮𝘶𝘮 𝘙𝘦𝘴𝘶𝘭𝘵𝘴:\nhttps://lnkd.in/dXhmT9aV\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQGQ_AZb7aHRMA/feedshare-shrink_800/0/1704439990187?e=1707350400&v=beta&t=rn4-kvvz0aN-LWd5Teob7rUtKfRBP4GX6I01vNETI0c"
        },
        "Post_1": {
            "text": "Ever thought about how 𝗮 𝘆𝗲𝗮𝗿 can 𝗰𝗼𝗺𝗽𝗹𝗲𝘁𝗲𝗹𝘆 𝗰𝗵𝗮𝗻𝗴𝗲 your 𝘃𝗶𝘀𝗶𝗼𝗻? 𝟮𝟬𝟮𝟯 did just that for me...\nI don't really care about New Year's Eve. It is just another trip around the sun.\n...but it is a great moment to pause and think about the last year and the future.\nPersonally, I love planning things and learning from my past. It helps me gain perspective and prioritize the important things in my life.\n.\n𝘚𝘰 𝘩𝘦𝘳𝘦 𝘢𝘳𝘦 6 𝘵𝘩𝘪𝘯𝘨𝘴 𝘵𝘩𝘢𝘵 𝘩𝘢𝘱𝘱𝘦𝘯𝘦𝘥 𝘪𝘯 2023 𝘵𝘩𝘢𝘵 𝘐 𝘢𝘮 𝘨𝘳𝘢𝘵𝘦𝘧𝘶𝘭 𝘧𝘰𝘳:\n1. Enough discipline to work out almost daily & eat healthy.\n2. Grew my MLE & MLOps content creation business.\n3. Created x2 open-source MLE & MLOps free courses that accumulated >1800 stars on GitHub.\n4. Met many incredible MLE & MLOps people from Europe and the US.\n5. Had the chance to work on awesome deep fake projects in production at Metaphysic\n6. Adopted my second cat: Arthur. I love this little fellow. Now we are a happy 2 people - 2 cat family.\n...𝘢𝘯𝘥 𝘮𝘺 𝘵𝘰𝘱 6 𝘵𝘩𝘪𝘯𝘨𝘴 𝘐 𝘱𝘭𝘢𝘯 𝘵𝘰 𝘥𝘰 𝘪𝘯 2024:\n1. Grow the 𝗗𝗲𝗰𝗼𝗱𝗶𝗻𝗴 𝗠𝗟 𝗽𝘂𝗯𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻. I have some exciting news here that I will share in the following days.\n2. Travel & work from a different country every 3-4 months to level up my social and emotional skills while exploring the world.\n3. Level up my MLE & MLOps production-ready skills on real-world projects to share better insights with you.\n4. Better understand myself and be 100% true to myself.\n5. Start creating video content.\n6. This is a grand one: Move to the city center to leave my house more.\n.\nMy final take is that you should always take care of your mind & body as much as your tech skills.\nIf you degrade, your skills degrade with you.\nLet the games begin!\nWhat are your main goals for 2024, or what are you grateful for from last year?\n.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQGUsi3lpqdeWQ/feedshare-shrink_800/0/1704353488146?e=1707350400&v=beta&t=U3svlKOKn7BQOSiGE3lMXJMOHQUhCPfx_hejoRpxqU8"
        },
        "Post_2": {
            "text": "Do you want to 𝗹𝗲𝘃𝗲𝗹 𝘂𝗽 your 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻-𝗿𝗲𝗮𝗱𝘆 𝗠𝗟𝗘 & 𝗠𝗟𝗢𝗽𝘀 game?\nThen I have some great news 🔥\nSince I started creating content, I learned one crucial thing: \"𝘌𝘷𝘦𝘳𝘺𝘣𝘰𝘥𝘺 𝘭𝘪𝘬𝘦𝘴 𝘵𝘰 𝘳𝘦𝘢𝘥 𝘢𝘯𝘥 𝘭𝘦𝘢𝘳𝘯 𝘥𝘪𝘧𝘧𝘦𝘳𝘦𝘯𝘵𝘭𝘺.\"\nThat is why I 𝘀𝘁𝗮𝗿𝘁𝗲𝗱 my own 𝗠𝗲𝗱𝗶𝘂𝗺 𝗽𝘂𝗯𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻, called after my newsletter: \"𝘋𝘦𝘤𝘰𝘥𝘪𝘯𝘨 𝘔𝘓\"\nStarting from now, all my articles can be found under this Medium publication: 🔗\nhttps://lnkd.in/dTizqHG7\nCurrently, it is empty, but in January 2024, I plan to drop 𝟵 𝘀𝘂𝗿𝗽𝗿𝗶𝘀𝗲𝘀 in my new 𝗗𝗲𝗰𝗼𝗱𝗶𝗻𝗴 𝗠𝗟 𝗠𝗲𝗱𝗶𝘂𝗺 𝗽𝘂𝗯𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻.\nIf you have liked my content so far...\n-> Support me and follow my new publication as you will enjoy what I prepared: 🔗\nhttps://lnkd.in/dTizqHG7\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQEmn1alF-aNGw/feedshare-shrink_800/0/1704267126276?e=1707350400&v=beta&t=Nv2P0OMYi5bO9kBY6RZzjX5OpG172UdYeR5fnPVoI8U"
        },
        "Post_3": {
            "text": "2023 was crazy. Here are my 𝗿𝗲𝘀𝘂𝗹𝘁𝘀 after 𝗼𝗻𝗲 𝘆𝗲𝗮𝗿 of 𝘄𝗿𝗶𝘁𝗶𝗻𝗴 𝗰𝗼𝗻𝘁𝗲𝗻𝘁 about 𝗠𝗟𝗘 and 𝗠𝗟𝗢𝗽𝘀 and starting as a nobody.\n...not that now I am somebody 😂\nI had some humble goals. At the beginning of 2022, I hadn't imagined I would plan to make a living out of creating content:\n- 𝗟𝗶𝗻𝗸𝗲𝗱𝗜𝗻: 3k -> 22.5k followers (goal 10k)\n- 𝗠𝗲𝗱𝗶𝘂𝗺: 200 -> 1.3k followers (goal 1k)\n...but I stumbled on some great sources of inspiration and people who helped me grow more than I could have imagined.\nThus, during 2023, I got excited and experimented with quite a few things, such as:\n- 𝘅𝟮 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗰𝗼𝘂𝗿𝘀𝗲𝘀 about MLOps, LLMOps, and MLE that accumulated over 1800 GitHub stars:\n-> 𝘏𝘢𝘯𝘥𝘴-𝘰𝘯 𝘓𝘓𝘔𝘴:\nhttps://lnkd.in/dZgqtf8f\n-> 𝘛𝘩𝘦 𝘍𝘶𝘭𝘭 𝘚𝘵𝘢𝘤𝘬 7-𝘚𝘵𝘦𝘱𝘴 𝘔𝘓𝘖𝘱𝘴 𝘍𝘳𝘢𝘮𝘦𝘸𝘰𝘳𝘬:\nhttps://lnkd.in/d5HUN39Q\n- I started my newsletter: \"𝗗𝗲𝗰𝗼𝗱𝗶𝗻𝗴 𝗠𝗟\", which grew to 2.8k followers\n- I tried to grow my 𝗧𝘄𝗶𝘁𝘁𝗲𝗿/𝗫 account (only 233 followers): this was quite a mess as I haven't    tweaked my posts well enough to satisfy the\n.\n2023 𝙬𝙖𝙨 𝙖𝙣 𝙚𝙭𝙘𝙞𝙩𝙞𝙣𝙜 𝙮𝙚𝙖𝙧, 𝙖𝙣𝙙 𝙄 𝙝𝙖𝙫𝙚 𝙨𝙤𝙢𝙚 𝙩𝙝𝙧𝙞𝙡𝙡𝙞𝙣𝙜 𝙥𝙡𝙖𝙣𝙨 𝙛𝙤𝙧 2024.\nBut...\nI want to thank everybody who followed me and engaged with my content. You are one of the first drivers that keep me going. So... Thank you 🙏\nSecondly, setting such goals isn't that essential. What if I haven't reached out to any of them? That would have demoralized me entirely. But, I think they are critical in giving a clear direction.\nRemember: \"Direction is more important than... anything.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQHhZmHkyXrrsQ/feedshare-shrink_800/0/1704180681017?e=1707350400&v=beta&t=gcNoiHATosFLEJeLjcvHQMLWTcHonktjxdcij57Qbpw"
        },
        "Post_4": {
            "text": "If anyone told you that 𝗠𝗟 or 𝗠𝗟𝗢𝗽𝘀 is 𝗲𝗮𝘀𝘆, they were 𝗿𝗶𝗴𝗵𝘁.\nHere is a simple trick that I learned the hard way ↓\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is → \"𝗙𝗼𝗰𝘂𝘀 𝗼𝗻 𝘁𝗵𝗲 𝗳𝘂𝗻𝗱𝗮𝗺𝗲𝗻𝘁𝗮𝗹𝘀.\"\n.\nLet me explain ↓\nWhen you constantly chase the latest models (aka FOMO), you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npersonaldevelopment\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQG1WqQlxxnj7A/image-shrink_800/0/1703057415998?e=1705082400&v=beta&t=N1j36QNO1kWqukgpJt1giHoIjlb7A6jOUTfEOmrtcWs"
        },
        "Post_5": {
            "text": "𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 FREE 𝗰𝗼𝘂𝗿𝘀𝗲 just 𝗽𝗮𝘀𝘀𝗲𝗱 750+ 𝗚𝗶𝘁𝗛𝘂𝗯 ⭐🌟⭐\nIf you want to 𝐥𝐞𝐚𝐫𝐧 for FREE to 𝐛𝐮𝐢𝐥𝐝 𝐡𝐚𝐧𝐝𝐬-𝐨𝐧 𝐋𝐋𝐌 𝐬𝐲𝐬𝐭𝐞𝐦𝐬 using good LLMOps principles, this might be something for you.\n.\nA big 𝗧𝗵𝗮𝗻𝗸 𝘆𝗼𝘂! ...for everyone who supported the GitHub repo. This means a lot to me.\nAlso, I want to thank Pau Labarta and Alexandru Razvant for this fantastic collaboration and for making this course possible.\n.\n𝘍𝘰𝘳 𝘵𝘩𝘦 𝘱𝘦𝘰𝘱𝘭𝘦 𝘸𝘩𝘰 𝘥𝘰𝘯'𝘵 𝘬𝘯𝘰𝘸, 𝘩𝘦𝘳𝘦 𝘪𝘴 𝘴𝘰𝘮𝘦 𝘤𝘰𝘯𝘵𝘦𝘹𝘵 𝘢𝘣𝘰𝘶𝘵 𝘵𝘩𝘦 𝘤𝘰𝘶𝘳𝘴𝘦 ↓\n𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗢𝗻 𝗟𝗟𝗠𝘀 course is not just another demo of how to make a few predictions in a notebook.\nWe will primarily focus on the engineering & MLOps aspects.\nYou'll walk away with a 𝗳𝘂𝗹𝗹𝘆 𝗼𝗽𝗲𝗿𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗽𝗿𝗼𝗱𝘂𝗰𝘁, leveraging Large Language Models (LLMs), LLMOps, and the 3-pipeline design to build a chatbot for financial investment advice.\nThese are 𝐭𝐡𝐞 3 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭𝐬 𝐲𝐨𝐮 𝐰𝐢𝐥𝐥 𝐥𝐞𝐚𝐫𝐧 𝐭𝐨 𝐛𝐮𝐢𝐥𝐝 during the course ↓\n1. a 𝐫𝐞𝐚𝐥-𝐭𝐢𝐦𝐞 𝐬𝐭𝐫𝐞𝐚𝐦𝐢𝐧𝐠 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2. a 𝐟𝐢𝐧𝐞-𝐭𝐮𝐧𝐢𝐧𝐠 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3. an 𝐢𝐧𝐟𝐞𝐫𝐞𝐧𝐜𝐞 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nWe will also show you how to integrate various serverless tools, such as:\n• Comet ML as your ML Platform;\n• Qdrant as your vector DB;\n• Beam as your infrastructure.\n.\n𝐖𝐡𝐨 𝐢𝐬 𝐭𝐡𝐢𝐬 𝐟𝐨𝐫?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\n𝐇𝐨𝐰 𝐰𝐢𝐥𝐥 𝐲𝐨𝐮 𝐥𝐞𝐚𝐫𝐧?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\n.\n𝐂𝐮𝐫𝐢𝐨𝐮𝐬?\n↳ Check out the 𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 FREE course and support us with a ⭐: 🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFJY7MXbxX_gA/image-shrink_800/0/1702971017234?e=1705082400&v=beta&t=zvhexAVrC7qkRWhYmvV2qRfnFqe5LpIiUqwRVcn38Ao"
        },
        "Post_6": {
            "text": "𝗦𝘂𝗽𝗲𝗿𝗰𝗵𝗮𝗿𝗴𝗲 𝘆𝗼𝘂𝗿 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺: 𝘂𝘀𝗲 𝗮 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\n𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗢𝗳𝗳𝗹𝗶𝗻𝗲 𝗙𝗲𝗮𝘁𝘂𝗿𝗲𝘀\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘄𝗵𝗲𝗿𝗲 𝘁𝗵𝗲 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆 𝗸𝗶𝗰𝗸𝘀 𝗶𝗻 ↓\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesn’t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\n.\nTo learn more, check out my hands-on article:\n↳🔗 𝘎𝘶𝘪𝘥𝘦 𝘵𝘰 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘌𝘧𝘧𝘦𝘤𝘵𝘪𝘷𝘦 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨 𝘗𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘔𝘢𝘹𝘪𝘮𝘶𝘮 𝘙𝘦𝘴𝘶𝘭𝘵𝘴:\nhttps://lnkd.in/d85u_wwN\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQHdEuVbu2i35g/feedshare-shrink_800/0/1702633568332?e=1707350400&v=beta&t=R031162rqsBad0glmrfjFZeZcAPIDHkp6ZR-YyW4324"
        },
        "Post_7": {
            "text": "Have you ever presented your MLOps ideas to upper management just to get ghosted?\nIn that case...\nRaphaël Hoogvliets\n,\nBaşak Tuğçe Eskili\n, and\nMaria Vechtomova\nfrom\nMarvelous MLOps\npresented a great step-by-step strategy for pitching your MLOps ideas to your upper management and getting attention and resources to implement them.\nHere are the 6 steps you have to know ↓\n1. 𝐂𝐨𝐥𝐥𝐞𝐜𝐭 𝐚𝐥𝐥 𝐭𝐡𝐞 𝐩𝐚𝐢𝐧 𝐩𝐨𝐢𝐧𝐭𝐬\nTalk to data scientists, product owners, and stakeholders in your organization to gather issues such as:\n- time to deployment\n- poor quality deployment\n- non-existing monitoring\n- lack of collaboration\n- external parties\n2. 𝐄𝐝𝐮𝐜𝐚𝐭𝐞 𝐩𝐞𝐨𝐩𝐥𝐞\nOrganize workshops, meetings, etc., to present what MLOps is and how it can help.\nI think it's critical to present it to your target audience. For example, an engineer looks at the problem differently than the business stakeholders.\n3. 𝐏𝐫𝐞𝐬𝐞𝐧𝐭 𝐛𝐞𝐟𝐨𝐫𝐞 𝐚𝐧𝐝 𝐚𝐟𝐭𝐞𝐫 𝐬𝐜𝐞𝐧𝐚𝐫𝐢𝐨𝐬\nShow how MLOps can solve the company's challenges and deliver tangible benefits to the organization, such as:\n- less cost\n- fast deployment\n- better collaboration\n- less risk\n4. 𝐏𝐫𝐨𝐯𝐞 𝐢𝐭\nUse concrete examples to support your ideas, such as:\n- how a competitor or an organization in the same or related field benefited from introducing MLOps\n- build a PoC within your organization\n5. 𝐒𝐞𝐭 𝐮𝐩 𝐲𝐨𝐮𝐫 𝐭𝐞𝐚𝐦\nChoose 2-3 experienced individuals (not juniors) to set up the foundations in your team/organization.\nWith an emphasis on starting with experienced engineers and only later bringing more juniors to the party.\n6. 𝐊𝐞𝐞𝐩 𝐨𝐧 𝐤𝐞𝐞𝐩𝐢𝐧' 𝐨𝐧\nOnce you successfully apply MLOps to one use case, you can bring in more responsibility by growing your team and taking on more projects.\n.\nAll of these are great tips for integrating MLOps in your organization.\nI love their \"Present before and after scenarios\" approach.\nYou can extrapolate this strategy for any other new processes (not only MLOps).\n.\n↳ To learn the details, check out Marvelous MLOps full article at 🔗\nhttps://lnkd.in/dBesYGBv\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFmfyzSQixw0Q/image-shrink_800/0/1702625411189?e=1705082400&v=beta&t=xhqh9DVW5UD6gkaiL40ITlUpZIGrunobF6XJEuFDtSo"
        },
        "Post_8": {
            "text": "Next Tuesday, 19 December, at 17:30 CET, I am pleased to participate in the 2nd edition of 𝗔𝘀𝗸\nMarvelous MLOps\n𝗔𝗻𝘆𝘁𝗵𝗶𝗻𝗴: a live Q&A session where we try to answer all your questions around\nhashtag\n#\nmlops\nand\nhashtag\n#\nllmops\n.\nIn the spirit of Christmas, you can ask my magic floating head anything, and I will grant you hands-on answers.\n↳ Join the event here: 🔗\nhttps://lnkd.in/d_bu2RJv\nSee you there 👀\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQF1V9ProdQ5nA/image-shrink_800/0/1702539012444?e=1705082400&v=beta&t=Y0GV9S91L-gVcFMbupr9qEbB2dAn-vMBsgmZuwsJXCc"
        },
        "Post_9": {
            "text": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process → you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do ↓\n#𝟭. 𝗦𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝗮𝗻𝘀𝘄𝗲𝗿𝘀 - 𝘆𝗼𝘂 𝗸𝗻𝗼𝘄 𝗲𝘅𝗮𝗰𝘁𝗹𝘆 𝘄𝗵𝗮𝘁 𝘆𝗼𝘂 𝘄𝗮𝗻𝘁 𝘁𝗼 𝗴𝗲𝘁\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#𝟮. 𝗡𝗼 \"𝗿𝗶𝗴𝗵𝘁\" 𝗮𝗻𝘀𝘄𝗲𝗿 (𝗲.𝗴., 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗻𝗴 𝗱𝗲𝘀𝗰𝗿𝗶𝗽𝘁𝗶𝗼𝗻𝘀, 𝘀𝘂𝗺𝗺𝗮𝗿𝗶𝗲𝘀, 𝗲𝘁𝗰.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\n↳ 𝟮.𝟭 𝗪𝗵𝗲𝗻 𝘆𝗼𝘂 𝗱𝗼𝗻'𝘁 𝗵𝗮𝘃𝗲 𝗮𝗻 𝗶𝗱𝗲𝗮𝗹 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 𝗰𝗼𝗺𝗽𝗮𝗿𝗲 𝘁𝗵𝗲 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 (𝘆𝗼𝘂 𝗱𝗼𝗻'𝘁 𝗵𝗮𝘃𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\n↳ 𝟮.𝟮. 𝗪𝗵𝗲𝗻 𝘆𝗼𝘂 𝗱𝗼 𝗵𝗮𝘃𝗲 𝗮𝗻 𝗶𝗱𝗲𝗮𝗹 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 𝗰𝗼𝗺𝗽𝗮𝗿𝗲 𝘁𝗵𝗲 𝗿𝗲𝘀𝗽𝗼𝗻𝘀𝗲 𝘁𝗼 (𝘆𝗼𝘂 𝗵𝗮𝘃𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D5610AQFoO9A1JvNRJw/image-shrink_800/0/1702452603385?e=1705082400&v=beta&t=AK8KePlsAkCVzWKPcGzCVisrINAayNL9hS9aOG_eyrw"
        },
        "Post_10": {
            "text": "What is the 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲 between your 𝗠𝗟 𝗱𝗲𝘃𝗲𝗹𝗼𝗽𝗺𝗲𝗻𝘁 and 𝗰𝗼𝗻𝘁𝗶𝗻𝘂𝗼𝘂𝘀 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗲𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁𝘀?\nThey might do the same thing, but their design is entirely different ↓\n𝗠𝗟 𝗗𝗲𝘃𝗲𝗹𝗼𝗽𝗺𝗲𝗻𝘁 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\n𝗖𝗼𝗻𝘁𝗶𝗻𝘂𝗼𝘂𝘀 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- 𝘪𝘯𝘱𝘶𝘵: data artifact\n- 𝘰𝘶𝘵𝘱𝘶𝘵: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- 𝘪𝘯𝘱𝘶𝘵: data, code, config artifacts\n- 𝘰𝘶𝘵𝘱𝘶𝘵: model artifact\n.\n↳ See this strategy in action in my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course: 🔗\nhttps://lnkd.in/d_GVpZ9X\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQEdpFdpJSlDKQ/image-shrink_800/0/1701156624205?e=1705082400&v=beta&t=jxPE3kyWPThjTX_XDPpcMOeSnaBplBodZgaU5ukMN3c"
        },
        "Post_11": {
            "text": "I am excited to announce that I will participate in a live Q&A session hosted by\nMarvelous MLOps\n.\nIn case you want to ask me anything. See you on Tuesday (19th of December) 👀\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQGA4rQNkKGyfw/image-shrink_800/0/1701070219962?e=1705082400&v=beta&t=g6vq_5Wkf8n8BkWmqMlXpViOvcdwHPfFBRO6zacGl8g"
        },
        "Post_12": {
            "text": "Next Tuesday we'll have the 2nd edition of 𝗔𝘀𝗸 𝗠𝗮𝗿𝘃𝗲𝗹𝗼𝘂𝘀 𝗠𝗟𝗢𝗽𝘀 𝗔𝗻𝘆𝘁𝗵𝗶𝗻𝗴:\nA live Q&A sessions where we try to answer all your questions 🤗\nThis time\nPaul Iusztin\nwill join us, sign up here:\nhttps://lnkd.in/eVk7e8k5",
            "image": "https://media.licdn.com/dms/image/D4D10AQG1TheTmWHOVQ/image-shrink_800/0/1700724626687?e=1705082400&v=beta&t=SMspEABg6mO8NCZxwfZGsLYx-KoRkXdNTyZYrCRTshE"
        },
        "Post_13": {
            "text": "If you want to 𝐥𝐞𝐚𝐫𝐧 for FREE to 𝐛𝐮𝐢𝐥𝐝 𝐡𝐚𝐧𝐝𝐬-𝐨𝐧 𝐋𝐋𝐌 𝐬𝐲𝐬𝐭𝐞𝐦𝐬 using good LLMOps principles, we want to announce that we just 𝐟𝐢𝐧𝐢𝐬𝐡𝐞𝐝 the code & video lessons for the \"𝐇𝐚𝐧𝐝𝐬-𝐨𝐧 𝐋𝐋𝐌𝐬\" 𝐜𝐨𝐮𝐫𝐬𝐞.\nBy finishing the Hands-On LLMs free course, you will learn how to use the 3-pipeline architecture & LLMOps good practices to design, build, and deploy a real-time financial advisor powered by LLMs & vector DBs.\nWe will primarily focus on the engineering & MLOps aspects.\nThus, by the end of this series, you will know how to build & deploy a real ML system, not some isolated code in Notebooks.\n𝐌𝐨𝐫𝐞 𝐩𝐫𝐞𝐜𝐢𝐬𝐞𝐥𝐲, 𝐭𝐡𝐞𝐬𝐞 𝐚𝐫𝐞 𝐭𝐡𝐞 3 𝐜𝐨𝐦𝐩𝐨𝐧𝐞𝐧𝐭𝐬 𝐲𝐨𝐮 𝐰𝐢𝐥𝐥 𝐥𝐞𝐚𝐫𝐧 𝐭𝐨 𝐛𝐮𝐢𝐥𝐝:\n1.  a 𝐫𝐞𝐚𝐥-𝐭𝐢𝐦𝐞 𝐬𝐭𝐫𝐞𝐚𝐦𝐢𝐧𝐠 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 (deployed on AWS) that listens to financial news, cleans & embeds the documents, and loads them to a vector DB\n2.  a 𝐟𝐢𝐧𝐞-𝐭𝐮𝐧𝐢𝐧𝐠 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 (deployed as a serverless continuous training) that fine-tunes an LLM on financial data using QLoRA, monitors the experiments using an experiment tracker and saves the best model to a model registry\n3.  an 𝐢𝐧𝐟𝐞𝐫𝐞𝐧𝐜𝐞 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 built in LangChain (deployed as a serverless RESTful API) that loads the fine-tuned LLM from the model registry and answers financial questions using RAG (leveraging the vector DB populated with financial news in real-time)\nWe will also show you how to integrate various serverless tools, such as:\n• Comet ML as your ML Platform;\n• Qdrant as your vector DB;\n• Beam as your infrastructure.\n𝐖𝐡𝐨 𝐢𝐬 𝐭𝐡𝐢𝐬 𝐟𝐨𝐫?\nThe series targets MLE, DE, DS, or SWE who want to learn to engineer LLM systems using LLMOps good principles.\n𝐇𝐨𝐰 𝐰𝐢𝐥𝐥 𝐲𝐨𝐮 𝐥𝐞𝐚𝐫𝐧?\nThe series contains 4 hands-on video lessons and the open-source code you can access on GitHub.\n𝐂𝐮𝐫𝐢𝐨𝐮𝐬?\n↳ Check it out and support us with a ⭐:  🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGFh8AbplRcZQ/image-shrink_800/0/1700638220231?e=1705082400&v=beta&t=M3cW_Kzw-1gWhLQEYoYkiJRIdfktmlp700RZWry_SCc"
        },
        "Post_14": {
            "text": "How do you 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲 a 𝘀𝘆𝗻𝘁𝗵𝗲𝘁𝗶𝗰 𝗱𝗼𝗺𝗮𝗶𝗻-𝘀𝗽𝗲𝗰𝗶𝗳𝗶𝗰 𝗤&𝗔 𝗱𝗮𝘁𝗮𝘀𝗲𝘁 in <𝟯𝟬 𝗺𝗶𝗻𝘂𝘁𝗲𝘀 to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 your 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠?\nThis method is also known as 𝗳𝗶𝗻𝗲𝘁𝘂𝗻𝗶𝗻𝗴 𝘄𝗶𝘁𝗵 𝗱𝗶𝘀𝘁𝗶𝗹𝗹𝗮𝘁𝗶𝗼𝗻. Here are its 3 𝘮𝘢𝘪𝘯 𝘴𝘵𝘦𝘱𝘴 ↓\n𝘍𝘰𝘳 𝘦𝘹𝘢𝘮𝘱𝘭𝘦, 𝘭𝘦𝘵'𝘴 𝘨𝘦𝘯𝘦𝘳𝘢𝘵𝘦 𝘢 𝘘&𝘈 𝘧𝘪𝘯𝘦-𝘵𝘶𝘯𝘪𝘯𝘨 𝘥𝘢𝘵𝘢𝘴𝘦𝘵 𝘶𝘴𝘦𝘥 𝘵𝘰 𝘧𝘪𝘯𝘦-𝘵𝘶𝘯𝘦 𝘢 𝘧𝘪𝘯𝘢𝘯𝘤𝘪𝘢𝘭 𝘢𝘥𝘷𝘪𝘴𝘰𝘳 𝘓𝘓𝘔.\n𝗦𝘁𝗲𝗽 𝟭: 𝗠𝗮𝗻𝘂𝗮𝗹𝗹𝘆 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲 𝗮 𝗳𝗲𝘄 𝗶𝗻𝗽𝘂𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀\nGenerate a few input samples (~3) that have the following structure:\n- 𝘶𝘴𝘦𝘳_𝘤𝘰𝘯𝘵𝘦𝘹𝘵: describe the type of investor (e.g., \"I am a 28-year-old marketing professional\")\n- 𝘲𝘶𝘦𝘴𝘵𝘪𝘰𝘯: describe the user's intention (e.g., \"Is Bitcoin a good investment option?\")\n𝗦𝘁𝗲𝗽 𝟮: 𝗘𝘅𝗽𝗮𝗻𝗱 𝘁𝗵𝗲 𝗶𝗻𝗽𝘂𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀 𝘄𝗶𝘁𝗵 𝘁𝗵𝗲 𝗵𝗲𝗹𝗽 𝗼𝗳 𝗮 𝘁𝗲𝗮𝗰𝗵𝗲𝗿 𝗟𝗟𝗠\nUse a powerful LLM as a teacher (e.g., GPT4, Falcon 180B, etc.) to generate up to +N similar input examples.\nWe generated 100 input examples in our use case, but you can generate more.\nYou will use the manually filled input examples to do few-shot prompting.\nThis will guide the LLM to give you domain-specific samples.\n𝘛𝘩𝘦 𝘱𝘳𝘰𝘮𝘱𝘵 𝘸𝘪𝘭𝘭 𝘭𝘰𝘰𝘬 𝘭𝘪𝘬𝘦 𝘵𝘩𝘪𝘴:\n\"\"\"\n...\nGenerate 100 more examples with the following pattern:\n# USER CONTEXT 1\n...\n# QUESTION 1\n...\n# USER CONTEXT 2\n...\n\"\"\"\n𝗦𝘁𝗲𝗽 𝟯: 𝗨𝘀𝗲 𝘁𝗵𝗲 𝘁𝗲𝗮𝗰𝗵𝗲𝗿 𝗟𝗟𝗠 𝘁𝗼 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲 𝗼𝘂𝘁𝗽𝘂𝘁𝘀 𝗳𝗼𝗿 𝗮𝗹𝗹 𝘁𝗵𝗲 𝗶𝗻𝗽𝘂𝘁 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀\nNow, you will have the same powerful LLM as a teacher, but this time, it will answer all your N input examples.\nBut first, to introduce more variance, we will use RAG to enrich the input examples with news context.\nAfterward, we will use the teacher LLM to answer all N input examples.\n...and bam! You generated a domain-specific Q&A dataset with almost 0 manual work.\n.\nNow, you will use this data to train a smaller LLM (e.g., Falcon 7B) on a niched task, such as financial advising.\nThis technique is known as finetuning with distillation because you use a powerful LLM as the teacher (e.g., GPT4, Falcon 180B) to generate the data, which will be used to fine-tune a smaller LLM (e.g., Falcon 7B), which acts as the student.\n✒️ 𝘕𝘰𝘵𝘦: To ensure that the generated data is of high quality, you can hire a domain expert to check & refine it.\n.\n↳ To learn more about this technique, check out this article from Pau Labarta's RLML newsletter: 🔗\nhttps://lnkd.in/d4m9pbn5\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGEejtz6vXI9w/image-shrink_800/0/1700551829710?e=1705082400&v=beta&t=aeWfGt6XqnDaSULDRD71dv9Rzaeli0QOFk7hpMKf3Sg"
        },
        "Post_15": {
            "text": "We just released the 3rd Lesson of the free Hands-on LLMs course. If you want to learn how to build a streaming pipeline for real-time text embedding, check it out ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\nmlops\nhashtag\n#\nllm",
            "image": "https://media.licdn.com/dms/image/D4D22AQGi5fXZelxn5A/feedshare-shrink_800/0/1700299851370?e=1707350400&v=beta&t=6HjAC5cnp7ExqbkCbQ1k7u87840_LKbHXwiqxtdf5Hs"
        },
        "Post_16": {
            "text": "🎬 𝗟𝗲𝘀𝘀𝗼𝗻 𝟯 of the 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠 𝗰𝗼𝘂𝗿𝘀𝗲 is 𝗢𝗨𝗧 🎬\n𝗖𝗼𝗻𝘁𝗲𝘅𝘁\nLLMs are as good as the data you embed in your prompts. And for many real-world problems, this means the data needs to be both\n→ good, and\n→ fresh\n𝗘𝘅𝗮𝗺𝗽𝗹𝗲 💁\nImagine you build a great LLM-based financial advisor… but you only feed it with outdated data.\nNo matter how good your model is, the predictions it will generate will be rubbish 🫣\nSo, the question is\n𝙃𝙤𝙬 𝙙𝙤 𝙛𝙚𝙚𝙙 𝙛𝙧𝙚𝙨𝙝 𝙙𝙖𝙩𝙖 𝙩𝙤 𝙮𝙤𝙪𝙧 𝙇𝙇𝙈 ❓\n𝗧𝗵𝗲 𝘀𝗼𝗹𝘂𝘁𝗶𝗼𝗻 🧠\nYou need to build a 𝗿𝗲𝗮𝗹-𝘁𝗶𝗺𝗲 𝘁𝗲𝘅𝘁 𝗲𝗺𝗯𝗲𝗱𝗱𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲, that\n→ 𝗜𝗻𝗴𝗲𝘀𝘁𝘀 raw text from your data source, in real-time\n→ 𝗧𝗿𝗮𝗻𝘀𝗳𝗼𝗿𝗺𝘀 this raw text into vector embeddings, and\n→ 𝗦𝘁𝗼𝗿𝗲𝘀 these embeddings in a VectorDB, so your LLM can fetch and use them for Retrieval Augmented Generation (RAG) at inference time.\n𝗙𝘂𝗹𝗹 𝘀𝗼𝘂𝗿𝗰𝗲 𝗰𝗼𝗱𝗲 👨🏻‍💻\nIn Lesson 3 of the 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠 𝗖𝗼𝘂𝗿𝘀𝗲, you will find a full source code implementation of a text embedding pipeline, that uses:\n→\nAlpaca\nNews API as our real-time data source,\n→\nBytewax\nto transform raw text into vector embeddings, and\n→\nQdrant\nas a Serverless Vector DB, to store and retrieve embeddings at inference time.\nYou will find the link to the 𝗚𝗶𝘁𝗛𝘂𝗯 𝗿𝗲𝗽𝗼 in the comment section below ⬇️\n𝗪𝗵𝗮𝘁'𝘀 𝗻𝗲𝘅𝘁? ⏭️\nIn the next lecture, we will cover the 𝗶𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲, so our system can serve predictions to end users. 🚀\nTHANK YOU\nPaul Iusztin\nand\nAlexandru Răzvanț 👋\nfor making this course possible. It is a pleasure working with you guys!\n----\nHi there! It's\nPau Labarta Bajo\n👋\nEvery week I share free, hands-on content, on production-grade ML, to help you build real-world ML products.\n𝗙𝗼𝗹𝗹𝗼𝘄 𝗺𝗲 and 𝗰𝗹𝗶𝗰𝗸 𝗼𝗻 𝘁𝗵𝗲 🔔 so you don't miss what's coming next\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nrealworldml\nhashtag\n#\nllmops\nhashtag\n#\nllms\nhashtag\n#\nbytewax\nhashtag\n#\nqdrant\nhashtag\n#\nvectordb\nhashtag\n#\nrealtimeml",
            "image": "https://media.licdn.com/dms/image/D4D10AQFT2dwwdcKnKw/image-shrink_800/0/1698910225666?e=1705082400&v=beta&t=I_tqazlPLy61eGxPuWRed7Ibat2si35BDaaV-xMA7eY"
        },
        "Post_17": {
            "text": "𝗗𝗲𝗽𝗹𝗼𝘆𝗶𝗻𝗴 & 𝗺𝗮𝗻𝗮𝗴𝗶𝗻𝗴 ML models is 𝗵𝗮𝗿𝗱, especially when running your models on GPUs.\nBut 𝘀𝗲𝗿𝘃𝗲𝗿𝗹𝗲𝘀𝘀 makes things 𝗲𝗮𝘀𝘆.\nUsing\nBeam\nas your serverless provider, deploying & managing ML models can be as easy as ↓\n𝗗𝗲𝗳𝗶𝗻𝗲 𝘆𝗼𝘂𝗿 𝗶𝗻𝗳𝗿𝗮𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲 & 𝗱𝗲𝗽𝗲𝗻𝗱𝗲𝗻𝗰𝗶𝗲𝘀\nIn a few lines of code, you define the application that contains:\n- the requirements of your infrastructure, such as the CPU, RAM, and GPU\n- the dependencies of your application\n- the volumes from where you can load your data and store your artifacts\n𝗗𝗲𝗽𝗹𝗼𝘆 𝘆𝗼𝘂𝗿 𝗷𝗼𝗯𝘀\nUsing the Beam application, you can quickly decore your Python functions to:\n- run them once on the given serverless application\n- put your task/job in a queue to be processed or even schedule it using a CRON-based syntax\n- even deploy it as a RESTful API endpoint\n.\nAs you can see in the image below, you can have one central function for training or inference, and with minimal effort, you can switch from all these deployment methods.\nAlso, you don't have to bother at all with managing the infrastructure on which your jobs run. You specify what you need, and Beam takes care of the rest.\nBy doing so, you can directly start to focus on your application and stop carrying about the infrastructure.\nThis is the power of serverless!\n↳ Check out\nBeam\nto learn more: 🔗\nhttps://lnkd.in/d4-pkCxc\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQFTZKv4E86hGw/image-shrink_800/0/1698737423218?e=1705082400&v=beta&t=BKwJmd05FjemiJhCa4RC2YjkCgVxWxNLR_IM87Cj49o"
        },
        "Post_18": {
            "text": "This is how I 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗲𝗱 𝗣𝘆𝗗𝗼𝗰𝘀 for 𝟭𝟬𝟬 𝗣𝘆𝘁𝗵𝗼𝗻 𝗳𝘂𝗻𝗰𝘁𝗶𝗼𝗻𝘀 in <𝟭 𝗵𝗼𝘂𝗿 ↓\nThe most boring programming part is to write PyDocs, so I usually write clean code and let it speak for itself.\nBut, for open-source projects where you have to generate robust documentation, PyDocs are a must.\nThe good news is that now you can automate this process using Copilot.\nYou can see in the video below an example of how easy it is.\nI tested it on more complex functions/classes, and it works well. I chose this example because it fits nicely on one screen.\nOnce I tested Copilot's experience, I will never go back.\nIt is true that, in some cases, you have to make some minor adjustments. But that is still 10000% more efficient than writing it from scratch.\nIf you want more examples, check out our Hands-on LLMs course, where all the PyDocs are generated 99% using Copilot in <1 hour.\n↳ Check it out: 🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGkhXbIeRV1ag/image-shrink_800/0/1698651020181?e=1705082400&v=beta&t=FBlsJxnnrIT3xDkoQ1qdkMXg17GKRyCK1y3GixwrULE"
        },
        "Post_19": {
            "text": "𝗪𝗵𝗮𝘁 do you need to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 an open-source 𝗟𝗟𝗠 to create your own 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝗱𝘃𝗶𝘀𝗼𝗿?\nThis is the 𝗟𝗟𝗠 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 𝗸𝗶𝘁 you must know ↓\n𝗗𝗮𝘁𝗮𝘀𝗲𝘁\nThe key component of any successful ML project is the data.\nYou need a 100 - 1000 sample Q&A (questions & answers) dataset with financial scenarios.\nThe best approach is to hire a bunch of experts to create it manually.\nBut, for a PoC, that might get expensive & slow.\nThe good news is that a method called \"𝘍𝘪𝘯𝘦𝘵𝘶𝘯𝘪𝘯𝘨 𝘸𝘪𝘵𝘩 𝘥𝘪𝘴𝘵𝘪𝘭𝘭𝘢𝘵𝘪𝘰𝘯\" exists.\nIn a nutshell, this is how it works: \"Use a big & powerful LLM (e.g., GPT4) to generate your fine-tuning data. After, use this data to fine-tune a smaller model (e.g., Falcon 7B).\"\nFor specializing smaller LLMs on specific use cases (e.g., financial advisors), this is an excellent method to kick off your project.\n𝗣𝗿𝗲-𝘁𝗿𝗮𝗶𝗻𝗲𝗱 𝗼𝗽𝗲𝗻-𝘀𝗼𝘂𝗿𝗰𝗲 𝗟𝗟𝗠\nYou never want to start training your LLM from scratch (or rarely).\nWhy? Because you need trillions of tokens & millions of $$$ in compute power.\nYou want to fine-tune your LLM on your specific task.\nThe good news is that you can find a plethora of open-source LLMs on HuggingFace (e.g., Falcon, LLaMa, etc.)\n𝗣𝗮𝗿𝗮𝗺𝗲𝘁𝗲𝗿 𝗲𝗳𝗳𝗶𝗰𝗶𝗲𝗻𝘁 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴\nAs LLMs are big... duh...\n... they don't fit on a single GPU.\nAs you want only to fine-tune the LLM, the community invented clever techniques that quantize the LLM (to fit on a single GPU) and fine-tune only a set of smaller adapters.\nOne popular approach is QLoRA, which can be implemented using HF's `𝘱𝘦𝘧𝘵` Python package.\n𝗠𝗟𝗢𝗽𝘀\nAs you want your project to get to production, you have to integrate the following MLOps components:\n- experiment tracker to monitor & compare your experiments\n- model registry to version & share your models between the FTI pipelines\n- prompts monitoring to debug & track complex chains\n↳ All of them are available on ML platforms, such as Comet ML 🔗\nhttps://lnkd.in/d7jNQz7m\n𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗽𝗹𝗮𝘁𝗳𝗼𝗿𝗺\nThe most common approach is to train your LLM on your on-prem Nivida GPUs cluster or rent them on cloud providers such as AWS, Paperspace, etc.\nBut what if I told you that there is an easier way?\nThere is! It is called serverless.\nFor example,\nBeam\nis a GPU serverless provider that makes deploying your training pipeline as easy as decorating your Python function with `@𝘢𝘱𝘱.𝘳𝘶𝘯()`.\nAlong with ease of deployment, you can easily add your training code to your CI/CD to add the final piece of the MLOps puzzle, called CT (continuous training).\n↳ Beam: 🔗\nhttps://lnkd.in/dedCaMDh\n.\n↳ To see all these components in action, check out my FREE 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲 & give it a ⭐:  🔗\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQHWQzZcToQQ1Q/image-shrink_800/0/1698388219549?e=1705082400&v=beta&t=9mrDC_NooJgD7u7Qk0PmrTGGaZtuwDIFKh3bEqeBsm0"
        },
        "Post_20": {
            "text": "𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 FREE 𝗰𝗼𝘂𝗿𝘀𝗲 just 𝗽𝗮𝘀𝘀𝗲𝗱 𝟰𝟬𝟬+ 𝗚𝗶𝘁𝗛𝘂𝗯 ⭐🌟⭐\nA big 𝗧𝗵𝗮𝗻𝗸 𝘆𝗼𝘂!, for everyone who supported the GitHub repo. This means a lot to me.\nAlso, I want to thank Pau Labarta and Alexandru Razvant for this fantastic collaboration and for making this course possible.\n.\n𝘍𝘰𝘳 𝘵𝘩𝘦 𝘱𝘦𝘰𝘱𝘭𝘦 𝘸𝘩𝘰 𝘥𝘰𝘯'𝘵 𝘬𝘯𝘰𝘸, 𝘩𝘦𝘳𝘦 𝘪𝘴 𝘴𝘰𝘮𝘦 𝘤𝘰𝘯𝘵𝘦𝘹𝘵 𝘢𝘣𝘰𝘶𝘵 𝘵𝘩𝘦 𝘤𝘰𝘶𝘳𝘴𝘦 ↓\n𝗧𝗵𝗲 𝗛𝗮𝗻𝗱𝘀-𝗢𝗻 𝗟𝗟𝗠𝘀 course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a 𝗳𝘂𝗹𝗹𝘆 𝗼𝗽𝗲𝗿𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗽𝗿𝗼𝗱𝘂𝗰𝘁, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== 𝗪𝗵𝗮𝘁 𝗬𝗼𝘂'𝗹𝗹 𝗕𝘂𝗶𝗹𝗱 ===\nWithin the course, you will leverage the 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲, as follows:\n𝟭. 𝗙𝗲𝗮𝘁𝘂𝗿𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll create a system to ingest real-time financial news—crucial for up-to-date advice.\n𝟮. 𝗧𝗿𝗮𝗱𝗶𝗻𝗴 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll fine-tune an LLM to specialize the model in making financial decisions.\n𝟯. 𝗜𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\n𝘊𝘩𝘦𝘤𝘬 𝘪𝘵 𝘰𝘶𝘵 & 𝘨𝘪𝘷𝘦 𝘪𝘵 𝘢 ⭐ ↓\n↳🔗 Hands-On LLMs course:\nhttps://lnkd.in/dZgqtf8f\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQElWHm8-4V6Bg/image-shrink_800/0/1698301818646?e=1705082400&v=beta&t=NeFBCUd-3EbJffNnwHA1eWEslDt4a3DUpTrklfTfo2Y"
        },
        "Post_21": {
            "text": "You must know these 𝟯 𝗺𝗮𝗶𝗻 𝘀𝘁𝗮𝗴𝗲𝘀 of 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗮𝗻 𝗟𝗟𝗠 to train your own 𝗟𝗟𝗠 on your 𝗽𝗿𝗼𝗽𝗿𝗶𝗲𝘁𝗮𝗿𝘆 𝗱𝗮𝘁𝗮.\n# 𝗦𝘁𝗮𝗴𝗲 𝟭: 𝗣𝗿𝗲𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗳𝗼𝗿 𝗰𝗼𝗺𝗽𝗹𝗲𝘁𝗶𝗼𝗻\nYou start with a bear foot randomly initialized LLM.\nThis stage aims to teach the model to spit out tokens. More concretely, based on previous tokens, the model learns to predict the next token with the highest probability.\nFor example, your input to the model is \"The best programming language is ___\", and it will answer, \"The best programming language is Rust.\"\nIntuitively, at this stage, the LLM learns to speak.\n𝘋𝘢𝘵𝘢:  >1 trillion token (~= 15 million books). The data quality doesn't have to be great. Hence, you can scrape data from the internet.\n# 𝗦𝘁𝗮𝗴𝗲 𝟮: 𝗦𝘂𝗽𝗲𝗿𝘃𝗶𝘀𝗲𝗱 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗶𝗻𝗴 (𝗦𝗙𝗧) 𝗳𝗼𝗿 𝗱𝗶𝗮𝗹𝗼𝗴𝘂𝗲\nYou start with the pretrained model from stage 1.\nThis stage aims to teach the model to respond to the user's questions.\nFor example, without this step, when prompting: \"What is the best programming language?\", it has a high probability of creating a series of questions such as: \"What is MLOps? What is MLE? etc.\"\nAs the model mimics the training data, you must fine-tune it on Q&A (questions & answers) data to align the model to respond to questions instead of predicting the following tokens.\nAfter the fine-tuning step, when prompted, \"What is the best programming language?\", it will respond, \"Rust\".\n𝘋𝘢𝘵𝘢: 10K - 100K Q&A example\n𝘕𝘰𝘵𝘦: After aligning the model to respond to questions, you can further single-task fine-tune the model, on Q&A data, on a specific use case to specialize the LLM.\n# 𝗦𝘁𝗮𝗴𝗲 𝟯: 𝗥𝗲𝗶𝗻𝗳𝗼𝗿𝗰𝗲𝗺𝗲𝗻𝘁 𝗹𝗲𝗮𝗿𝗻𝗶𝗻𝗴 𝗳𝗿𝗼𝗺 𝗵𝘂𝗺𝗮𝗻 𝗳𝗲𝗲𝗱𝗯𝗮𝗰𝗸 (𝗥𝗟𝗛𝗙)\nDemonstration data tells the model what kind of responses to give but doesn't tell the model how good or bad a response is.\nThe goal is to align your model with user feedback (what users liked or didn't like) to increase the probability of generating answers that users find helpful.\n𝘙𝘓𝘏𝘍 𝘪𝘴 𝘴𝘱𝘭𝘪𝘵 𝘪𝘯 2:\n1. Using the LLM from stage 2, train a reward model to act as a scoring function using (prompt, winning_response, losing_response) samples (= comparison data). The model will learn to maximize the difference between these 2. After training, this model outputs rewards for (prompt, response) tuples.\n𝘋𝘢𝘵𝘢: 100K - 1M comparisons\n2. Use an RL algorithm (e.g., PPO) to fine-tune the LLM from stage 2. Here, you will use the reward model trained above to give a score for every: (prompt, response). The RL algorithm will align the LLM to generate prompts with higher rewards, increasing the probability of generating responses that users liked.\n𝘋𝘢𝘵𝘢: 10K - 100K prompts\n.\nNote: Post inspired by Chip Huyen's \"RLHF: Reinforcement Learning from Human Feedback\" article: 🔗\nhttps://lnkd.in/dRTFHeFZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQEDEtt1xb7W3Q/image-shrink_800/0/1698129027577?e=1705082400&v=beta&t=O1MqhJko8sj4YYOtVbvxAOddmM8M7SzD4pGSBfEJg54"
        },
        "Post_22": {
            "text": "These are the 𝟴 𝘁𝘆𝗽𝗲𝘀 of 𝗠𝗟𝗢𝗽𝘀 𝘁𝗼𝗼𝗹𝘀 that must be in your toolbelt to be a 𝘀𝘂𝗰𝗰𝗲𝘀𝘀𝗳𝘂𝗹 𝗠𝗟𝗢𝗽𝘀 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿 ↓\nIf you are into MLOps, you are aware of the 1000+ tools in the space and think you have to know.\nThe reality is that all of these tools can be boiled down to 8 main categories.\nIf you learn the fundamentals and master one tool from each category, you will be fine.\n.\nBaşak Tuğçe Eskili\nand\nMaria Vechtomova\nfrom\nMarvelous MLOps\nwrote an excellent summary highlighting these 8 categories:\n1. 𝙑𝙚𝙧𝙨𝙞𝙤𝙣 𝙘𝙤𝙣𝙩𝙧𝙤𝙡: crucial for the traceability and reproducibility of an ML model deployment or run. Without a version control system, it is difficult to find out what exact code version was responsible for specific runs or errors you might have in production. (🔧 GitHub, GitLab, etc.)\n2. 𝘾𝙄/𝘾𝘿: automated tests are triggered upon pull request creation & deployment to production should only occur through the CD pipeline (🔧 GitHub Actions, GitLab CI/CD, Jenkins, etc.)\n3. 𝙒𝙤𝙧𝙠𝙛𝙡𝙤𝙬 𝙤𝙧𝙘𝙝𝙚𝙨𝙩𝙧𝙖𝙩𝙞𝙤𝙣: manage complex dependencies between different tasks, such as data preprocessing, feature engineering, ML model training (🔧  Airflow, ZenML, AWS Step Functions, etc.)\n4. 𝙈𝙤𝙙𝙚𝙡 𝙧𝙚𝙜𝙞𝙨𝙩𝙧𝙮: store, version, and share trained ML model artifacts, together with additional metadata (🔧  Comet ML, W&B, MLFlow, etc.)\n5. 𝘿𝙤𝙘𝙠𝙚𝙧 𝙧𝙚𝙜𝙞𝙨𝙩𝙧𝙮: store, version, and share Docker images. Basically, all your code will be wrapped up in Docker images and shared through this registry (🔧 Docker Hub, ECR, etc.)\n6 & 7. 𝙈𝙤𝙙𝙚𝙡 𝙩𝙧𝙖𝙞𝙣𝙞𝙣𝙜 & 𝙨𝙚𝙧𝙫𝙞𝙣𝙜 𝙞𝙣𝙛𝙧𝙖𝙨𝙩𝙧𝙪𝙘𝙩𝙪𝙧𝙚: if on-premise, you will likely have to go with Kubernetes. There are multiple choices if you are on a cloud provider: Azure ML on Azure, Sagemaker on AWS, and Vertex AI on GCP.\n8. 𝙈𝙤𝙣𝙞𝙩𝙤𝙧𝙞𝙣𝙜: Monitoring in ML systems goes beyond what is needed for monitoring regular software applications. The distinction lies in that the model predictions can fail even if all typical health metrics appear in good condition. (🔧  SageMaker, NannyML, Arize, etc.)\nThe secret sauce in MLOps is knowing how to glue all these pieces together while keeping things simple.\n.\n↳ To read more about these components, check out the\nMarvelous MLOps\narticle: 🔗\nhttps://lnkd.in/dUHRBzDt\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHFvX6aK5CX0Q/image-shrink_800/0/1697005817415?e=1705082400&v=beta&t=1xYpatJcu_GUTmJOQDLgsmkdTkuPe5ySrTiIADN62zQ"
        },
        "Post_23": {
            "text": "This is 𝗵𝗼𝘄 you can 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 to populate a 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 to do 𝗥𝗔𝗚 for a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁 powered by 𝗟𝗟𝗠𝘀.\nIn a previous post, I covered 𝘄𝗵𝘆 you need a streaming pipeline over a batch pipeline when implementing RAG.\nNow, we will focus on the 𝗵𝗼𝘄, aka implementation details ↓\n🐝 All the following steps are wrapped in\nBytewax\nfunctions and connected in a single streaming pipeline.\n𝗘𝘅𝘁𝗿𝗮𝗰𝘁 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗻𝗲𝘄𝘀 𝗳𝗿𝗼𝗺 𝗔𝗹𝗽𝗮𝗰𝗮\nYou need 2 types of inputs:\n1. A WebSocket API to listen to financial news in real time. This will be used to listen 24/7 for new data and ingest it as soon as it is available.\n2. A RESTful API to ingest historical data in batch mode. When you deploy a fresh vector DB, you must populate it with data between a given range [date_start; date_end].\nYou wrap the ingested HTML document and its metadata in a `pydantic` NewsArticle model to validate its schema.\nRegardless of the input type, the ingested data is the same. Thus, the following steps are the same for both data inputs ↓\n𝗣𝗮𝗿𝘀𝗲 𝘁𝗵𝗲 𝗛𝗧𝗠𝗟 𝗰𝗼𝗻𝘁𝗲𝗻𝘁\nAs the ingested financial news is in HTML, you must extract the text from particular HTML tags.\n`unstructured` makes it as easy as calling `partition_html(document)`, which will recursively return the text within all essential HTML tags.\nThe parsed NewsArticle model is mapped into another `pydantic` model to validate its new schema.\nThe elements of the news article are the headline, summary and full content.\n𝗖𝗹𝗲𝗮𝗻 𝘁𝗵𝗲 𝘁𝗲𝘅𝘁\nNow we have a bunch of text that has to be cleaned. Again, `unstructured` makes things easy. Calling a few functions we clean:\n- the dashes & bullets\n- extra whitespace & trailing punctuation\n- non ascii chars\n- invalid quotes\nFinally, we standardize everything to lowercase.\n𝗖𝗵𝘂𝗻𝗸 𝘁𝗵𝗲 𝘁𝗲𝘅𝘁\nAs the text can exceed the context window of the embedding model, we have to chunk it.\nYet again, `unstructured` provides a valuable function that splits the text based on the tokenized text and expected input length of the embedding model.\nThis strategy is naive, as it doesn't consider the text's structure, such as chapters, paragraphs, etc. As the news is short, this is not an issue, but LangChain provides a `RecursiveCharacterTextSplitter` class that does that if required.\n𝗘𝗺𝗯𝗲𝗱 𝘁𝗵𝗲 𝗰𝗵𝘂𝗻𝗸𝘀\nYou pass all the chunks through an encoder-only model.\nWe have used `all-MiniLM-L6-v2` from `sentence-transformers`, a small model that can run on a CPU and outputs a 384 embedding.\nBut based on the size and complexity of your data, you might need more complex and bigger models.\n𝗟𝗼𝗮𝗱 𝘁𝗵𝗲 𝗱𝗮𝘁𝗮 𝗶𝗻 𝘁𝗵𝗲 𝗤𝗱𝗿𝗮𝗻𝘁 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕\nFinally, you insert the embedded chunks and their metadata into the\nQdrant\nvector DB.\nThe metadata contains the embedded text, the source_url and the publish date.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4D10AQFL1FYbWw2npQ/image-shrink_800/0/1696919418846?e=1705082400&v=beta&t=q9v0_nqPdV4EQminbksTOQ5h8xmUtr9HlAj_nC-aXfM"
        },
        "Post_24": {
            "text": "My 𝗻𝗲𝘄𝘀𝗹𝗲𝘁𝘁𝗲𝗿 just 𝗽𝗮𝘀𝘀𝗲𝗱 1400 subscribers 🎉🎉🎉  If you are into 𝗟𝗟𝗠𝘀, 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕𝘀, and 𝗠𝗟𝗢𝗽𝘀, you will like the 𝗳𝘂𝘁𝘂𝗿𝗲 𝘀𝗲𝗿𝗶𝗲𝘀 I am about to 𝘀𝘁𝗮𝗿𝘁 ↓\n.\n✌️ 𝗙𝗶𝗿𝘀𝘁, I want to thank everybody who reads my newsletter: \"Decoding ML.\" As it is completely free, your engagement is the only thing that motivates me.\n.\n🔥 𝗦𝗲𝗰𝗼𝗻𝗱𝗹𝘆, **𝘦𝘹𝘤𝘪𝘵𝘦𝘥 𝘷𝘰𝘪𝘤𝘦** here are my plans for the \"Decoding ML\" newsletter\nUntil now, the weekly articles were randomly picked from various 𝗠𝗟𝗘, 𝗠𝗟𝗢𝗽𝘀, 𝗗𝗟, and 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝘃𝗲 𝗔𝗜 𝘁𝗼𝗽𝗶𝗰𝘀.\n𝗦𝘁𝗮𝗿𝘁𝗶𝗻𝗴 𝘁𝗵𝗶𝘀 𝘄𝗲𝗲𝗸, based on my 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲, I will 𝗹𝗮𝘂𝗻𝗰𝗵 a 𝘀𝗲𝗿𝗶𝗲𝘀 of 𝘀𝗵𝗼𝗿𝘁 𝗮𝗿𝘁𝗶𝗰𝗹𝗲𝘀 that will teach you how to 𝗱𝗲𝘀𝗶𝗴𝗻, 𝗯𝘂𝗶𝗹𝗱, and 𝗱𝗲𝗽𝗹𝗼𝘆 an 𝗲𝗻𝗱-𝘁𝗼-𝗲𝗻𝗱 𝗟𝗟𝗠 𝘀𝘆𝘀𝘁𝗲𝗺 for a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁.\nIt will cover topics such as:\n- the 3-pipeline / FTI architecture\n- building your own QA dataset\n- fine-tuning an LLM using QLoRA\n- building a streaming pipeline\n- using a vector DB for RAG\n- gluing everything together using LangChain\n- deploying the solution\nThis is not the course itself. It is just an overview of the most essential aspects.\nBut, if you are too busy to take the whole course, these weekly FREE 5-minute lessons are a great way to learn how to build an end-to-end LLM product seamlessly.\n.\n👀 𝗦𝘂𝗯𝘀𝗰𝗿𝗶𝗯𝗲 to start receiving them in your mail ↓\n↳🔗\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n----\n💡 Follow me for daily lessons about ML engineering and MLOps",
            "image": "https://media.licdn.com/dms/image/D4D22AQE42Jts6xW5ZQ/feedshare-shrink_800/0/1696243073549?e=1707350400&v=beta&t=2DUcli8YM5s91a5Wi4Xwn4Vo6TWF6hemcUFLOPHoqO0"
        },
        "Post_25": {
            "text": "𝗪𝗵𝘆 do you need a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻𝘀𝘁𝗲𝗮𝗱 of a 𝗯𝗮𝘁𝗰𝗵 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 when implementing 𝗥𝗔𝗚 in your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀? 𝗪𝗵𝗮𝘁 do you need to 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 for a financial assistant?\n↳ 𝗪𝗵𝘆 𝗱𝗼 𝘆𝗼𝘂 𝗻𝗲𝗲𝗱 𝘁𝗼 𝗯𝘂𝗶𝗹𝗱 𝗮 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻𝘀𝘁𝗲𝗮𝗱 𝗼𝗳 𝗮 𝗯𝗮𝘁𝗰𝗵 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲?\nThe quality of your RAG implementation is as good as the quality & freshness of your data.\nThus, depending on your use case, you have to ask:\n\"How fresh does my data from the vector DB have to be to provide accurate answers?\"\nBut for the best user experience, the data has to be as fresh as possible, aka real-time data.\nFor example, when implementing a financial assistant, being aware of the latest financial news is critical. A new piece of information can completely change the course of your strategy.\nHence, when implementing RAG, one critical aspect is to have your vector DB synced with all your external data sources in real time.\nA batch pipeline will work if your use case accepts a particular delay (e.g., one hour, one day, etc.).\nBut with tools like Bytewax 🐝, building streaming applications becomes much more accessible. So why not aim for the best?\n↳ 𝗪𝗵𝗮𝘁 𝗱𝗼 𝘆𝗼𝘂 𝗻𝗲𝗲𝗱 𝘁𝗼 𝗯𝘂𝗶𝗹𝗱 𝗮 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗳𝗼𝗿 𝗮 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁?\n- A financial news data source exposed through a web socket (e.g., Alpaca)\n- A Python streaming processing framework. For example,\nBytewax\n🐝 is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need the Java ecosystem to implement real-time pipelines anymore.  ↳🔗\nhttps://lnkd.in/dWJytkZ5\n- A Python package to process, clean, and chunk documents. `unstructured` offers a rich set of features that makes parsing HTML documents extremely convenient.\n- An encoder-only language model that maps your chunked documents into embeddings. `setence-transformers` is well integrated with HuggingFace and has a huge list of models of various sizes.\n- A vector DB, where to insert your embeddings and their metadata (e.g., the embedded text, the source_url, the creation date, etc.). For example,\nQdrant\nprovides a rich set of features and a seamless experience.  ↳🔗\nhttps://lnkd.in/d_FA9Bb3\n- A way to deploy your streaming pipeline. Docker + AWS will never disappoint you.\n- A CI/CD pipeline for continuous tests & deployments. GitHub Actions is a great serverless option with a rich ecosystem.\nThis is what you need to build & deploy a streaming pipeline solely in Python 🔥\n.\nWhat is your experience with batch vs. streaming pipelines? Let me know in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n.\n💡 Follow me for daily lessons about ML engineering and MLOps",
            "image": "https://media.licdn.com/dms/image/D4D10AQG5nAhxYnjMLQ/image-shrink_800/0/1696573908831?e=1705082400&v=beta&t=sSLpB124q-OAdtUniLR4cycLRbi57QJzvSoXDmOJrxY"
        },
        "Post_26": {
            "text": "𝗛𝗼𝘄 𝘁𝗼 𝗮𝗱𝗱 𝗿𝗲𝗮𝗹-𝘁𝗶𝗺𝗲 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗮𝗻𝗱 𝗺𝗲𝘁𝗿𝗶𝗰𝘀 to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\n𝗜𝗻 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻, 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗮𝘂𝘁𝗼𝗺𝗮𝘁𝗶𝗰𝗮𝗹𝗹𝘆 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵 𝗶𝗻 𝟯 𝗺𝗮𝗶𝗻 𝘀𝗰𝗲𝗻𝗮𝗿𝗶𝗼𝘀:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\n𝗙𝗼𝗿 𝘂𝘀𝗲 𝗰𝗮𝘀𝗲𝘀 𝟮. 𝗮𝗻𝗱 𝟯. 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗾𝘂𝗶𝗰𝗸𝗹𝘆 𝗰𝗼𝗺𝗽𝘂𝘁𝗲 𝘆𝗼𝘂𝗿 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻 𝘁𝗵𝗲 𝗳𝗼𝗹𝗹𝗼𝘄𝗶𝗻𝗴 𝘄𝗮𝘆:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article:\n↳🔗 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨:\nhttps://lnkd.in/dhqCrGkD\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEbYONqBbq7lg/image-shrink_800/0/1696487427417?e=1705082400&v=beta&t=MKvYXlUNHom2hiPWjK7Br4TpJvS8dwlnaSGSTPT0FrY"
        },
        "Post_27": {
            "text": "𝗧𝗼𝗽 𝟲 𝗠𝗟 𝗣𝗹𝗮𝘁𝗳𝗼𝗿𝗺 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 you must know and use in your ML system.\nHere they are ↓\n#𝟭. 𝗘𝘅𝗽𝗲𝗿𝗶𝗺𝗲𝗻𝘁 𝗧𝗿𝗮𝗰𝗸𝗶𝗻𝗴\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#𝟮. 𝗠𝗲𝘁𝗮𝗱𝗮𝘁𝗮 𝗦𝘁𝗼𝗿𝗲\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#𝟯. 𝗩𝗶𝘀𝘂𝗮𝗹𝗶𝘀𝗮𝘁𝗶𝗼𝗻𝘀\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#𝟰. 𝗥𝗲𝗽𝗼𝗿𝘁𝘀\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#𝟱. 𝗔𝗿𝘁𝗶𝗳𝗮𝗰𝘁𝘀\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#𝟲. 𝗠𝗼𝗱𝗲𝗹 𝗥𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course. Link in the comments ↓\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQHdzHBAZbhnBg/feedshare-shrink_800/0/1696407245195?e=1707350400&v=beta&t=DFGXVyjJd2SvFcgV1Pe_ZXMfM-n4kJ6ckAKx_r6J7-M"
        },
        "Post_28": {
            "text": "This is what you need to know about 𝗰𝗵𝗮𝗶𝗻𝗶𝗻𝗴 𝗽𝗿𝗼𝗺𝗽𝘁𝘀 to 𝗿𝗲𝗱𝘂𝗰𝗲 𝗰𝗼𝘀𝘁𝘀, 𝗶𝗻𝗰𝗿𝗲𝗮𝘀𝗲 𝗮𝗰𝗰𝘂𝗿𝗮𝗰𝘆, 𝗮𝗻𝗱 𝗲𝗮𝘀𝗶𝗹𝘆 𝗱𝗲𝗯𝘂𝗴 your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀.\nHere it is ↓\n𝗖𝗵𝗮𝗶𝗻𝗶𝗻𝗴 𝗽𝗿𝗼𝗺𝗽𝘁𝘀 is an intuitive technique that states that you must split your prompts into multiple calls.\n𝗪𝗵𝘆? 𝗟𝗲𝘁'𝘀 𝘂𝗻𝗱𝗲𝗿𝘀𝘁𝗮𝗻𝗱 𝘁𝗵𝗶𝘀 𝘄𝗶𝘁𝗵 𝘀𝗼𝗺𝗲 𝗮𝗻𝗮𝗹𝗼𝗴𝗶𝗲𝘀.\nWhen cooking, you are following a recipe split into multiple steps. You want to move to the next step only when you know what you have done so far is correct.\n↳ You want every prompt to be simple & focused.\nAnother analogy is between reading all the code in one monolith/god class and using DRY to separate the logic between multiple modules.\n↳ You want to understand & debug every prompt easily.\n.\nChaining prompts is a 𝗽𝗼𝘄𝗲𝗿𝗳𝘂𝗹 𝘁𝗼𝗼𝗹 𝗳𝗼𝗿 𝗯𝘂𝗶𝗹𝗱𝗶𝗻𝗴 𝗮 𝘀𝘁𝗮𝘁𝗲𝗳𝘂𝗹 𝘀𝘆𝘀𝘁𝗲𝗺 where you must take different actions depending on the current state.\nIn other words, you control what happens between 2 chained prompts.\n𝘉𝘺𝘱𝘳𝘰𝘥𝘶𝘤𝘵𝘴 𝘰𝘧 𝘤𝘩𝘢𝘪𝘯𝘪𝘯𝘨 𝘱𝘳𝘰𝘮𝘱𝘵𝘴:\n- increase in accuracy\n- reduce the number of tokens -> lower costs (skips steps of the workflow when not needed)\n- avoid context limitations\n- easier to include a human-in-the-loop -> easier to control, moderate, test & debug\n- use external tools/plugins (web search, API, databases, calculator, etc.)\n.\n𝗘𝘅𝗮𝗺𝗽𝗹𝗲\nYou want to build a virtual assistant to respond to customer service queries.\nInstead of adding in one single prompt the system message, all the available products, and the user inquiry, you can split it into the following:\n1. Use a prompt to extract the products and categories of interest.\n2. Enrich the context only with the products of interest.\n3. Call the LLM for the final answer.\nYou can evolve this example by adding another prompt that classifies the nature of the user inquiry. Based on that, redirect it to billing, technical support, account management, or a general LLM (similar to the complex system of GPT-4).\n.\n𝗧𝗼 𝘀𝘂𝗺𝗺𝗮𝗿𝗶𝘇𝗲:\nInstead of writing a giant prompt that includes multiple steps:\nSplit the god prompt into multiple modular prompts that let you keep track of the state externally and orchestrate the program.\nIn other words, you want modular prompts that you can combine easily (same as in writing standard functions/classes)\n.\nTo 𝗮𝘃𝗼𝗶𝗱 𝗼𝘃𝗲𝗿𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴, use this technique when your prompt contains >= instruction.\nYou can leverage the DRY principle from software -> one prompt = one instruction.\n↳ Tools to chain prompts: LangChain\n↳ Tools to monitor and debug prompts: Comet LLMOps Tools\nLinks in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEwI0DWVEmMVQ/image-shrink_800/0/1696401020225?e=1705082400&v=beta&t=t1_Qh1_2pKp-A6xmbxUOzJgOTXLO7rFjPbweZ3OL-Pc"
        },
        "Post_29": {
            "text": "Want to 𝘀𝘁𝗮𝗿𝘁 𝗹𝗲𝗮𝗿𝗻𝗶𝗻𝗴 to 𝗯𝘂𝗶𝗹𝗱 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻-𝗿𝗲𝗮𝗱𝘆 applications using 𝗟𝗟𝗠𝘀? Then, I want to let you know that Pau and I 𝘀𝘁𝗮𝗿𝘁𝗲𝗱 𝗿𝗲𝗹𝗲𝗮𝘀𝗶𝗻𝗴 𝘁𝗵𝗲 𝘃𝗶𝗱𝗲𝗼 𝗹𝗲𝗰𝘁𝘂𝗿𝗲𝘀 for the FREE 𝗛𝗮𝗻𝗱𝘀-𝗢𝗻 𝗟𝗟𝗠𝘀 𝗰𝗼𝘂𝗿𝘀𝗲.\nThe Hands-On LLMs course is not just another demo of how to make a few predictions in a notebook.\nYou'll walk away with a 𝗳𝘂𝗹𝗹𝘆 𝗼𝗽𝗲𝗿𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗽𝗿𝗼𝗱𝘂𝗰𝘁, leveraging Large Language Models (LLMs) to build a chatbot for financial investment advice.\n=== 𝗪𝗵𝗮𝘁 𝗬𝗼𝘂'𝗹𝗹 𝗕𝘂𝗶𝗹𝗱 ===\nWithin the course, you will leverage the 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲, as follows:\n𝟭. 𝗙𝗲𝗮𝘁𝘂𝗿𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll create a system to ingest real-time financial news—crucial for up-to-date advice.\n𝟮. 𝗧𝗿𝗮𝗱𝗶𝗻𝗴 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll fine-tune an LLM to specialize the model in making financial decisions.\n𝟯. 𝗜𝗻𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗣𝗶𝗽𝗲𝗹𝗶𝗻𝗲: You'll combine all the components and deploy the model as a RESTful API, making your application accessible worldwide.\nThese pipelines will be independently developed, deployed, and scaled, ensuring modular and clean code.\nCheck it out ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGCjY8CTFExMw/image-shrink_800/0/1696314619063?e=1705082400&v=beta&t=f_Jc5oxVRZoju7Rq3qQbHlaIGV3JnIJ31FixFLoJ65M"
        },
        "Post_30": {
            "text": "𝗥𝗔𝗚: 𝘄𝗵𝗮𝘁 problems does it solve, and 𝗵𝗼𝘄 it's integrated into 𝗟𝗟𝗠-𝗽𝗼𝘄𝗲𝗿𝗲𝗱 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀?\nLet's find out ↓\nRAG is a popular strategy when building LLMs to add external data to your prompt.\n=== 𝗣𝗿𝗼𝗯𝗹𝗲𝗺 ===\nWorking with LLMs has 3 main issues:\n1. The world moves fast\nAn LLM learns an internal knowledge base. However, the issue is that its knowledge is limited to its training dataset.\nThe world moves fast. New data flows on the internet every second. Thus, the model's knowledge base can quickly become obsolete.\nOne solution is to fine-tune the model every minute or day...\nIf you have some billions to spend around, go for it.\n2. Hallucinations\nAn LLM  is full of testosterone and likes to be blindly confident.\nEven if the answer looks 100% legit, you can never fully trust it.\n3. Lack of reference links\nIt is hard to trust the response of the LLM if we can't see the source of its decisions.\nEspecially for important decisions (e.g., health, financials)\n=== 𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻 ===\n→ Surprize! It is RAG.\n1. Avoid fine-tuning\nUsing RAG, you use the LLM as a reasoning engine and the external knowledge base as the main memory (e.g., vector DB).\nThe memory is volatile, so you can quickly introduce or remove data.\n2. Avoid hallucinations\nBy forcing the LLM to answer solely based on the given context, the LLM will provide an answer as follows:\n-  use the external data to respond to the user's question if it contains the necessary insights\n- \"I don't know\" if not\n3. Add reference links\nUsing RAG, you can easily track the source of the data and highlight it to the user.\n=== 𝗛𝗼𝘄 𝗱𝗼𝗲𝘀 𝗥𝗔𝗚 𝘄𝗼𝗿𝗸? ===\nLet's say we want to use RAG to build a financial assistant.\n𝘞𝘩𝘢𝘵 𝘥𝘰 𝘸𝘦 𝘯𝘦𝘦𝘥?\n- a data source with historical and real-time financial news (e.g. Alpaca)\n- a stream processing engine (e.g., Bytewax - 🔗\nhttps://lnkd.in/dWJytkZ5\n)\n- an encoder-only model for embedding the documents (e.g., pick one from `sentence-transformers`)\n- a vector DB (e.g., Qdrant - 🔗\nhttps://lnkd.in/d_FA9Bb3\n)\n𝘏𝘰𝘸 𝘥𝘰𝘦𝘴 𝘪𝘵 𝘸𝘰𝘳𝘬?\n↳ On the feature pipeline side:\n1. using Bytewax, you ingest the financial news and clean them\n2. you chunk the news documents and embed them\n3. you insert the embedding of the docs along with their metadata (e.g., the initial text, source_url, etc.) to Qdrant\n↳ On the inference pipeline side:\n4. the user question is embedded (using the same embedding model)\n5. using this embedding, you extract the top K most similar news documents from Qdrant\n6. along with the user question, you inject the necessary metadata from the extracted top K documents into the prompt template (e.g., the text of documents & its source_url)\n7. you pass the whole prompt to the LLM for the final answer\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQGlcGb3FkBzhQ/image-shrink_800/0/1695105033416?e=1705082400&v=beta&t=4RL40LiFU6Ri1telE0z71cQerxXsSF5v1EFh-5xs5sU"
        },
        "Post_31": {
            "text": "Want to learn 𝗠𝗟𝗘 & 𝗠𝗟𝗢𝗽𝘀 in a 𝘀𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝘄𝗮𝘆, for 𝗳𝗿𝗲𝗲, and with 𝗵𝗮𝗻𝗱𝘀-𝗼𝗻 𝗲𝘅𝗮𝗺𝗽𝗹𝗲𝘀?\nThen you should check out my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course.\nIn 𝟮.𝟱 𝗵𝗼𝘂𝗿𝘀 𝗼𝗳 𝗿𝗲𝗮𝗱𝗶𝗻𝗴 & 𝘃𝗶𝗱𝗲𝗼 𝗺𝗮𝘁𝗲𝗿𝗶𝗮𝗹𝘀, you will 𝗹𝗲𝗮𝗿𝗻 𝗵𝗼𝘄 𝘁𝗼:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\n- trade-offs & future improvements discussion\n…where all the pieces are integrated into a single end-to-end ML system that forecasts hourly energy levels across Denmark.\n𝗬𝗼𝘂 𝗰𝗮𝗻 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗰𝗼𝘂𝗿𝘀𝗲 𝗼𝗻:\n➝ 𝘔𝘦𝘥𝘪𝘶𝘮'𝘴 𝘛𝘋𝘚 𝘱𝘶𝘣𝘭𝘪𝘤𝘢𝘵𝘪𝘰𝘯: text tutorials + videos\n➝ 𝘎𝘪𝘵𝘏𝘶𝘣: open-source code + docs\nI published the course on Medium's TDS publication to make it accessible to as many people as people. Thus ↓\n... anyone can learn the fundamentals of MLE & MLOps.\nSo no more excuses. Just go and build your own project 🔥\nCheck it out ↓\n↳🔗 𝘛𝘩𝘦 𝘍𝘶𝘭𝘭 𝘚𝘵𝘢𝘤𝘬 7-𝘚𝘵𝘦𝘱𝘴 𝘔𝘓𝘖𝘱𝘴 𝘍𝘳𝘢𝘮𝘦𝘸𝘰𝘳𝘬:\nhttps://lnkd.in/daShNdjw\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\nmlops\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHCppG0uxa0zA/image-shrink_800/0/1695018620482?e=1705082400&v=beta&t=ueqFxy38kIm_lDispp15acnQKGiCqhs8whC2gLyJq7U"
        },
        "Post_32": {
            "text": "We all know how 𝗺𝗲𝘀𝘀𝘆 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺𝘀 can get. That is where the 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗮𝗿𝗰𝗵𝗶𝘁𝗲𝗰𝘁𝘂𝗿𝗲 𝗸𝗶𝗰𝗸𝘀 𝗶𝗻.\nThe 3-pipeline design is a way to bring structure & modularity to your ML system and improve your MLOps processes.\nThis is how ↓\n=== 𝗣𝗿𝗼𝗯𝗹𝗲𝗺 ===\nDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.\nIn 2022, only 54% of the models get into production. Auch.\nSo what happens?\nSometimes the model is not mature enough, sometimes there are some security risks, but most of the time...\n...the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.\nSo, good processes and a well-defined architecture are as crucial as good tools and models.\n=== 𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻 ===\n𝘛𝘩𝘦 3-𝘱𝘪𝘱𝘦𝘭𝘪𝘯𝘦 𝘢𝘳𝘤𝘩𝘪𝘵𝘦𝘤𝘵𝘶𝘳𝘦.\nFirst, let's understand what the 3-pipeline design is.\nIt is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:\n1. the feature pipeline\n2. the training pipeline\n3. the inference pipeline\n...also known as the Feature/Training/Inference (FTI) architecture.\n.\n#𝟭. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store.\n#𝟮. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained models, which are stored and versioned inside a model registry.\n#𝟯. The inference pipeline takes a given version of the features and trained models and outputs the predictions to a client.\n.\nThis is why the 3-pipeline design is so beautiful:\n- it is intuitive\n- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components\n- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate\n- the ML system has been built with modularity in mind since the beginning\n- the 3 components can easily be divided between multiple teams (if necessary)\n- every component can use the best stack of technologies available for the job\n- every component can be deployed, scaled, and monitored independently\n- the feature pipeline can easily be either batch, streaming or both\nBut the most important benefit is that...\n...by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.\n.\nWhat do you think about the 3-pipeline architecture? Have you used it?\nIf you want to know more about the 3-pipeline design, I recommend this awesome article from\nHopsworks\n↓\n↳🔗 From MLOps to ML Systems with Feature/Training/Inference Pipelines:\nhttps://lnkd.in/dRnhHDdg\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about MLE and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGk0NIAvpKW4g/image-shrink_800/0/1694759426656?e=1705082400&v=beta&t=-gwb-vVolI4vIk5rtzpTQBloeMm8z7qT3JTqFTLR-_s"
        },
        "Post_33": {
            "text": "Want to learn how to 𝗳𝗶𝗻𝗲-𝘁𝘂𝗻𝗲 𝗮𝗻 𝗟𝗟𝗠, build a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲, use a 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕, build a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗯𝗼𝘁 and 𝗱𝗲𝗽𝗹𝗼𝘆 𝗲𝘃𝗲𝗿𝘆𝘁𝗵𝗶𝗻𝗴 using serverless solutions?\nThen maybe you know that I,\nPau Labarta Bajo\nand\nAlexandru Răzvanț 👋\n(one of the best MLEs I know out there in the wild) are working on our 𝗛𝗮𝗻𝗱𝘀-𝗼𝗻 𝗟𝗟𝗠𝘀 𝗳𝗿𝗲𝗲 𝗰𝗼𝘂𝗿𝘀𝗲. If not, now you know.\n→ The course will teach you how to build a 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 powered by 𝗟𝗟𝗠𝘀 leveraging the 𝗠𝗟𝗢𝗽𝘀 𝟯-𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗱𝗲𝘀𝗶𝗴𝗻.\nThe 𝗼𝘂𝘁𝗰𝗼𝗺𝗲 of the product is a 𝗱𝗲𝗽𝗹𝗼𝘆𝗲𝗱 𝗽𝗿𝗼𝗱𝘂𝗰𝘁 that you can already show off.\n... and not another Notebook.\n.\nAs the course is still a 𝘄𝗼𝗿𝗸 𝗶𝗻 𝗽𝗿𝗼𝗴𝗿𝗲𝘀𝘀, we want to 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂 𝘂𝗽𝗱𝗮𝘁𝗲𝗱 on our progress ↓\n↳ Thus, we opened up the 𝗱𝗶𝘀𝗰𝘂𝘀𝘀𝗶𝗼𝗻 𝘁𝗮𝗯 under the course's GitHub Repository, where we will 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂 𝘂𝗽𝗱𝗮𝘁𝗲𝗱 with everything is happening.\n.\nAlso, if you have any 𝗶𝗱𝗲𝗮𝘀, 𝘀𝘂𝗴𝗴𝗲𝘀𝘁𝗶𝗼𝗻𝘀, 𝗾𝘂𝗲𝘀𝘁𝗶𝗼𝗻𝘀 or want to 𝗰𝗵𝗮𝘁, we encourage you to 𝗰𝗿𝗲𝗮𝘁𝗲 𝗮 \"𝗻𝗲𝘄 𝗱𝗶𝘀𝗰𝘂𝘀𝘀𝗶𝗼𝗻\".\n↓ We want the course to fill your real needs ↓\n↳ Hence, if your suggestion fits well with our hands-on course direction, we will consider implementing it.\n.\nCheck it out and leave a ⭐ if you like what you see:\n↳🔗 Hands-on LLMs Course:\nhttps://lnkd.in/dKRmRgfZ\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEP-TMrmwBHrA/image-shrink_800/0/1694673016012?e=1705082400&v=beta&t=YZdMD5iLU2iQdzMt1vf2PwBBLuqVMYUD3OXPBCKO79s"
        },
        "Post_34": {
            "text": "If anyone told you that 𝗠𝗟 or 𝗠𝗟𝗢𝗽𝘀 is 𝗲𝗮𝘀𝘆, they were 𝗿𝗶𝗴𝗵𝘁.\nHere is a simple trick that I learned the hard way ↓\nIf you are in this domain, you already know that everything changes fast:\n- a new tool every month\n- a new model every week\n- a new project every day\nYou know what I did? I stopped caring about all these changes and switched my attention to the real gold.\nWhich is → \"𝗙𝗼𝗰𝘂𝘀 𝗼𝗻 𝘁𝗵𝗲 𝗳𝘂𝗻𝗱𝗮𝗺𝗲𝗻𝘁𝗮𝗹𝘀.\"\n.\nLet me explain ↓\nWhen you constantly chase the latest models (aka FOMO),  you will only have a shallow understanding of that new information (except if you are a genius or already deep into that niche).\nBut the joke's on you. In reality, most of what you think you need to know, you don't.\nSo you won't use what you learned and forget most of it after 1-2 months.\nWhat a waste of time, right?\n.\nBut...\nIf you master the fundamentals of the topic, you want to learn.\nFor example, for deep learning, you have to know:\n- how models are built\n- how they are trained\n- groundbreaking architectures (Resnet, UNet, Transformers, etc.)\n- parallel training\n- deploying a model, etc.\n...when in need (e.g., you just moved on to a new project), you can easily pick up the latest research.\nThus, after you have laid the foundation, it is straightforward to learn SoTA approaches when needed (if needed).\nMost importantly, what you learn will stick with you, and you will have the flexibility to jump from one project to another quickly.\n.\nI am also guilty. I used to FOMO into all kinds of topics until I was honest with myself and admitted I am no Leonardo Da Vinci.\nBut here is what I did and worked well:\n- building projects\n- replicating the implementations of famous papers\n- teaching the subject I want to learn\n... and most importantly, take my time to relax and internalize the information.\n.\nTo conclude:\n- learn ahead only the fundamentals\n- learn the latest trend only when needed\nWhat is your learning strategy? Let me know in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHiVva0h_Lf3w/image-shrink_800/0/1694586613948?e=1705082400&v=beta&t=2rOKnJVx_ly96k12ocHZaoOT342k0-NBj5QDM872Ofc"
        },
        "Post_35": {
            "text": "This is my 𝗳𝗮𝘃𝗼𝗿𝗶𝘁𝗲 𝗱𝗲𝘀𝗶𝗴𝗻 𝗽𝗮𝘁𝘁𝗲𝗿𝗻 that you must know as an ML engineer.\nMost ML engineers completely ignore software design patterns, but let me explain why you should know this one for your machine learning projects ↓\nI am talking about Composite.\nThe Composite pattern is a structural design pattern that helps you compose objects in a tree-like structure.\nLet me explain by starting with the problem.\n𝗣𝗿𝗼𝗯𝗹𝗲𝗺\nLet's say that you want to build an ML pipeline that performs object detection + tracking.\nYou can easily divide it into smaller pipelines, such as:\n1. preprocessing\n2. training | inference\n3. postprocessing\nAlso, these 3 pipelines, in their turn, are split into smaller components.\nLet's say that to speed up the ML pipeline. You want to run everything in parallel if possible.\nThus, depending on the use case, it would be best to have a module to compose components sequentially or in parallel.\n❌ If you don't think this through, your code can quickly transform into spaghetti.\n𝗦𝗼𝗹𝘂𝘁𝗶𝗼𝗻\n✅ Now, the Composite design pattern kicks in.\n-> 𝘛𝘩𝘪𝘴 𝘪𝘴 𝘩𝘰𝘸 𝘺𝘰𝘶 𝘤𝘢𝘯 𝘪𝘮𝘱𝘭𝘦𝘮𝘦𝘯𝘵 𝘵𝘩𝘦 𝘔𝘓 𝘱𝘪𝘱𝘦𝘭𝘪𝘯𝘦 𝘢𝘣𝘰𝘷𝘦 𝘶𝘴𝘪𝘯𝘨 𝘵𝘩𝘦 𝘊𝘰𝘮𝘱𝘰𝘴𝘪𝘵𝘦 𝘱𝘢𝘵𝘵𝘦𝘳𝘯:\n1. Define a standard interface for all the transformations. Let's call it \"Transformation.\"\n2. We create an abstract class called \"AtomicTransformation\" that inherits the \"Transformation\" interface for an atomic transformation.\n3. We implement an abstract class called \"CompositeTransformation\" for running multiple transformations. This class inherits the \"Transformation\" interface but also inputs a list of \"Transformation\" objects as input.\n4. Depending on how you want to call a sequence of transformations, you can inherit the \"CompositeTransformation\" interface and implement classes for:\n- \"SequenceTransformations\"\n- \"ParallelTransformations,\"\n- \"DistributedTransformations,\" etc.\n5. Now, when you want to implement a granular transformation (e.g., normalize the image). You implement the \"AtomicTransformation\" interface.\n6. When you want to glue multiple transformations together, you leverage the \"CompositeTransformation\" classes.\n7. When you call a \"CompositeTransformation\" under the hood, it calls the list of \"Transformation\" objects until it hits an \"AtomicTransformation\" object which will do the actual transformation.\nNote that because both the \"AtomicTransformation\" and \"CompositeTransformation\" inherit the \"Transformation\" interface, you can use them interchangeably, like LEGOs.\nThat is powerful.\nThat is why we all love Sklearn and their \"Pipeline\" interface 🔥\n.\nIf you want to know how to apply other software design patterns in MLE, I left in the comments an interesting article ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ndesignpattern\nhashtag\n#\nmlops\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHWkQTWdDff3Q/image-shrink_800/0/1694500223093?e=1705082400&v=beta&t=SIucLwWC7eEP5m4obCwTaQG9GQ4LPveUofOqcK-5A4g"
        },
        "Post_36": {
            "text": "To successfully use 𝗥𝗔𝗚 in your 𝗟𝗟𝗠 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻𝘀, your 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 must constantly be updated with the latest data.\nHere is how you can implement a 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 to keep your vector DB in sync with your datasets ↓\n.\n𝗥𝗔𝗚 is a popular strategy when building LLMs to add context to your prompt about your private datasets.\nLeveraging your domain data using RAG provides 2 significant benefits:\n- you don't need to fine-tune your model as often (or at all)\n- avoid hallucinations\n.\nOn the 𝗯𝗼𝘁 𝘀𝗶𝗱𝗲, to implement RAG, you have to:\n3. Embed the user's question using an embedding model (e.g., BERT). Use the embedding to query your vector DB and find the most similar vectors using a distance function (e.g., cos similarity).\n4. Get the top N closest vectors and their metadata.\n5. Attach the extracted top N vectors metadata + the chat history to the input prompt.\n6. Pass the prompt to the LLM.\n7. Insert the user question + assistant answer to the chat history.\n.\nBut the question is, 𝗵𝗼𝘄 do you 𝗸𝗲𝗲𝗽 𝘆𝗼𝘂𝗿 𝘃𝗲𝗰𝘁𝗼𝗿 𝗗𝗕 𝘂𝗽 𝘁𝗼 𝗱𝗮𝘁𝗲 𝘄𝗶𝘁𝗵 𝘁𝗵𝗲 𝗹𝗮𝘁𝗲𝘀𝘁 𝗱𝗮𝘁𝗮?\n↳ You need a real-time streaming pipeline.\nHow do you implement it?\nYou need 2 components:\n↳ A streaming processing framework. For example, Bytewax is built in Rust for efficiency and exposes a Python interface for ease of use - you don't need Java to implement real-time pipelines anymore.\n🔗 Bytewax:\nhttps://lnkd.in/dbJDDvKB\n↳ A vector DB. For example, Qdrant provides a rich set of features and a seamless experience.\n🔗 Qdrant:\nhttps://qdrant.tech/\n.\nHere is an example of how to implement a streaming pipeline for financial news ↓\n𝟭. Financial news data source (e.g., Alpaca):\nTo populate your vector DB, you need a historical API (e.g., RESTful API) to add data to your vector DB in batch mode between a desired [start_date, end_date] range. You can tweak the number of workers to parallelize this step as much as possible.\n→ You run this once in the beginning.\nYou need the data exposed under a web socket to ingest news in real time. So, you'll be able to listen to the news and ingest it in your vector DB as soon as they are available.\n→ Listens 24/7 for financial news.\n𝟮. Build the streaming pipeline using Bytewax:\nImplement 2 input connectors for the 2 different types of APIs: RESTful API & web socket.\nThe rest of the steps can be shared between both connectors ↓\n- Clean financial news documents.\n- Chunk the documents.\n- Embed the documents (e.g., using Bert).\n- Insert the embedded documents + their metadata to the vector DB (e.g., Qdrant).\n𝟯-𝟳. When the users ask a financial question, you can leverage RAG with an up-to-date vector DB to search for the latest news in the industry.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4D22AQGnoFVBiprmOg/feedshare-shrink_800/0/1694424095344?e=1707350400&v=beta&t=ulw2PNkbS7g2UkrXvw17jlQ2-6e8s5m2OPvFrMHQXSI"
        },
        "Post_37": {
            "text": "After 1 year, I finally decided to 𝘀𝘁𝗮𝗿𝘁 𝗽𝗼𝘀𝘁𝗶𝗻𝗴 𝗼𝗻 𝗧𝘄𝗶𝘁𝘁𝗲𝗿 or, as others like to call it, 𝗫.\nI took this decision because everybody has a different way of reading and interacting with their socials.\n...and I want everyone to enjoy my content on their favorite platform.\nIt took me a while to make this decision as I was not a Twitter user, but despite what people say, I started using it lately and enjoyed it.\nThus... It just made sense to start posting there, but I must warn you I don't have any followers 👀\nI even bought that stu*** blue ticker to see that I am serious about this 😂\nSo...\nIf you like my content and you are a Twitter/X person ↓\nFollow me on Twitter/X:\n↳ 🔗\nhttps://lnkd.in/d5ad8YSD\n↳ handler: @𝗶𝘂𝘀𝘇𝘁𝗶𝗻𝗽𝗮𝘂𝗹\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience",
            "image": "https://media.licdn.com/dms/image/D4D10AQEO3O4dH2H-zQ/image-shrink_800/0/1692685814516?e=1705082400&v=beta&t=7_pYP7T3fDfKQgTWmUM7TybCiPuB-38XMs_2YVOLv2o"
        },
        "Post_38": {
            "text": "This is how you can build a CI/CD pipeline using GitHub Actions and Docker in just a few lines of code.\nAs an ML/MLOps engineer, you should master serving models by building CI/CD pipelines.\nThe good news is that GitHub Actions + Docker simplifies building a CI/CD pipeline.\n.\n𝗪𝗵𝘆?\n- you can easily trigger jobs when merging various branches\n- the CI/CD jobs run on GitHub's VMs (free)\n- easy to implement: copy & paste pre-made templates + adding credentials\n.\n𝗙𝗼𝗿 𝗲𝘅𝗮𝗺𝗽𝗹𝗲, 𝘁𝗵𝗶𝘀 𝗶𝘀 𝗵𝗼𝘄 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗯𝘂𝗶𝗹𝗱 𝗮 𝗖𝗜 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻 𝟯 𝘀𝗶𝗺𝗽𝗹𝗲 𝘀𝘁𝗲𝗽𝘀:\n#1. The CI pipeline is triggered when you merge your new feature branch into the main branch.\n#2. You log into the Docker Registry (or any other compatible registry such as ECR).\n#3. You build the image. Run your tests (if you have any), and if the tests pass, you push the image into the registry.\n.\n𝗧𝗼 𝗶𝗺𝗽𝗹𝗲𝗺𝗲𝗻𝘁 𝘁𝗵𝗲𝗺 𝘂𝘀𝗶𝗻𝗴 𝗚𝗶𝘁𝗛𝘂𝗯 𝗔𝗰𝘁𝗶𝗼𝗻𝘀, 𝘆𝗼𝘂 𝗵𝗮𝘃𝗲 𝘁𝗼:\n- Dockerize your code\n- search \"CI Template GitHub Actions\" on Google\n- copy-paste the template\n- add your Docker Registry credentials\n...and bam... you are done.\nEasy right? The steps are similar when building your CD pipeline (deploying the new image to production).\nIf you want to see how I used GitHub Actions to build & deploy an ML system to GCP, check out my article from the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ninfrastructure\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFhSY09duAKvQ/image-shrink_800/0/1692599416201?e=1705082400&v=beta&t=C3ewhhj7MLwbqWRj6veAOcUmaBsj4Vi9XGefKx_8Tp8"
        },
        "Post_39": {
            "text": "Here are 3 techniques you must know to evaluate your LLMs quickly.\nManually testing the output of your LLMs is a tedious and painful process → you need to automate it.\nIn generative AI, most of the time, you cannot leverage standard metrics.\nThus, the real question is, how do you evaluate the outputs of an LLM?\nDepending on your problem, here is what you can do ↓\n#𝟭. 𝗦𝘁𝗿𝘂𝗰𝘁𝘂𝗿𝗲𝗱 𝗮𝗻𝘀𝘄𝗲𝗿𝘀 - 𝘆𝗼𝘂 𝗸𝗻𝗼𝘄 𝗲𝘅𝗮𝗰𝘁𝗹𝘆 𝘄𝗵𝗮𝘁 𝘆𝗼𝘂 𝘄𝗮𝗻𝘁 𝘁𝗼 𝗴𝗲𝘁\nEven if you use an LLM to generate text, you can ask it to generate a response in a structured format (e.g., JSON) that can be parsed.\nYou know exactly what you want (e.g., a list of products extracted from the user's question).\nThus, you can easily compare the generated and ideal answers using classic approaches.\nFor example, when extracting the list of products from the user's input, you can do the following:\n- check if the LLM outputs a valid JSON structure\n- use a classic method to compare the generated and real answers\n#𝟮. 𝗡𝗼 \"𝗿𝗶𝗴𝗵𝘁\" 𝗮𝗻𝘀𝘄𝗲𝗿 (𝗲.𝗴., 𝗴𝗲𝗻𝗲𝗿𝗮𝘁𝗶𝗻𝗴 𝗱𝗲𝘀𝗰𝗿𝗶𝗽𝘁𝗶𝗼𝗻𝘀, 𝘀𝘂𝗺𝗺𝗮𝗿𝗶𝗲𝘀, 𝗲𝘁𝗰.)\nWhen generating sentences, the LLM can use different styles, words, etc. Thus, traditional metrics (e.g., BLUE score) are too rigid to be useful.\nYou can leverage another LLM to test the output of our initial LLM. The trick is in what questions to ask.\nWhen testing LLMs, you won't have a big testing split size as you are used to. A set of 10-100 tricky examples usually do the job (it won't be costly).\nHere, we have another 2 sub scenarios:\n↳ 𝟮.𝟭 𝗪𝗵𝗲𝗻 𝘆𝗼𝘂 𝗱𝗼𝗻'𝘁 𝗵𝗮𝘃𝗲 𝗮𝗻 𝗶𝗱𝗲𝗮𝗹 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 𝗰𝗼𝗺𝗽𝗮𝗿𝗲 𝘁𝗵𝗲 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 (𝘆𝗼𝘂 𝗱𝗼𝗻'𝘁 𝗵𝗮𝘃𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵)\nYou don't have access to an expert to write an ideal answer for a given question to compare it to.\nBased on the initial prompt and generated answer, you can compile a set of questions and pass them to an LLM. Usually, these are Y/N questions that you can easily quantify and check the validity of the generated answer.\nThis is known as \"Rubric Evaluation\"\nFor example:\n\"\"\"\n- Is there any disagreement between the response and the context? (Y or N)\n- Count how many questions the user asked. (output a number)\n...\n\"\"\"\nThis strategy is intuitive, as you can ask the LLM any question you are interested in as long it can output a quantifiable answer (Y/N or a number).\n↳ 𝟮.𝟮. 𝗪𝗵𝗲𝗻 𝘆𝗼𝘂 𝗱𝗼 𝗵𝗮𝘃𝗲 𝗮𝗻 𝗶𝗱𝗲𝗮𝗹 𝗮𝗻𝘀𝘄𝗲𝗿 𝘁𝗼 𝗰𝗼𝗺𝗽𝗮𝗿𝗲 𝘁𝗵𝗲 𝗿𝗲𝘀𝗽𝗼𝗻𝘀𝗲 𝘁𝗼 (𝘆𝗼𝘂 𝗵𝗮𝘃𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵)\nWhen you have access to an answer manually created by a group of experts, things are easier.\nYou will use an LLM to compare the generated and ideal answers based on semantics, not structure.\nFor example:\n\"\"\"\n(A) The submitted answer is a subset of the expert answer and entirely consistent.\n...\n(E) The answers differ, but these differences don't matter.\n\"\"\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4E10AQFVvUzUSkKEdw/image-shrink_800/0/1692253818126?e=1705082400&v=beta&t=s9-vu0ns7Xl4RNCiUhX6rg76J8K1I3GNoiiFrQRYdK0"
        },
        "Post_40": {
            "text": "Writing your own ML models is history.\nThe true value is in your data, how you prepare it, and your computer power.\nTo demonstrate my statement. Here is how you can write a Python script to train your LLM at scale in under 5 minutes ↓\n#𝟭. Load your data in JSON format and convert it into a Hugging Dataset\n#𝟮. Use Huggingface to load the LLM and pass it to the SFTTrainer, along with the tokenizer and training & evaluation datasets.\n#𝟯. Wrap your training script with a serverless solution, such as Beam, which quickly lets you access a cluster of GPUs to train large models.\n🚨 As you can see, the secret ingredients are not the LLM but:\n- the amount of data\n- the quality of data\n- how you process the data\n- $$$ for compute power\n- the ability to scale the system\n.\n💡 My advice\n↳ If you don't plan to become an ML researcher, shift your focus from the latest models to your data and infrastructure.\n.\n𝗡𝗼𝘁𝗲: Integrating serverless services, such as Beam, makes the deployment of your training pipeline fast & seamless, leaving you to focus only on the last piece of the puzzle: your data.\n↳🔗 Check out Beam's docs to find out more:\nhttps://lnkd.in/dtu2MWSp\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQGIoYT_PRwkIQ/image-shrink_800/0/1692167420327?e=1705082400&v=beta&t=uCNKgRs0cnILi5Jgui4Fp6zf6ylxsRn9TpoYvMuziLg"
        },
        "Post_41": {
            "text": "𝗝𝗼𝗯 𝗿𝗼𝗹𝗲𝘀 tell you there is just 𝗼𝗻𝗲 𝘁𝘆𝗽𝗲 𝗼𝗳 𝗠𝗟 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴, but there are 𝗮𝗰𝘁𝘂𝗮𝗹𝗹𝘆 𝟯\nHere they are ↓\nThese are the 3 ML engineering personas I found while working with different teams in the industry:\n#𝟭. 𝗥𝗲𝘀𝗲𝗮𝗿𝗰𝗵𝗲𝗿𝘀 𝘂𝗻𝗱𝗲𝗿𝗰𝗼𝘃𝗲𝗿\nThey like to stay in touch with the latest papers, understand the architecture of models, optimize them, run experiments, etc.\nThey are great at picking the best models but not that great at writing clean code and scaling the solution.\n#𝟮. 𝗦𝗪𝗘 𝘂𝗻𝗱𝗲𝗿𝗰𝗼𝘃𝗲𝗿\nThey pretend they read papers but don't (maybe only when they have to). They are more concerned with writing modular code and data quality than the latest hot models. Usually, these are the \"data-centric\" people.\nThey are great at writing clean code & processing data at scale but lack deep mathematical skills to develop complex DL solutions.\n#𝟯. 𝗠𝗟𝗢𝗽𝘀 𝗳𝗿𝗲𝗮𝗸𝘀\nThey ultimately don't care about the latest research & hot models. They are more into the latest MLOps tools and building ML systems. They love to automate everything and use as many tools as possible.\nGreat at scaling the solution and building ML pipelines, but not great at running experiments & tweaking ML models. They love to treat the ML model as a black box.\n.\nI started as #1. , until I realized I hated it - now I am a mix of:\n→ #𝟭. 20%\n→ #𝟮. 40%\n→ #𝟯. 40%\nBut that doesn't mean one is better - these types are complementary.\nA great ML team should have at least one of each persona.\nWhat do you think? Did I get it right?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQG4Bc3gA-HP5A/image-shrink_800/0/1692081017812?e=1705082400&v=beta&t=nu3M1J9RgpkB0AziriXGZZdeiCcWr0imJrLnLTZj9So"
        },
        "Post_42": {
            "text": "What is the 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲 between your 𝗠𝗟 𝗱𝗲𝘃𝗲𝗹𝗼𝗽𝗺𝗲𝗻𝘁 and 𝗰𝗼𝗻𝘁𝗶𝗻𝘂𝗼𝘂𝘀 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗲𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁𝘀?\nThey might do the same thing, but their design is entirely different ↓\n𝗠𝗟 𝗗𝗲𝘃𝗲𝗹𝗼𝗽𝗺𝗲𝗻𝘁 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁\nAt this point, your main goal is to ingest the raw and preprocessed data through versioned artifacts (or a feature store), analyze it & generate as many experiments as possible to find the best:\n- model\n- hyperparameters\n- augmentations\nBased on your business requirements, you must maximize some specific metrics, find the best latency-accuracy trade-offs, etc.\nYou will use an experiment tracker to compare all these experiments.\nAfter you settle on the best one, the output of your ML development environment will be:\n- a new version of the code\n- a new version of the configuration artifact\nHere is where the research happens. Thus, you need flexibility.\nThat is why we decouple it from the rest of the ML systems through artifacts (data, config, & code artifacts).\n𝗖𝗼𝗻𝘁𝗶𝗻𝘂𝗼𝘂𝘀 𝗧𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗘𝗻𝘃𝗶𝗿𝗼𝗻𝗺𝗲𝗻𝘁\nHere is where you want to take the data, code, and config artifacts and:\n- train the model on all the required data\n- output a staging versioned model artifact\n- test the staging model artifact\n- if the test passes, label it as the new production model artifact\n- deploy it to the inference services\nA common strategy is to build a CI/CD pipeline that (e.g., using GitHub Actions):\n- builds a docker image from the code artifact (e.g., triggered manually or when a new artifact version is created)\n- start the training pipeline inside the docker container that pulls the feature and config artifacts and outputs the staging model artifact\n- manually look over the training report -> If everything went fine, manually trigger the testing pipeline\n- manually look over the testing report -> if everything worked fine (e.g., the model is better than the previous one), manually trigger the CD pipeline that deploys the new model to your inference services\nNote how the model registry quickly helps you to decouple all the components.\nAlso, because training and testing metrics are not always black & white, it is tough to 100% automate the CI/CD pipeline.\nThus, you need a human in the loop when deploying ML models.\nTo conclude...\nThe ML development environment is where you do your research to find better models:\n- 𝘪𝘯𝘱𝘶𝘵: data artifact\n- 𝘰𝘶𝘵𝘱𝘶𝘵: code & config artifacts\nThe continuous training environment is used to train & test the production model at scale:\n- 𝘪𝘯𝘱𝘶𝘵: data, code, config artifacts\n- 𝘰𝘶𝘵𝘱𝘶𝘵: model artifact\nThis is not a fixed solution, as ML systems are still an open question. I would love to see your opinion in the comments. ↓\n.\nBut if you want to see this strategy in action,\nCheck out my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE Course.\nLink in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning",
            "image": "https://media.licdn.com/dms/image/D4D10AQHkGDkmdTfbxA/image-shrink_800/0/1691994731716?e=1705082400&v=beta&t=5Ob3Dy8O4KvKsezzvFWlqpen5xjsOYcgvVKbPoEzTvE"
        },
        "Post_43": {
            "text": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways you can transform your data:\n𝟏. 𝐁𝐞𝐟𝐨𝐫𝐞 𝐬𝐭𝐨𝐫𝐢𝐧𝐠 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐢𝐧 𝐭𝐡𝐞 𝐟𝐞𝐚𝐭𝐮𝐫𝐞 𝐬𝐭𝐨𝐫𝐞\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\n🟢 ultra-low latency\n🔴 hard to do EDA on transformed data\n🔴 store duplicated/redundant data\n𝟐. 𝐒𝐭𝐨𝐫𝐞 𝐭𝐡𝐞 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐢𝐧 𝐲𝐨𝐮𝐫 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 𝐨𝐫 𝐦𝐨𝐝𝐞𝐥 𝐩𝐫𝐞-𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 𝐥𝐚𝐲𝐞𝐫𝐬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reduction steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🔴 the transformations are done on the client\n𝟑. 𝐘𝐨𝐮 𝐚𝐭𝐭𝐚𝐜𝐡 𝐭𝐨 𝐞𝐯𝐞𝐫𝐲 𝐜𝐥𝐞𝐚𝐧𝐞𝐝 𝐝𝐚𝐭𝐚 𝐬𝐨𝐮𝐫𝐜𝐞 𝐚 𝐔𝐃𝐅 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🟢 the transformations are done on the server\n🟢 scalable (using Spark)\n🔴 hard to implement\nAs a recap,\nThere are 3 ways you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\n.\n↳ To see method #𝟮. in action\nCheck out my 𝘈 𝘎𝘶𝘪𝘥𝘦 𝘵𝘰 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘌𝘧𝘧𝘦𝘤𝘵𝘪𝘷𝘦 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨 𝘗𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘔𝘢𝘹𝘪𝘮𝘶𝘮 𝘙𝘦𝘴𝘶𝘭𝘵𝘴 article\nLink in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHyraJCZsv7-w/image-shrink_800/0/1691735430767?e=1705082400&v=beta&t=43FY2Ra1R3VmJqga4T13tgDdk-IJ6VdwFpHJYB82vv8"
        },
        "Post_44": {
            "text": "𝟳 𝘀𝘁𝗲𝗽𝘀 on how to 𝗰𝗵𝗮𝗶𝗻 your 𝗽𝗿𝗼𝗺𝗽𝘁𝘀 to build a production-ready 𝗳𝗶𝗻𝗮𝗻𝗰𝗶𝗮𝗹 𝗮𝘀𝘀𝗶𝘀𝘁𝗮𝗻𝘁 using 𝗟𝗟𝗠𝘀 ↓\nWhen building LLM applications, you frequently have to divide your application into multiple steps & prompts, which are known as \"chaining prompts\".\nHere are 7 standard steps when building a financial assistant using LLMs (or any other assistant) ↓\n𝗦𝘁𝗲𝗽 𝟭: Check if the user's question is safe using OpenAI's Moderation API\nIf the user's query is safe, move to 𝗦𝘁𝗲𝗽 𝟮 ↓\n𝗦𝘁𝗲𝗽 𝟮: Query your proprietary data (e.g., financial news) to enrich the prompt with fresh data & additional context.\nTo do so, you have to:\n- use an LM to embed the user's input\n- use the embedding to query your proprietary data stored in a vector DB\n𝘕𝘰𝘵𝘦: You must use the same LM model to embed:\n- the data that will be stored in the vector DB\n- the user's question used to query the vector DB\n𝗦𝘁𝗲𝗽 𝟯: Build the prompt using:\n- a predefined template\n- the user's question\n- extracted financial news as context\n- your conversation history as context\n𝗦𝘁𝗲𝗽 𝟰: Call the LLM\n𝗦𝘁𝗲𝗽 𝟱: Check if the assistant's answer is safe using the OpenAI's Moderation API.\nIf the assistant's answer is safe, move to 𝗦𝘁𝗲𝗽 𝟱 ↓\n𝗦𝘁𝗲𝗽 𝟲: Use an LLM to check if the final answer is satisfactory.\nTo do so, you build a prompt using the following:\n- a validation predefined template\n- the user's initial question\n- the assistants answer\nThe LLM has to give a \"yes\" or \"no\" answer.\nThus, if it answers \"yes,\" we show the final answer to the user. Otherwise, we will return a predefined response, such as:\n\"Sorry, we couldn't answer your question because we don't have enough information.\"\n𝗦𝘁𝗲𝗽 𝟳: Add the user's question and assistant's answer to a history cache. Which will be used to enrich the following prompts with the current conversation.\nJust to remind you, the assistant should support a conversation. Thus, it needs to know what happened in the previous questions.\n→ In practice, you usually keep only the latest N (question, answer) tuples or a conversation summary to keep your context length under control.\n.\n↳ If you want to see this strategy in action, check out our new FREE Hands-on LLMs course (work in progress) & give it a ⭐ to stay updated with its latest progress.\nLink in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\ndeeplearning\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHQ6ha-ey95XA/image-shrink_800/0/1691649020280?e=1705082400&v=beta&t=RGO3hWvTXhtqallV3uw2ows6kMD9XQUT6xnSpD-QyPk"
        },
        "Post_45": {
            "text": "Here are 4 ways to monitor and check the output prompts of any LLM to increase the reliability and accuracy of your system.\n#𝟭. 𝗢𝗽𝗲𝗻𝗔𝗣𝗜 𝗠𝗼𝗱𝗲𝗿𝗮𝘁𝗶𝗼𝗻 𝗔𝗣𝗜\nYou can check whether the LLM's answer is harmful with a simple API call. It classifies the prompt as hate, harassment, self-harm, sexual, and violence.\nYou don't want your LLM to become a bully without knowing it.\n#𝟮. 𝗟𝗟𝗠𝗢𝗽𝘀: 𝗠𝗼𝗻𝗶𝘁𝗼𝗿 𝘁𝗵𝗲 𝗽𝗿𝗼𝗺𝗽𝘁𝘀\nOne part of LLMOps is to monitor, track, and see the lineage of all the prompts that come into & out of your system.\nYou can easily do that with Comet ML's LLMOps features. Link in the comments ↓\n#𝟯. 𝗨𝘀𝗲 𝘁𝗵𝗲 𝘀𝗮𝗺𝗲 𝗟𝗟𝗠 𝘁𝗼 𝗰𝗹𝗮𝘀𝘀𝗶𝗳𝘆 𝘁𝗵𝗲 𝗼𝘂𝘁𝗽𝘂𝘁 𝗮𝘀 𝘀𝗮𝘁𝗶𝘀𝗳𝘆𝗶𝗻𝗴 𝗼𝗿 𝗻𝗼𝘁\nAlong with generating text, an LLM can also be used as a classifier (without additional training).\nAfter all, outputting a class can still be considered text generation, right?\nTo do so, you have to:\n- write a system prompt: \"You are an assistant that evaluates ... respond with \"Y\" if the output is sufficient and \"N\" otherwise.\n- add the user question\n- add the LLM answer\n- add the additional context used by the LLM to generate the answers (e.g., a set of product information)\n↳ concatenate everything and pass it to the same LLM...\n... and vualá, you've built a monitoring system that constantly classifies the LLM's answers between satisfying or not.\n#𝟰. 𝗚𝗲𝗻𝗲𝗿𝗮𝘁𝗲 𝗺𝗼𝗿𝗲 𝗮𝗻𝘀𝘄𝗲𝗿𝘀 𝗮𝗻𝗱 𝘂𝘀𝗲 𝘁𝗵𝗲 𝘀𝗮𝗺𝗲 𝗟𝗟𝗠 𝘁𝗼 𝗽𝗶𝗰𝗸 𝘁𝗵𝗲 𝗯𝗲𝘀𝘁 𝗮𝗻𝘀𝘄𝗲𝗿\nQuite self-explanatory.\nAnother option is letting the user pick the best option - a popular strategy for generating stuff.\nA big downside to this strategy is that it adds extra costs.\n.\nSo remember...\nThere are 4 ways to parse your LLM's outputs:\n1. use the OpenAI Moderation API\n2. log them to Comet ML\n3. build a Y/N satisfying classifier\n4. generate more options and pick the best\nHave you used any of these options? Let me know ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndeeplearning\n-----\n💡  Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQHVuWwBp11U0Q/image-shrink_800/0/1690266018546?e=1705082400&v=beta&t=9DgPQZiZNycSLA21fx3WoTdtYJMB1fj4jpml4bSUaZ4"
        },
        "Post_46": {
            "text": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\n↳🔗\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\n↳🔗\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\n↳🔗\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndatascience\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQEJh8LVPrW0KQ/image-shrink_800/0/1689920419727?e=1705082400&v=beta&t=Kn2_tZ5f_fApWKtXEWt17CP9DLkumJ6ejobTApK_dZI"
        },
        "Post_47": {
            "text": "I was uselessly spending 1000$ dollars every month on cloud machines until I started using this tool 👇\nTerraform!\n.\n𝐅𝐢𝐫𝐬𝐭, 𝐥𝐞𝐭'𝐬 𝐮𝐧𝐝𝐞𝐫𝐬𝐭𝐚𝐧𝐝 𝐰𝐡𝐲 𝐰𝐞 𝐧𝐞𝐞𝐝 𝐓𝐞𝐫𝐫𝐚𝐟𝐨𝐫𝐦.\nWhen you want to deploy a software application, there are two main steps:\n1. Provisioning infrastructure\n2. Deploying applications\nA regular workflow would be that before deploying your applications or building your CI/CD pipelines, you manually go and spin up your, let's say, AWS machines.\nInitially, this workflow should be just fine, but there are two scenarios when it could get problematic.\n#1. Your infrastructure gets too big and complicated. Thus, it is cumbersome and might yield bugs in manually replicating it.\n#2. In the world of AI, there are many cases when you want to spin up a GPU machine to train your models, and afterward, you don't need it anymore. Thus, if you forget to close it, you will end up uselessly paying a lot of $$$.\nWith Terraform, you can solve both of these issues.\n.\nSo...\n𝐖𝐡𝐚𝐭 𝐢𝐬 𝐓𝐞𝐫𝐫𝐚𝐟𝐨𝐫𝐦?\nIt sits on the provisioning infrastructure layer as a: \"infrastructure as code\" tool that:\n- is declarative (you focus on the WHAT, not on the HOW)\n- automates and manages your infrastructure\n- is open source\nYeah... yeah... that sounds fancy. But 𝐰𝐡𝐚𝐭 𝐜𝐚𝐧 𝐈 𝐝𝐨 𝐰𝐢𝐭𝐡 𝐢𝐭?\nLet's take AWS as an example, where you have to:\n- create a VPC\n- create AWS users and permissions\n- spin up EC2 machines\n- install programs (e.g., Docker)\n- create a K8s cluster\nUsing Terraform...\nYou can do all that just by providing a configuration file that reflects the state of your infrastructure.\nBasically, it helps you create all the infrastructure you need programmatically. Isn't that awesome?\n.\nIf you want to quickly understand Terraform enough to start using it in your own projects,\n↳ check out my 7-minute read article: 𝘚𝘵𝘰𝘱 𝘔𝘢𝘯𝘶𝘢𝘭𝘭𝘺 𝘊𝘳𝘦𝘢𝘵𝘪𝘯𝘨 𝘠𝘰𝘶𝘳 𝘈𝘞𝘚 𝘐𝘯𝘧𝘳𝘢𝘴𝘵𝘳𝘶𝘤𝘵𝘶𝘳𝘦. 𝘜𝘴𝘦 𝘛𝘦𝘳𝘳𝘢𝘧𝘰𝘳𝘮!\nLink from the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFwiTy6CMuN7A/image-shrink_800/0/1689834015158?e=1705082400&v=beta&t=qWEce49W9R-8Qr265fOPs3DR6fkcppH0iFQy_Ovk8Cg"
        },
        "Post_48": {
            "text": "One strategy that makes the 𝗱𝗶𝗳𝗳𝗲𝗿𝗲𝗻𝗰𝗲 𝗯𝗲𝘁𝘄𝗲𝗲𝗻 𝗴𝗼𝗼𝗱 𝗰𝗼𝗱𝗲 𝗮𝗻𝗱 𝗴𝗿𝗲𝗮𝘁 𝗰𝗼𝗱𝗲 is adding 𝗿𝗲𝘁𝗿𝘆 𝗽𝗼𝗹𝗶𝗰𝗶𝗲𝘀.\nTo manually implement them can get tedious and complicated.\nRetry policies are a must when you:\n- make calls to an external API\n- read from a queue, etc.\n.\n𝗨𝘀𝗶𝗻𝗴 𝘁𝗵𝗲 𝗧𝗲𝗻𝗮𝗰𝗶𝘁𝘆 𝗣𝘆𝘁𝗵𝗼𝗻 𝗽𝗮𝗰𝗸𝗮𝗴𝗲...\n𝘠𝘰𝘶 𝘤𝘢𝘯 𝘲𝘶𝘪𝘤𝘬𝘭𝘺 𝘥𝘦𝘤𝘰𝘳𝘢𝘵𝘦 𝘺𝘰𝘶𝘳 𝘧𝘶𝘯𝘤𝘵𝘪𝘰𝘯𝘴 𝘢𝘯𝘥 𝘢𝘥𝘥 𝘤𝘶𝘴𝘵𝘰𝘮𝘪𝘻𝘢𝘣𝘭𝘦 𝘳𝘦𝘵𝘳𝘺 𝘱𝘰𝘭𝘪𝘤𝘪𝘦𝘴, 𝘴𝘶𝘤𝘩 𝘢𝘴:\n1. Add fixed and random wait times between multiple retries.\n2. Add a maximum number of attempts or computation time.\n3. Retry only when specific errors are thrown (or not thrown).\n... as you can see, you easily compose these policies between them.\nThe cherry on top is that you can access the statistics of the retries of a specific function:\n\"\nprint(raise_my_exception.retry.statistics)\n\"\n.\nWhat is your current strategy for adding retry policies to your Python code?\nhashtag\n#\nmachinelearning\nhashtag\n#\npython\nhashtag\n#\nsoftwareengineering\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHECGHK_U3a1g/image-shrink_800/0/1689661214323?e=1705082400&v=beta&t=Lw8bYVKOpi6qiO6hTOZJItL62FyRJuTKmuo0fHEggQY"
        },
        "Post_49": {
            "text": "𝗛𝗼𝘄 𝘁𝗼 𝗮𝗱𝗱 𝗿𝗲𝗮𝗹-𝘁𝗶𝗺𝗲 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗮𝗻𝗱 𝗺𝗲𝘁𝗿𝗶𝗰𝘀 to your ML system.\nYour model is exposed to performance degradation after it is deployed to production.\nThat is why you need to monitor it constantly.\nThe most common way to monitor an ML model is to compute its metrics.\nBut for that, you need the ground truth.\n𝗜𝗻 𝗽𝗿𝗼𝗱𝘂𝗰𝘁𝗶𝗼𝗻, 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗮𝘂𝘁𝗼𝗺𝗮𝘁𝗶𝗰𝗮𝗹𝗹𝘆 𝗮𝗰𝗰𝗲𝘀𝘀 𝘁𝗵𝗲 𝗴𝗿𝗼𝘂𝗻𝗱 𝘁𝗿𝘂𝘁𝗵 𝗶𝗻 𝟯 𝗺𝗮𝗶𝗻 𝘀𝗰𝗲𝗻𝗮𝗿𝗶𝗼𝘀:\n1. near real-time: you can access it quite quickly\n2. delayed: you can access it after a considerable amount of time (e.g., one month)\n3. never: you have to label the data manually\n.\n𝗙𝗼𝗿 𝘂𝘀𝗲 𝗰𝗮𝘀𝗲𝘀 𝟮. 𝗮𝗻𝗱 𝟯. 𝘆𝗼𝘂 𝗰𝗮𝗻 𝗾𝘂𝗶𝗰𝗸𝗹𝘆 𝗰𝗼𝗺𝗽𝘂𝘁𝗲 𝘆𝗼𝘂𝗿 𝗺𝗼𝗻𝗶𝘁𝗼𝗿𝗶𝗻𝗴 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲 𝗶𝗻 𝘁𝗵𝗲 𝗳𝗼𝗹𝗹𝗼𝘄𝗶𝗻𝗴 𝘄𝗮𝘆:\n- store the model predictions and GT as soon as they are available (these 2 will be out of sync -> you can't compute the metrics right away)\n- build a DAG (e.g., using Airflow) that extracts the predictions & GT computes the metrics in batch mode and loads them into another storage (e.g., GCS)\n- use an orchestration tool to run the DAG in the following scenarios:\n1. scheduled: if the GT is available in near real-time (e.g., hourly), then it makes sense to run your monitoring pipeline based on the known frequency\n2. triggered: if the GT is delayed and you don't know when it may come up, then you can implement a webhook to trigger your monitoring pipeline\n- attach a consumer to your storage to use and display the metrics (e.g., trigger alarms and display them in a dashboard)\n.\nIf you want to see how to implement a near real-time monitoring pipeline using Airflow and GCS, check out my article: 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨. Link in the comments ↓\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQE6nfAE4SVpAw/feedshare-shrink_800/0/1689424500226?e=1707350400&v=beta&t=z77jCyq0jh-myateTVvL2Fv5dGej7wMJVftWnq0y81Y"
        },
        "Post_50": {
            "text": "I wasn't expecting this, but somehow it happened.\nI recently hit 10k+ followers on LinkedIn.\nI will be honest with you guys. That was my goal for the end of 2023.\nBut I don't mind 😂 This was a great surprise for me.\nI have to thank you guys for following me  🙏\nThis motivates me to create more and better content to help you decode ML & MLOps concepts.\nHave a fantastic day, and see you tomorrow 🔥\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata",
            "image": "https://media.licdn.com/dms/image/D4D10AQGvZkacf6SLPA/image-shrink_800/0/1689315014338?e=1705082400&v=beta&t=yOioehrE_wH3ve12D32e2UPD24c47lpOLbJGN6CqQhU"
        },
        "Post_51": {
            "text": "𝗧𝗼𝗽 𝟲 𝗠𝗟 𝗣𝗹𝗮𝘁𝗳𝗼𝗿𝗺 𝗳𝗲𝗮𝘁𝘂𝗿𝗲𝘀 you must know and use in your ML system.\nHere they are ↓\n#𝟭. 𝗘𝘅𝗽𝗲𝗿𝗶𝗺𝗲𝗻𝘁 𝗧𝗿𝗮𝗰𝗸𝗶𝗻𝗴\nIn your ML development phase, you generate lots of experiments.\nTracking and comparing the metrics between them is crucial in finding the optimal model.\n#𝟮. 𝗠𝗲𝘁𝗮𝗱𝗮𝘁𝗮 𝗦𝘁𝗼𝗿𝗲\nIts primary purpose is reproducibility.\nTo know how a model was generated, you need to know:\n- the version of the code\n- the version of the packages\n- hyperparameters/config\n- total compute\n- version of the dataset\n... and more\n#𝟯. 𝗩𝗶𝘀𝘂𝗮𝗹𝗶𝘀𝗮𝘁𝗶𝗼𝗻𝘀\nMost of the time, along with the metrics, you must log a set of visualizations for your experiment.\nSuch as:\n- images\n- videos\n- prompts\n- t-SNE graphs\n- 3D point clouds\n... and more\n#𝟰. 𝗥𝗲𝗽𝗼𝗿𝘁𝘀\nYou don't work in a vacuum.\nYou have to present your work to other colleges or clients.\nA report lets you take the metadata and visualizations from your experiment...\n...and create, deliver and share a targeted presentation for your clients or peers.\n#𝟱. 𝗔𝗿𝘁𝗶𝗳𝗮𝗰𝘁𝘀\nThe most powerful feature out of them all.\nAn artifact is a versioned object that is an input or output for your task.\nEverything can be an artifact, but the most common cases are:\n- data\n- model\n- code\nWrapping your assets around an artifact ensures reproducibility.\nFor example, you wrap your features into an artifact (e.g., features:3.1.2), which you can consume into your ML development step.\nThe ML development step will generate config (e.g., config:1.2.4) and code (e.g., code:1.0.2) artifacts used in the continuous training pipeline.\nDoing so lets you quickly respond to questions such as \"What I used to generate the model?\" and \"What Version?\"\n#𝟲. 𝗠𝗼𝗱𝗲𝗹 𝗥𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nThe model registry is the ultimate way to make your model accessible to your production ecosystem.\nFor example, in your continuous training pipeline, after the model is trained, you load the weights as an artifact into the model registry (e.g., model:1.2.4).\nYou label this model as \"staging\" under a new version and prepare it for testing. If the tests pass, mark it as \"production\" under a new version and prepare it for deployment (e.g., model:2.1.5).\n.\nAll of these features are used in a mature ML system. What is your favorite one?\nYou can see all these features in action in my 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 FREE course. Link in the comments ↓\nhashtag\n#\nmlops\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQE85Agi1BGMAA/image-shrink_800/0/1689228615215?e=1705082400&v=beta&t=HzT3hh9rPEb7qMUdX6Wzjh9w_lxadcDjs5n1f_Vo2ko"
        },
        "Post_52": {
            "text": "Here is how you can 𝗲𝗺𝗯𝗲𝗱 𝗮 𝘀𝗽𝗿𝗲𝗮𝗱𝘀𝗵𝗲𝗲𝘁 𝗱𝗶𝗿𝗲𝗰𝘁𝗹𝘆 𝗶𝗻𝘁𝗼 𝘆𝗼𝘂𝗿 𝗦𝘁𝗿𝗲𝗮𝗺𝗹𝗶𝘁 𝗮𝗽𝗽𝗹𝗶𝗰𝗮𝘁𝗶𝗼𝗻 with just 𝗮 𝗳𝗲𝘄 𝗹𝗶𝗻𝗲𝘀 𝗼𝗳 𝗰𝗼𝗱𝗲.\nFor sure, you heard about\nTry Mito\n.\n𝘐𝘧 𝘯𝘰𝘵, 𝘩𝘦𝘳𝘦 𝘪𝘴 𝘢 𝘲𝘶𝘪𝘤𝘬 𝘳𝘦𝘤𝘢𝘱:\n\"Mito is a way to incorporate an Excel-like component into your beloved notebooks to explore data without writing any Python code (it writes it for you based on your interactions).\"\n.\nRecently, they introduced the same experience, but this time for Streamlit.\nNow you can add a spreadsheet in your dashboard with just a few lines of code:\n\"\"\"\nfrom mitosheet.streamlit.v1 import spreadsheet\n# ... rest of your streamlit app\nspreadsheet()\n\"\"\"\n.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘄𝗵𝗮𝘁 𝗶𝘁 𝗰𝗮𝗻 𝗱𝗼:\n- Import, clean, and transform datasets into a format required by the rest of the Streamlit app.\n- Do flexible data exploration and analysis.\n- Create Python scripts using Mito's code-gen capabilities.\n.\nAccess the docs in the comments ↓\nhashtag\n#\nmachinelearning\nhashtag\n#\ndatascience\nhashtag\n#\ndata\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQH8mAym6SvqHg/image-shrink_800/0/1687242012148?e=1705082400&v=beta&t=8EFx7xxarg74R-sQJMimfPhMH1hcZL2ykQkEtTEaQhU"
        },
        "Post_53": {
            "text": "Looking for a hub where to 𝗹𝗲𝗮𝗿𝗻 𝗮𝗯𝗼𝘂𝘁 𝗠𝗟 𝗲𝗻𝗴𝗶𝗻𝗲𝗲𝗿𝗶𝗻𝗴 𝗮𝗻𝗱 𝗠𝗟𝗢𝗽𝘀 𝗳𝗿𝗼𝗺 𝗿𝗲𝗮𝗹-𝘄𝗼𝗿𝗹𝗱 𝗲𝘅𝗽𝗲𝗿𝗶𝗲𝗻𝗰𝗲?\nThen, I want to let you know that I just launched my personal site, where I will constantly aggregate my:\n- courses\n- articles\n- talks\n...and more\n→ Sweet part: Everything will revolve around MLE & MLOps\nIt is still a work in progress...\nBut please check it out and let me know what you think ↓\nYour opinion is deeply appreciated 🙏\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D22AQGz5m27QZuHGg/feedshare-shrink_800/0/1687236920136?e=1707350400&v=beta&t=Pb65fX_86R5VsAsVtWshtNRzCzT0onebA8oQz-VwsyU"
        },
        "Post_54": {
            "text": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\n.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\n𝐓𝐡𝐞𝐬𝐞 𝐚𝐫𝐞 𝐭𝐡𝐞 𝐦𝐚𝐢𝐧 𝐬𝐭𝐞𝐩𝐬 𝐨𝐟 𝐚 𝐛𝐚𝐭𝐜𝐡 𝐚𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\n.\n𝘛𝘩𝘦 𝘮𝘢𝘪𝘯 𝘥𝘰𝘸𝘯𝘴𝘪𝘥𝘦 of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real-time, and it will update the predictions only at the end of the day.\nMoving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\n.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your usual strategy to serve models is ↓\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFsY8se-q7v6w/image-shrink_800/0/1686896419062?e=1705082400&v=beta&t=YGNi-lOIAne-g3mFn2DkKSjAo6rHpe1etjhLx4XD7bk"
        },
        "Post_55": {
            "text": "Your quick guide on how diffusion models learn to predict your favorite images.\n.\n𝗤𝘂𝗶𝗰𝗸 𝗿𝗲𝗺𝗶𝗻𝗱𝗲𝗿!\nA diffusion model takes a noisy image as input and outputs the noise level from the image.\nAt inference time, you take the input image and subtract the predicted noise from it.\nAlso, A diffusion model is parameterized by a timestamp T that reflects the diffusion process from T to 0.\nThus, for different timestamps, it predicts different levels of noise.\nWhen the timestamp is near T, the model expects noisier images.\nAs it approaches 0, the expected noise level in the image is reduced.\n.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘁𝗵𝗲 𝘁𝗿𝗮𝗶𝗻𝗶𝗻𝗴 𝗹𝗼𝗼𝗽 👇\n1. Sample a training image from the dataset.\n1. Sample timestamp t from the interval [0, T], which determines the noise level.\n2. Sample the noise.\n3. Add the noise to the image based on the sample timestamp t.\n4. Pass it through the diffusion model, which predicts the noise from the image.\n5. Use an MSE loss to compare the predicted noise with the true one.\n6. Use backpropagation to update the model.\n6. Repeat!\nFollowing this training strategy, the model learns to differentiate between the actual information from an image (e.g., the features of a cat) and the noise.\n.\nTo summarize...\nTo train a diffusion model you:\n- add noise to an image based on timestamp t\n- the models learn to predict the noise from timestamp t\n- you use MSE as a loss to compare the real noise with the predicted noise\nhashtag\n#\nmachinelearning\nhashtag\n#\ngenerativeai\nhashtag\n#\nstablediffusion\n-----\n💡 Follow me for daily lessons about ML engineering and MLOps.",
            "image": "https://media.licdn.com/dms/image/D4E10AQEF8BtilstppQ/image-shrink_800/0/1686810013162?e=1705082400&v=beta&t=wRCmrZtbAKf7po6Maa4Wpc1rxOtFeWXArqTSYsN3NRE"
        },
        "Post_56": {
            "text": "Don't know what 𝗙𝗲𝗮𝘁𝘂𝗿𝗲 𝗦𝘁𝗼𝗿𝗲 to use?\nI recommend you check out\nHopsworks\n.\nI had an excellent time using it while implementing 𝗧𝗵𝗲 𝗙𝘂𝗹𝗹 𝗦𝘁𝗮𝗰𝗸 𝟳-𝗦𝘁𝗲𝗽𝘀 𝗠𝗟𝗢𝗽𝘀 𝗙𝗿𝗮𝗺𝗲𝘄𝗼𝗿𝗸 free course.\nThey provide:\n- an intuitive Python package to interface with their platform\n- a robust documentation\n- all the features you need from a feature store\nThey are still a fast-growing company, so the tool is evolving, making it more robust & feature-rich.\nWhile developing the course, I didn't know the team or have any connections with them, but they were kind enough to approach me and send me a gift.\nI love the logo & branding on their items. 🔥\nI guess I have no other option but to drink this weekend 😂\nThank you,\nHopsworks\n!\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nfeaturestore\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFPhgv-9PyAeg/image-shrink_800/0/1686637213150?e=1705082400&v=beta&t=YVhnHy5TUvtxDTvqyIJVFtnwYHK0IYgrQEpAh0cYoT0"
        },
        "Post_57": {
            "text": "𝗨𝗻𝗶𝗳𝘆 𝗯𝗮𝘁𝗰𝗵 𝗮𝗻𝗱 𝘀𝘁𝗿𝗲𝗮𝗺𝗶𝗻𝗴 𝗠𝗟 𝗽𝗶𝗽𝗲𝗹𝗶𝗻𝗲𝘀\nWhat happens if you want to introduce a real-time/streaming data source into your system?\nYou cry. Just kidding. It is a lot easier than it sounds.\nLet's get some context.\nUntil now, you used only a static data source to train your model & compute your features.\nBut you find out that your business wants to use real-time news feeds as features for your model.\n𝗪𝗵𝗮𝘁 𝗱𝗼 𝘆𝗼𝘂 𝗱𝗼?\nYou have to implement 2 𝘮𝘢𝘪𝘯 𝘱𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘺𝘰𝘶𝘳 𝘯𝘦𝘸 𝘴𝘵𝘳𝘦𝘢𝘮𝘪𝘯𝘨 𝘪𝘯𝘱𝘶𝘵 𝘴𝘰𝘶𝘳𝘤𝘦:\n#𝟭. One that will quickly transform the raw data into features and make them accessible into the feature store to be used by the production services.\n#𝟮. One that will store the raw data in the static raw data source (e.g., a warehouse) so it will be used later for experimentation and research.\nBefore ingesting into your system, the real-time data source might need an extra processing step to standardize and adapt the data format to your interface.\nA standard strategy for:\n#𝟭. Kafka as your streaming platform\n#𝟮. Flink/Kafka Streams as your streaming processing units\nFor step #2. most of the time, you will have access to out-of-the-box data connectors that quickly load the real-time data into your static data storage (e.g., from Kafka to an S3 bucket or Big Query data warehouse).\nTo conclude...\nTo add a streaming data source to your current infrastructure, you need the following:\n- Kafka\n- Flink/Kafka Streams\n- to move your streaming data source into your static one\n- to quickly compute features and load them into the feature store\nThus, it isn't hard—just a lot of infrastructure to set up.\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFm1MPAqvvB8Q/image-shrink_800/0/1684306818157?e=1705082400&v=beta&t=glcu9B71RfiBr9fjV8wSi2li6vhEdmB60BGki0H1Tqg"
        },
        "Post_58": {
            "text": "Are you into MLOps and ML Engineering?\nI was honored to pour my MLE & MLOps wisdom into the podcast 𝘓𝘦𝘵'𝘴 𝘛𝘢𝘭𝘬 𝘈𝘐 hosted.\nI had a great time talking with\nThomas Bustos\n, where he had some fantastic questions about:\n- building and engineering AI systems\n- finding your niche in AI\n- different ML job positions\n- Airflow for automating ML\n- deploying multiple versions and communicating effectively\n- explaining technical complexity to customers\n... and more\nIf this sounds like something you are interested in, check it out 👇\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQH6UaaJhK3--Q/image-shrink_800/0/1684221352292?e=1705082400&v=beta&t=L19Jwyd3RGuI5I-yDXgwz-5VntzN659z88kdPDzmAGU"
        },
        "Post_59": {
            "text": "𝗦𝘂𝗽𝗲𝗿𝗰𝗵𝗮𝗿𝗴𝗲 𝘆𝗼𝘂𝗿 𝗠𝗟 𝘀𝘆𝘀𝘁𝗲𝗺: 𝘂𝘀𝗲 𝗮 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆\nA model registry is the holy grail of any production-ready ML system.\nThe model registry is the critical component that decouples your offline pipeline (experimental/research phase) from your production pipeline.\n𝗖𝗼𝗺𝗽𝘂𝘁𝗲 𝗢𝗳𝗳𝗹𝗶𝗻𝗲 𝗙𝗲𝗮𝘁𝘂𝗿𝗲𝘀\nUsually, when training your model, you use a static data source.\nUsing a feature engineering pipeline, you compute the necessary features used to train the model.\nThese features will be stored inside a features store.\nAfter processing your data, your training pipeline creates the training & testing splits and starts training the model.\nThe output of your training pipeline is the trained weights, also known as the model artifact.\n𝗛𝗲𝗿𝗲 𝗶𝘀 𝘄𝗵𝗲𝗿𝗲 𝘁𝗵𝗲 𝗺𝗼𝗱𝗲𝗹 𝗿𝗲𝗴𝗶𝘀𝘁𝗿𝘆 𝗸𝗶𝗰𝗸𝘀 𝗶𝗻 👇\nThis artifact will be pushed into the model registry under a new version that can easily be tracked.\nSince this point, the new model artifact version can be pulled by any serving strategy:\n#1. batch\n#2. request-response\n#3. streaming\nYour inference pipeline doesn’t care how the model artifact was generated. It just has to know what model to use and how to transform the data into features.\nNote that this strategy is independent of the type of model & hardware you use:\n- classic model (Sklearn, XGboost),\n- distributed system (Spark),\n- deep learning model (PyTorch)\nTo summarize...\nUsing a model registry is a simple and effective method to:\n-> detach your experimentation from your production environment,\nregardless of what framework or hardware you use.\nTo learn more, check out my practical & detailed example of how to use a model registry in my article: 𝘈 𝘎𝘶𝘪𝘥𝘦 𝘵𝘰 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘌𝘧𝘧𝘦𝘤𝘵𝘪𝘷𝘦 𝘛𝘳𝘢𝘪𝘯𝘪𝘯𝘨 𝘗𝘪𝘱𝘦𝘭𝘪𝘯𝘦𝘴 𝘧𝘰𝘳 𝘔𝘢𝘹𝘪𝘮𝘶𝘮 𝘙𝘦𝘴𝘶𝘭𝘵𝘴\nLink in the comments 👇\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\n-----\n💡 Follow me if you want to level up in designing ML systems using MLOps good practices.",
            "image": "https://media.licdn.com/dms/image/D4D10AQHL0Zcqj1dXEA/image-shrink_800/0/1683788419175?e=1705082400&v=beta&t=wfbz-WEVU78rH95Vy3FR7yiv4sXv8-6KbwqVf6kkwNk"
        },
        "Post_60": {
            "text": "I usually don’t care about someone's nationality, but damn, I am proud of my country Romania 🎉\nhashtag\n#\nmathematics\nhashtag\n#\nromania",
            "image": "https://media.licdn.com/dms/image/D4D10AQGLL5T6K2SNDA/image-shrink_800/0/1683704670833?e=1705082400&v=beta&t=njHqMcTrrzmGxwBOB7fG_ZCoP46xFeilb9y0MKF2MRw"
        },
        "Post_61": {
            "text": "I never forget anything. Said no one but only your second brain.\nAfter 6+ months of refinement, this is my second brain strategy 👇\nTiago's Forte book inspired me, but I adapted his system to my needs.\n.\n#𝟬. 𝗖𝗼𝗹𝗹𝗲𝗰𝘁\nThis is where you are bombarded with information from all over the place.\n#𝟭. 𝗧𝗵𝗲 𝗚𝗿𝗮𝘃𝗲𝘆𝗮𝗿𝗱\nThis is where I save everything that looks interesting.\nI won't use 90% of what is here, but it satisfied my urge to save that \"cool article\" I saw on LinkedIn.\nTools: Mostly Browser Bookmarks, but I rarely use GitHub stars, Medium lists, etc.\n#𝟮. 𝗧𝗵𝗲 𝗕𝗼𝗮𝗿𝗱\nHere, I start converging the information and planning what to do next.\nTools: Notion\n#𝟯. 𝗧𝗵𝗲 𝗙𝗶𝗲𝗹𝗱\nHere is where I express myself through learning, coding, writing, etc.\nTools: whatever you need to express yourself.\n2 & 3 are iterative processes. Thus I often bounce between them until the information is distilled.\n#𝟰. 𝗧𝗵𝗲 𝗪𝗮𝗿𝗲𝗵𝗼𝘂𝘀𝗲\nHere is where I take the distilled information and write it down for cold storage.\nTools: Notion, Google Drive\n.\nWhen I want to search for a piece of information, I start from the Warehouse and go backward until I find what I need.\nAs a minimalist, I  kept my tools to a minimum. I primarily use only: Brave, Notion, and Google Drive.\nYou don't need 100+ tools to be productive. They just want to take your money from you.\nSo remember...\nYou have to:\n- collect\n- link\n- plan\n- distill\n- store\nWhat is your second brain strategy? Leave your thoughts in the comments.\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain\nhashtag\n#\nmachinelearning\n-----\n💡 Follow me for weekly insights about designing ML systems.",
            "image": "https://media.licdn.com/dms/image/D4D22AQFhN9q0xEvHqA/feedshare-shrink_800/0/1683286221390?e=1707350400&v=beta&t=9ftIcuPqc_gnlCd9tkH4rzyItg7oukmcYSapa9FP7ic"
        },
        "Post_62": {
            "text": "Have you ever searched for yourself on ChatGPT?\nDo you think this is the next generation of hiring and dating?\nThis is what it returned after prompting: \"You are a private detective. Tell me everything you know about Paul Iusztin.\"\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\nchatgpt\n-----\n💡 Follow me for weekly insights about designing ML systems.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFD5wyVmvXyQg/image-shrink_800/0/1683183618617?e=1705082400&v=beta&t=ZlKX0EQtkG1YcTfXVqv2hX-ZeLfsouFyTgZanQfElCc"
        },
        "Post_63": {
            "text": "The Perfect DUO: FastAPI + Streamlit\n2 tools you should know as an ML Engineer\nHere are 2 reasons why FastAPI & Streamlit should be in your MLE stack 👇\n#𝟭. 𝗣𝘆𝘁𝗵𝗼𝗻, 𝗣𝘆𝘁𝗵𝗼𝗻, 𝗣𝘆𝘁𝗵𝗼𝗻!\nAs an MLE, Python is your magic wand.\nUsing FastAPI & Streamlit, you can build full-stack web apps using solely Python.\n#𝟮. 𝗘𝘅𝘁𝗿𝗲𝗺𝗲𝗹𝘆 𝗳𝗹𝗲𝘅𝗶𝗯𝗹𝗲\nUsing FastAPI & Streamlit, you can deploy an ML model in almost any scenario.\n<< 𝘉𝘢𝘵𝘤𝘩 >>\nExpose the predictions from any storage, such as S3 or Redis, using FastAPI as REST endpoints.\nVisualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< 𝘙𝘦𝘲𝘶𝘦𝘴𝘵-𝘙𝘦𝘴𝘱𝘰𝘯𝘴𝘦 >>\nWrap your model using FastAPI and expose its functionalities as REST endpoints.\nYet again... visualize the predictions using Streamlit by calling the FastAPI REST endpoints.\n<< 𝘚𝘵𝘳𝘦𝘢𝘮 >>\nWrap your model using FastAPI and expose it as REST endpoints.\nBut this time, the REST endpoints will be called from a Flink or Kafka Streams microservice.\n.\nUsing this tech stack won't be the most optimal solution in 100% use cases,\n... but in most cases:\n- it will get the job done\n- you can quickly prototype almost any ML application.\n.\nSo remember...\nYou should learn FastAPI & Streamlit because:\n- Python all the way!\n- you can quickly deploy a model in almost any architecture scenario\nDo you use FastAPI & Streamlit?\nTo learn more, check out my Medium article 𝘍𝘢𝘴𝘵𝘈𝘗𝘐 𝘢𝘯𝘥 𝘚𝘵𝘳𝘦𝘢𝘮𝘭𝘪𝘵: 𝘛𝘩𝘦 𝘗𝘺𝘵𝘩𝘰𝘯 𝘋𝘶𝘰 𝘠𝘰𝘶 𝘔𝘶𝘴𝘵 𝘒𝘯𝘰𝘸 𝘈𝘣𝘰𝘶𝘵. Link in the comments 👇\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\npython\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/D4D10AQG0vAyQB1gZeA/image-shrink_800/0/1683097221571?e=1705082400&v=beta&t=UexIMwfQHpZzg-c8R_pdMTUn2F5RE4RpIa6OyCLhKlg"
        },
        "Post_64": {
            "text": "The saddest truth about programming.\nEven amplified if you work in AI.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelliegence\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/D4D10AQFknHQZaDMvoQ/image-shrink_800/0/1680764421146?e=1705082400&v=beta&t=5-MPY9CFT_NwQb_1L5d-D5_HpIjfjtsQKcmyKJUon5A"
        },
        "Post_65": {
            "text": "Close to releasing all the lessons from The Full Stack 7-Steps MLOps Framework.\nhashtag\n#\nlearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/D4D10AQF8ncTNw73s7A/image-shrink_800/0/1680159618403?e=1705082400&v=beta&t=-hfCmObDdAFltKxlsxjnSl0-rTGnf2iz3GZUz8A0x74"
        },
        "Post_66": {
            "text": "As\nPaul Iusztin\n's series continues to explore a 7-step MLOps framework, a new installment explains how to consume and visualize your model's predictions.",
            "image": "https://media.licdn.com/dms/image/D4D10AQFybK57dgdnyQ/image-shrink_1280/0/1680073215153?e=1705082400&v=beta&t=Ne8rem0KQTZ4pXTdcOq0_DvDURsqNR3XsQrDrm0kjCE"
        },
        "Post_67": {
            "text": "I am always anxious that I forget what I read.\nThat is why I built my second brain.\nIf you are working in any data/AI-related field, you are constantly bombarded with information.\nMuch of it is noise,  but some will be of great value.\n.\nHere are the main ideas I extracted from the 𝘉𝘶𝘪𝘭𝘥𝘪𝘯𝘨 𝘢 𝘚𝘦𝘤𝘰𝘯𝘥 𝘉𝘳𝘢𝘪𝘯 𝘣𝘺 𝘛𝘪𝘢𝘨𝘰 𝘍𝘰𝘳𝘵𝘦 famous book 👇\n#𝟭. Your brain's job is to plan, create and enjoy life, not to remember things.\n#𝟮. Four steps to follow:\n1. Capture the relevant ideas\n2. Organize by actionability\n3. Distill your notes to prevent from hoarding\n4. Use & express your work\n#𝟯. Follow the PARA system to organize your information:\nP - Projects: bounded responsibilities\nA - Areas: timeless responsibilities\nR - Resources: useful data\nA - Archive: your graveyard\n#𝟰. Main productivity tools:\n- calendar\n- TODOist\n- note-taking app\n- Notion: the central hub\nThe tools are not revolutionary.\nYou need the right mindset to apply them consistently in the right way.\n.\nTo conclude...\nYou must create a robust system to capture & use all the essential information from your environment.\nOtherwise, you will overload your brain, forgetting important information and wasting opportunities.\nDo you have a second brain?\nhashtag\n#\nproductivity\nhashtag\n#\nsecondbrain",
            "image": "https://media.licdn.com/dms/image/C4D10AQEYvoupSYzw1g/image-shrink_800/0/1678348835498?e=1705082400&v=beta&t=YGcB3s38hVroIz2VxWgq3LjR4S5OFRTiRFM8n3QBf3s"
        },
        "Post_68": {
            "text": "A wise man said: 𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗲 𝗲𝘃𝗲𝗿𝘆𝘁𝗵𝗶𝗻𝗴!\n100% you heard that data validation is good...\nbut where should we validate the data? Everywhere!\nThat might be an overstatement, but let me explain.\nWhen the outputs of an ML model are poor, there are 1000+ reasons why that happened.\nBut even if you know that the issue is data related...\nNarrowing down to the actual function that messed up everything is extremely hard.\nThus, by adding data validation before & after:\n- the ingestion ETL;\n- the data engineering pipeline;\n- the feature engineering pipeline;\nyou might add some redundancy, but this will make scanning for errors extremely easy.\n.\nImagine that you would have a data validation check only after the FE pipeline. If that fails, you know it failed 𝘣𝘶𝘵 𝘥𝘰𝘯'𝘵 𝘬𝘯𝘰𝘸 𝘸𝘩𝘦𝘳𝘦 𝘪𝘵 𝘧𝘢𝘪𝘭𝘦𝘥.\nIf the system is small, that is not an issue, but imagine you have 100+ transformations spread across multiple teams...\n🥲 Finding the right error might take you hours or even days.\n💛 By adding multiple data validation points in your system, you can quickly answer to: \"where the system failed\".\nThus, by adding data validation in multiple, you automatically slice the pipeline making it easy to diagnose.\n.\nNote that this is just an example. Your data infrastructure might look different.\nBut the fundamental idea remains the same. Add data validation in all the essential points of your data pipelines to quickly slice and dice the upcoming errors.\nIf you want a hands-on example about using GE to validate your data, check out my article 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨. Link in the comments 👇\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C4D10AQFnA5xq6M00Jg/image-shrink_800/0/1678089625961?e=1705082400&v=beta&t=kh1pU6ZkncYYICMWcTCDgX46K2CeIwox-yC2Fr4b3dU"
        },
        "Post_69": {
            "text": "Do you want to learn ML & MLOps from real-world experience?\nThen I suggest you join\nPau Labarta Bajo\nReal-World Machine Learning\nweekly newsletter, along with another 7.5 ML developer.\nPau Labarta Bajo\nis a great teacher that makes learning seamless ✌\n→\nhttps://lnkd.in/dXx-GxN8\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/C4D22AQHTedzl38JhtQ/feedshare-shrink_800/0/1677919949980?e=1707350400&v=beta&t=LIuW6QKyYyZIPI5rylNx8ODkYO3_puu5wi4MgU07G2w"
        },
        "Post_70": {
            "text": "𝗗𝗮𝘁𝗮 𝘃𝗮𝗹𝗶𝗱𝗮𝘁𝗶𝗼𝗻 shouldn't be hard.\nHere is your data validation guide in under 2 minutes 👇\nData validation ensures the integrity and quality of your data ingested automatically into your ML system.\nThus, implementing your data validation layer is crucial in any successful ML system.\n.\n🧘🏼‍♂️ 𝘎𝘳𝘦𝘢𝘵 𝘌𝘹𝘱𝘦𝘤𝘵𝘢𝘵𝘪𝘰𝘯𝘴 makes everything straightforward.\nUsing GE, you must stack multiple 𝘌𝘹𝘱𝘦𝘤𝘵𝘢𝘵𝘪𝘰𝘯𝘊𝘰𝘯𝘧𝘪𝘨𝘶𝘳𝘢𝘵𝘪𝘰𝘯 objects, where each object checks a single rule/feature.\nFor example:\n\"\"\"\nExpectationConfiguration(\nexpectation_type=\"expect_column_distinct_values_to_be_in_set\",\nkwargs={\"column\": \"area\", \"value_set\": (0, 1, 2)},\n)\n\"\"\"\n, checks if the \"area\" feature contains only values equal to 0, 1 or 2.\nThe most common checks you have to do are for the following:\n- the schema of the table;\n- the type of each column;\n- the values of each column: an interval for continuous variables or an expected set for discrete variables;\n- null values.\n.\nAfter you run your GE validation suit, you will get a success %.\nBased on the success % you can make various decisions, such as:\n🟢  == 100% - ingest the data without an alert\n🟡  >=90% - ingest the data with an alert\n🔴   <90% - drop the data with an error\nP.S. Using GE +\nHopsworks\nas your Feature Store makes everything even simpler 🔥\n.\nSo remember...\nGE makes implementing your data validation layer straightforward.\nYou have to check every feature for a given set of rules.\nBased on the success % you have to take various actions.\nIf you want a hands-on example about using GE to validate your data, check out my article 𝘌𝘯𝘴𝘶𝘳𝘪𝘯𝘨 𝘛𝘳𝘶𝘴𝘵𝘸𝘰𝘳𝘵𝘩𝘺 𝘔𝘓 𝘚𝘺𝘴𝘵𝘦𝘮𝘴 𝘞𝘪𝘵𝘩 𝘋𝘢𝘵𝘢 𝘝𝘢𝘭𝘪𝘥𝘢𝘵𝘪𝘰𝘯 𝘢𝘯𝘥 𝘙𝘦𝘢𝘭-𝘛𝘪𝘮𝘦 𝘔𝘰𝘯𝘪𝘵𝘰𝘳𝘪𝘯𝘨. Link in the comments 👇\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C5622AQE8-0GY_fbvdg/feedshare-shrink_800/0/1677826092295?e=1707350400&v=beta&t=sz6CMOI_4dj1uAheZYW98aErSI7OzPSy_4rO3cVINVY"
        },
        "Post_71": {
            "text": "Master writing clean & modular Terraform files.\nUsing this one simple technique:\nVariables\nLet's take a look at how to:\n- define a variable\n- reference a variable\n- assign a value to a variable\n- use files to assign values to variables\n... in Terraform.\nNote that assigning values in Terraform is quite strange, as in your Terraform file, you define the structure and type of the variable. The value is assigned only on runtime.\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelliegence\nhashtag\n#\nmlops\n-----\n💡 Follow me if you want to level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHKt-pNZ90O9g/image-shrink_800/0/1676884514444?e=1705082400&v=beta&t=JFgFodA6CsV5ORPHTqrRYCc-CCLFcGPne7m0Z3oxZQY"
        },
        "Post_72": {
            "text": "90% of the ML models start being served in batch mode.\nThus, as an ML engineer, learning to train and serve a model in batch mode successfully is your first step to success.\nYou need to know how to implement the following:\n- a feature engineering pipeline\n- a training pipeline\n- a batch prediction pipeline\n- a feature store\n- an ML Platform\n- a storage system to store your predictions\nMany ML systems start in batch mode and naturally move toward other architectures.\nThis happens because serving an ML model in batch mode is the fastest without spending too much time on constraints such as low latency and high throughput.\nRemember that while in production, if your model isn't running fast enough, it will probably be useless.\nIf you want to learn how to implement the steps above step-by-step practically,\nI want to let you know that...\nI finally released the first 3 lessons for  \"The Full Stack 7-Steps MLOps Framework\" FREE course that will teach you just that.\nThe lessons are accessible on Towards Data Science Medium's publication.\nCheck the comments for the links...\nAnd start learning about training and serving your ML model in batch mode.\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/C4D22AQEB4t8RjoaRsw/feedshare-shrink_800/0/1676127628660?e=1707350400&v=beta&t=KBk3hv2ojdxmDW80fvGwUSjt2Y41E92FdwuzvPclloQ"
        },
        "Post_73": {
            "text": "Why serving an ML model using a batch architecture is so powerful?\nWhen you first start deploying your ML model, you want an initial end-to-end flow as fast as possible.\nDoing so lets you quickly provide value, get feedback, and even collect data.\nBut here is the catch...\nSuccessfully serving an ML model is tricky as you need many iterations to optimize your model to work in real-time:\n- low latency\n- high throughput\nInitially, serving your model in batch mode is like a hack.\nBy storing the model's predictions in dedicated storage, you automatically move your model from offline mode to a real-time online model.\nThus, you no longer have to care for your model's latency and throughput. The consumer will directly load the predictions from the given storage.\n𝐓𝐡𝐞𝐬𝐞 𝐚𝐫𝐞 𝐭𝐡𝐞 𝐦𝐚𝐢𝐧 𝐬𝐭𝐞𝐩𝐬 𝐨𝐟 𝐚 𝐛𝐚𝐭𝐜𝐡 𝐚𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞:\n- extracts raw data from a real data source\n- clean, validate, and aggregate the raw data within a feature pipeline\n- load the cleaned data into a feature store\n- experiment to find the best model + transformations using the data from the feature store\n- upload the best model from the training pipeline into the model registry\n- inside a batch prediction pipeline, use the best model from the model registry to compute the predictions\n- store the predictions in some storage\n- the consumer will download the predictions from the storage\n- repeat the whole process hourly, daily, weekly, etc. (it depends on your context)\nThe main downside of deploying your model in batch mode is that the predictions will have a level of lag.\nFor example, in a recommender system, if you make your predictions daily, it won't capture a user's behavior in real time, and it will update the predictions only at the end of the day.\nThat is why moving to other architectures, such as request-response or streaming, will be natural after your system matures in batch mode.\nSo remember, when you initially deploy your model, using a batch mode architecture will be your best shot for a good user experience.\nLet me know in the comments what your strategy is.\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 My goal is to help machine learning engineers level up in designing and productionizing ML systems.\n👉 Follow me for weekly insights.",
            "image": "https://media.licdn.com/dms/image/C4D10AQF1jBJF8PzmRQ/image-shrink_800/0/1676625314479?e=1705082400&v=beta&t=mE2kLlpXPQOKYmkSYCMrlhxqOsvZdutIWiSVT70Xxzc"
        },
        "Post_74": {
            "text": "These are 3 ways you didn't know about how you can transform your data when using a feature store.\nA feature store helps you quickly solve the training serving skew issue by offering you a consistent way to transform your data into features between the training and inference pipelines.\nThe issue boils down to WHEN you do the transformation.\nWhen using a feature store, there are 3 main ways how you can transform your data:\n𝟏. 𝐁𝐞𝐟𝐨𝐫𝐞 𝐬𝐭𝐨𝐫𝐢𝐧𝐠 𝐭𝐡𝐞 𝐝𝐚𝐭𝐚 𝐢𝐧 𝐭𝐡𝐞 𝐟𝐞𝐚𝐭𝐮𝐫𝐞 𝐬𝐭𝐨𝐫𝐞\nIn the feature engineering pipeline, you do everything: clean, validate, aggregate, reduce, and transform your data.\nEven if this is the most intuitive way of doing things, it is the worse.\n🟢 ultra-low latency\n🔴 hard to do EDA on transformed data\n🔴 store duplicated/redundant data\n𝟐. 𝐒𝐭𝐨𝐫𝐞 𝐭𝐡𝐞 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧 𝐢𝐧 𝐲𝐨𝐮𝐫 𝐩𝐢𝐩𝐞𝐥𝐢𝐧𝐞 𝐨𝐫 𝐦𝐨𝐝𝐞𝐥 𝐩𝐫𝐞-𝐩𝐫𝐨𝐜𝐞𝐬𝐬𝐢𝐧𝐠 𝐥𝐚𝐲𝐞𝐫𝐬\nIn the feature engineering pipeline, you perform only the cleaning, validation, aggregations, and reductions steps.\nLater, by incorporating all your transformations into your pipeline object or pre-processing layers, you automatically save them along your model.\nThus, you can input your cleaned data into your pipeline, and it will know how to handle it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🔴 the transformations are done on the client\n𝟑. 𝐘𝐨𝐮 𝐚𝐭𝐭𝐚𝐜𝐡 𝐭𝐨 𝐞𝐯𝐞𝐫𝐲 𝐜𝐥𝐞𝐚𝐧𝐞𝐝 𝐝𝐚𝐭𝐚 𝐬𝐨𝐮𝐫𝐜𝐞 𝐚 𝐔𝐃𝐅 𝐭𝐫𝐚𝐧𝐬𝐟𝐨𝐫𝐦𝐚𝐭𝐢𝐨𝐧\nThis is similar to solution 2., but instead of attaching the transformation directly to your model, you attached them as a UDF to the feature store.\nfeature = cleaned data source + UDF\nSo when you request a feature, the feature store will automatically trigger the UDF on a server and return it.\n🟢 store only cleaned data\n🟢 easily explore your data\n🟢 the transformations are done on the server\n🟢 scalable (using Spark)\n🔴 hard to implement\nAs a recap,\nThere are 3 ways when you can perform your transformations to solve the train serving skew when using a feature store.\nWhat method do you think is the best?\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\nI just started my ML engineering weekly newsletter.\nSubscribe to level up in building ML systems:\nhttps://lnkd.in/dsMR4ivA",
            "image": "https://media.licdn.com/dms/image/C4D22AQHm2wmWIwnJ7Q/feedshare-shrink_800/0/1676121104276?e=1707350400&v=beta&t=h6L-7qXJOca3jNcnQQxKYHJspKUnNfRbnncruyNiFQo"
        },
        "Post_75": {
            "text": "After 3 months of overthinking...\nI finally did it.\nAs I value people's time, and I understand that everybody has their way of reading...\nI want to announce,\nThat I will start my free weekly ML engineering newsletter.\n.\nThe mission of my newsletter will be the same as my LinkedIn content:\n\"To help machine learning engineers level up in designing and productionizing ML systems.\"\nI will do my best to provide the best value for your time.\n.\nIf you never want to miss my weekly insights about ML engineering,\nSubscribe to my free newsletter here:\nhttps://lnkd.in/dsMR4ivA\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops",
            "image": "https://media.licdn.com/dms/image/C4D10AQHuTXQlOvxAUw/image-shrink_800/0/1676282268515?e=1705082400&v=beta&t=4pyghbq-GoBA615bvVg_Nze1OskhDuc_W3bg_DAg8Kc"
        },
        "Post_76": {
            "text": "In the last month, I read 100+ ML monitoring articles.\nI trimmed them for you to 3 key resources:\n1. A series of excellent articles made by\nArize AI\nthat will make you understand what ML monitoring is all about.\nLink:\nhttps://lnkd.in/dDVWRujh\n2. The\nEvidently AI\nBlog, where you can find answers to all your questions regarding ML monitoring.\nLink:\nhttps://lnkd.in/du35hWp2\n3. The monitoring hands-on examples hosted by\nDataTalksClub\nwill teach you how to implement an ML monitoring system.\nLink:\nhttps://lnkd.in/d4ziHhxH\nAfter wasting a lot of time reading other resources...\nUsing these 3 resources is a solid start for learning about monitoring ML systems.\nHave you tried them?\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmonitoring\n-----\n💡 My goal is to help machine learning engineers level up in designing and productionizing ML systems.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHq3HFws2E6SA/image-shrink_800/0/1674730816012?e=1705082400&v=beta&t=6jjEb928g4_a4MASWyIPGmKIWA1bOcvxTqhc6FLZvww"
        },
        "Post_77": {
            "text": "My dreams came true ✌️\nWhile on vacation, I had the wonderful surprise of passing 5k followers on LinkedIn.\nAs a side note, Amsterdam is a beautiful place, and it 100% deserves to be on most of the go-to tourist destinations.\nFirst of all, I want to thank everybody who is reading my content. I am doing my best to provide you with value about ML engineering, building ML systems and MLOps.\nSecondly, I want to keep the tradition of sharing the key steps that got me here:\n1. Consistency when you have <10 likes\nWhen you are just starting, you probably haven’t built trust around your online persona.\nThus most likely, you will be writing into the void.\nJust don’t get discouraged. At some point, your content will gain traction.\n2. Just start and adapt along the way\nIt is hard to find your best niche, plan, etc., without any real feedback.\nStop overthinking and start posting.\nBut always listen and adapt to see what suits you and your audience better.\nAlways learn ✌️\nSo...\nConsistency, regardless of your engagement and taking action when you think you don’t know what you are doing are my two best pieces of advice for building your brand on LinkedIn.\nWhat do you think about my advice? Would you add something else?\nhashtag\n#\nmachinelearning\nhashtag\n#\nlinkedin\nhashtag\n#\nbrand\n---\nIf you want to learn more about ML engineering, ML systems, and MLOps...\nFollow me on LinkedIn and Medium 👇",
            "image": "https://media.licdn.com/dms/image/C4D10AQH0My-ZT05H0w/image-shrink_800/0/1674725410167?e=1705082400&v=beta&t=sxe1SfdxO8aUJDe3SnnQZQFU7tzBIgPTxXh8NAa4c3E"
        },
        "Post_78": {
            "text": "This is one MLOps practice you 100% have to know.\nMany engineers ignore it,\nbut let me explain why it is so important.\nI showed you one post before about how to extract an embedding from your model.\nI showed you a few examples of extraction methods for various models.\nBut what happens if you want to change the extraction method or model to compare the performance?\nMost probably, it will soon become a mess.\nWe all encountered situations such as: \"final_model,\" \"best_final_model,\" \"best_final_final_model,\" etc. You get the idea... It is tough to keep track of our changes.\n.\n3 types of changes can occur when extracting embeddings:\n#𝟏. 𝐂𝐡𝐚𝐧𝐠𝐞 𝐲𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥 𝐚𝐫𝐜𝐡𝐢𝐭𝐞𝐜𝐭𝐮𝐫𝐞\nThis is considered a major change: O.x.x\nChanging your model architecture might change the semantics of the embeddings and their dimensionality. Also, as a by-product, it changes your extraction method, and you must retrain your model.\n#𝟐. 𝐂𝐡𝐚𝐧𝐠𝐞 𝐲𝐨𝐮𝐫 𝐞𝐱𝐭𝐫𝐚𝐜𝐭𝐢𝐨𝐧 𝐭𝐲𝐩𝐞\nThis is considered a minor change: x.O.x\nAgain this might result in changes in your semantics or dimensionality, but you don't have to retrain your model.\n#𝟑. 𝐑𝐞𝐭𝐫𝐚𝐢𝐧 𝐲𝐨𝐮𝐫 𝐦𝐨𝐝𝐞𝐥\nThis is considered a patch version change: x.x.O\nThis won't change your embedding structure, but by retraining, they won't be compatible with your old set of embeddings as the vector space might change.\n.\nAs you see, your embeddings will change quite often, that is why you need to...\nVersion your data!\nData versioning is one key aspect of a clean ML system.\nEvery change will result in a new data version. Then, when you use a specific set of embeddings, you will know exactly how they were computed.\nYou can easily version your data directly in the feature store for structured data. You can quickly add data versioning for unstructured data using tools such as S3 + DVC/your custom software.\nTo conclude...\nYou should fire up a new version of your data whenever you change your data process.\nHow are you versioning your data?\nhashtag\n#\ndata\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\n-----\n💡 My goal is to help machine learning engineers level up in designing and productionizing ML systems.\n👉 Follow me for weekly insights.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHsdra6RmOgHA/image-shrink_1280/0/1674558018513?e=1705082400&v=beta&t=yRYMDXlH4BaV-A7I__HL72qJcGqCPbp7gEDSHYyrHIk"
        },
        "Post_79": {
            "text": "After 3 months of hard work and no sleep...\nI can finally say that I know everything about MLOps.\nHehe, not really, but I have put in a lot of work and...\nI finally finished my hands-on \"Full Stack 7-Steps MLOps Framework\" series.\nThis a step-by-step course that will explain to you how to design, implement, and deploy an ML system using MLOps good practices.\nAt the end of the 7 lessons course, you will know how to:\n- design a batch-serving architecture\n- use Hopsworks as a feature store\n- design a feature engineering pipeline that reads data from an API\n- build a training pipeline with hyper-parameter tunning\n- use W&B as an ML Platform to track your experiments, models, and metadata\n- implement a batch prediction pipeline\n- use Poetry to build your own Python packages\n- deploy your own private PyPi server\n- orchestrate everything with Airflow\n- use the predictions to code a web app using FastAPI and Streamlit\n- use Docker to containerize your code\n- use Great Expectations to ensure data validation and integrity\n- monitor the performance of the predictions over time\n- deploy everything to GCP\n- build a CI/CD pipeline using GitHub Actions\nIf this sounds interesting to you.\nI want you to know that...\nThe course is free and published on Medium under the\nTowards Data Science\npublication so that anybody can level up their ML engineering journey.\nWant to start building your project?\nJust check out the GitHub repository and the first lesson of the series in the comments.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nlearning\n-----\nI want to thank @Kurtis Pykes for helping me build this series and being an excellent copilot. It was a great collaboration.",
            "image": "https://media.licdn.com/dms/image/C4D10AQE5QnWPZfsOkQ/image-shrink_1280/0/1673515816117?e=1705082400&v=beta&t=4wfPxD2ZsU9ZBhpeXZOY4N4eqSr4T2voGYWH4EbYYNI"
        },
        "Post_80": {
            "text": "ML is more than training, evaluating and beating metrics.\nTo keep your model healthy while in production, you must carefully validate and monitor your data.\nThese are the 5 main criteria you have to be careful about.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nmonitoring",
            "image": "https://media.licdn.com/dms/image/C4D10AQFCnb9VmoV23A/image-shrink_800/0/1673257519840?e=1705082400&v=beta&t=q46DldIbZmoUrdiysGV0gZ6hRHgJrgH3AyvA6uKNstE"
        },
        "Post_81": {
            "text": "My recommender infrastructure was a mess until I started using these 2 key components.\n#1 Feature Store\nYou can efficiently query features anywhere in the online pipeline using a feature store. Thus your online application shouldn't bother anymore with cleaning, validation, and feature engineering. All of that is delegated to the feature store.\nTherefore the feature store solves the train-serve skew and makes your features available by exposing them in a low latency key-value database such as Redis or RocksDB.\n#2 Swapped NN with ANN\nAs the users x items space might get to the magnitude of millions, querying items using a naive NN approach will probably break your system. That is why you must use an ANN (Approximate Neireght Neighbor) approach that gives up perfect results in favor of speed. This step should reduce your space from millions to thousands of candidates. Hence a coarse but fast search is excellent.\nPopular ANN tools: Faiss, Scann\nhashtag\n#\nrecommendersystem\nhashtag\n#\nml\nhashtag\n#\nmlops\n-----\n💡 My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4D10AQGid1pq6qdwHg/image-shrink_800/0/1670056776109?e=1705082400&v=beta&t=UswSMCZglI6E1OHrsVFGsVglT9wUJf4nlqL8gu08wbg"
        },
        "Post_82": {
            "text": "My last recommender system was so slow that instead of improving the user experience, people stopped using the app.\nIn the real world, user experience is the most critical aspect you must take care of. When it narrows down to ML, balancing performance and speed is the most essential trade-off you must consider.\n.\nIn a recommender system, when we want to query for similar users/items to speed up your system, you should:\n𝐬𝐰𝐢𝐭𝐜𝐡\n❗ nearest neighbors (NN)\n𝐰𝐢𝐭𝐡\n✅ approximate nearest neighbors (ANN)\nRunning NN on millions of records will burn down your system. Read below to understand why.\n.\n𝐖𝐡𝐲?\nWhen building a recommender system, we transform the users and items into a lower-dimensional embedding space. Afterward, we use different distances in the embedding space, e.g., cosine similarity, to find similar relationships: user-item, user-user, and item-item.\nFor example, to find the most similar items for a user, using the nearest neighbors technique, you have to compute the distances between a user and all the items. Imagine how slow this could get when you have millions of items.\n𝐈𝐧𝐬𝐭𝐞𝐚𝐝, 𝐝𝐨 𝐭𝐡𝐢𝐬...\nUsing an approximate nearest neighbors approach, you can find \"approximate\" similar items but in a much faster way. In a recsys setup, that is plenty because the results shouldn't be perfect.\nUsing such a technique, you can quickly reduce your space from millions to thousands/hundreds of items which you can later order and refine.\nCheckout the comments for various packages of ANN implementations 👇\nhashtag\n#\nrecommendersystem\nhashtag\n#\nml\nhashtag\n#\nmlops\n-----\n💡 My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4D10AQHvGSYfleTqBQ/image-shrink_800/0/1669793938941?e=1705082400&v=beta&t=w72OkEylqhijKOGnDnW8AORPbP2kFn6gdvSDGce-ALA"
        },
        "Post_83": {
            "text": "Successfully packaging and deploying a Python module was so tedious and painful until I started using this simple technique\nhashtag\n#\npython\nhashtag\n#\nmlops\nhashtag\n#\nsoftwareengineering",
            "image": "https://media.licdn.com/dms/image/C4D10AQGeWy2QzFKfEw/image-shrink_800/0/1669707787813?e=1705082400&v=beta&t=NZ13E4ejgtF5RiHi_GYfo7SdXPEhR239VtnLrjms4eo"
        },
        "Post_84": {
            "text": "If you are learning all the nitty little details of all the new shiny algorithms that our there, you won't have time to learn how to deploy them to production professionally.\nPick your battles wisely.\nIf you are inclined more towards researching, do 80% research and 20% engineering ML systems.\nIf you are inclined more towards engineering, do 20% research and 80% engineering ML systems.\nIndeed you need to understand both to grasp the whole perspective, but you don't have to master both.\nPick one and be the best at it 🔥\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nlearning\n-----\n💡 My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4D10AQEiWclKmd0tMQ/image-shrink_800/0/1668757490230?e=1705082400&v=beta&t=RUMqj5opBdbsxPpLv6pWSKuH2OfXE8xrunjm65kFq88"
        },
        "Post_85": {
            "text": "How exciting is this? 🔥",
            "image": "https://media.licdn.com/dms/image/C4D10AQHuRhRcs0_oJg/image-shrink_800/0/1668671242174?e=1705082400&v=beta&t=da1NzIRIETJcr7S4Z80-T0DUciL_-QOc6RHAmriSxlY"
        },
        "Post_86": {
            "text": "As you type, a 3D world emerges.\nThis is super cool.\nFrom:\nOpusAI\nPS. Get today's top AI stories in a quick 3-minute digest. Join 33K+ other professionals staying smart on AI:\nhttp://bit.ly/3iKiI10",
            "image": "https://media.licdn.com/dms/image/C4D10AQEE2GmsXlQ2Fg/image-shrink_800/0/1668584550200?e=1705082400&v=beta&t=tnX0Lwr7cw2zHOrjuzN3yqUeAkEWjXJIlwG-CjjpqBw"
        },
        "Post_87": {
            "text": "With all the hype around ChatGPT, let me tell you how, in reality, 𝐈 𝐛𝐨𝐨𝐬𝐭𝐞𝐝 𝐝𝐞𝐯𝐞𝐥𝐨𝐩𝐦𝐞𝐧𝐭 𝐩𝐫𝐨𝐝𝐮𝐜𝐭𝐢𝐯𝐢𝐭𝐲 𝐛𝐲 𝟓𝟎% - Spoiler alert: not by using ChatGPT.\n.\nDo you know ChatGPT has a distant, less famous cousin called GitHub Copilot?\nWell... That is what I am really using in my day-to-day programming experience.\n.\n𝐖𝐡𝐲?\n- it is specialized only in writing code\n- it generates code based on the context of your current codebase\n- it is already integrated into your IDE - you hit tab & bam... you have your code\n- it saves a lot of time in writing tedious boiler code\n- it saves a lot of time in writing tedious boiler documentation 😍\n- it saves a lot of time in searching syntax for various packages\n.\nI read stories about people writing an entire MVP without Google or ChatGPT but using solely GitHub Copilot. How cool is that?\nThat probably won't be the case for you, as you still need Google to look up more complex or subtle topics. But hey, for sure, it will lift the burden of looking up syntax every 5 minutes 😂\n.\n𝐁𝐨𝐧𝐮𝐬 𝐭𝐨𝐨𝐥 - Besides GitHub Copilot, I use 'black' as a formatter that automatically formats my code following PEP8 standards.\nUsing these 2 tools, all the boring part of coding is automated, and I can focus on building awesome stuff 🔥\nhashtag\n#\nproductivity\nhashtag\n#\ndevelopment\nhashtag\n#\nprogramming\n-----\n💡 My goal is to document the ups and downs of my learning journey about ML engineering. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQF0Q3ChUrjdyQ/feedshare-shrink_800/0/1668508686857?e=1707350400&v=beta&t=9uA7pmxxYiCOvVBKc8D98cRkVdzlbtR69QJlsnyh1RA"
        },
        "Post_88": {
            "text": "As a self-taught ML engineer, I used to learn daily and still felt like I didn't know enough.\nAs I love learning, I didn't want to give up, but seeing that information did not stick with me was unfulfilling. Everything changed when I realized and applied this one simple trick 👇\n𝐘𝐨𝐮 𝐬𝐡𝐨𝐮𝐥𝐝 𝐚𝐥𝐰𝐚𝐲𝐬 𝐡𝐚𝐯𝐞 𝐚 𝐜𝐥𝐞𝐚𝐫 𝐠𝐨𝐚𝐥 𝐚𝐧𝐝 𝐩𝐥𝐚𝐧 𝐟𝐨𝐫 𝐰𝐡𝐚𝐭 𝐲𝐨𝐮 𝐰𝐚𝐧𝐭 𝐭𝐨 𝐥𝐞𝐚𝐫𝐧.\n.\n# 𝐖𝐡𝐲 𝐢𝐬 𝐭𝐡𝐢𝐬 𝐚 𝐠𝐚𝐦𝐞-𝐜𝐡𝐚𝐧𝐠𝐞𝐫?\nWell, I love systems 👀 But doing this...\n- you can focus more easily\n- you eliminate distractions more easily (e.g., social media)\n- you eliminate anxiety/impostor syndrome by reducing the number of new concepts that you have to learn\n- you have time to understand the fundamentals that will stick with you in the long run\n- you have time to really understand the topic and not just memorize it\n.\n# 𝐇𝐨𝐰 𝐬𝐡𝐨𝐮𝐥𝐝 𝐲𝐨𝐮 𝐩𝐢𝐜𝐤 𝐲𝐨𝐮𝐫 𝐠𝐨𝐚𝐥?\nWell, this depends on your context, like level of experience, dedication, time, etc. Still, usually, it should be something concrete, something that excites you, and something incremental (go from T to T+1).\n# 𝐇𝐨𝐰 𝐬𝐡𝐨𝐮𝐥𝐝 𝐲𝐨𝐮 𝐩𝐥𝐚𝐧 𝐢𝐭?\nFor example, instead of saying I want to learn about MLOps, make a plan and divide it into smaller sections like:\n- Week 1: Experiment tracking\n- Week 2: Data versioning\n- Week 3: Orchestration\netc...\nThe essential idea is that in the timespan you planned to learn 𝐨𝐧𝐞 𝐭𝐡𝐢𝐧𝐠, you should read articles, watch videos, and code examples only on that one thing. Through this, your brain will work for you and quickly internalize the new concept.\n.\nIf you believe it or not, I was that guy that wanted to learn everything as fast as possible. But that left me only with confusion, anxiety, and a superficial understanding.\nTake your time and enjoy your learning process ✌️\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nlearning\n-----\n💡 My goal is to share the ups and downs of my ML engineering learning journey. Follow me for daily insights about data, ML, and MLOps.",
            "image": "https://media.licdn.com/dms/image/C4E10AQGd4jl0O4vlaw/image-shrink_800/0/1668497957230?e=1705082400&v=beta&t=dmuJJQthGad1V0DsNOwqEUg9cqxrKtTvctJjg5dZVsI"
        },
        "Post_89": {
            "text": "These are the two most common recommendation system algorithms you have to know 🔥.\n#𝟏.  𝐂𝐨𝐥𝐥𝐚𝐛𝐨𝐫𝐚𝐭𝐢𝐯𝐞 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠\nIt is based on the interaction between users and items (likes, ratings, comments, etc.).\nFor example, if Paul likes Python and SQL and Andrew likes Python, SQL, and ML. Then, we can recommend to Paul to look into ML.\nMost common algorithms learn user and items embeddings through:\n- Matrix Factorization  (e.g., SVD)\n- Deep Learning\n#𝟐. 𝐂𝐨𝐧𝐭𝐞𝐧𝐭 𝐅𝐢𝐥𝐭𝐞𝐫𝐢𝐧𝐠\nBased on the items a user likes, it learns with the help of the user's and the item's features to predict if a user might like an unseen item.\nFor example, suppose Paul likes Python and SQL. In that case, we can look at the characteristics of Paul (engineer, young, excited about tech, etc.) and the features of Python & SQL (programming languages, used in data fields, etc.) and find a confident match between Paul and ML.\nA common approach is to use various models (even a simple logistic regression can work) that take as input the features of a user and item and predict the probability that the user might interact with that item.\n-----\n💡 In the coming weeks (maybe months), I will focus my posts on:\n-\nhashtag\n#\nrecommendersystems\n- hands-on examples of\nhashtag\n#\nML\nin production using\nhashtag\n#\nMLOps\ngood practices.\nFollow me if you want to learn more!",
            "image": "https://media.licdn.com/dms/image/C4D10AQGsUBcmioEt0g/image-shrink_800/0/1668066551583?e=1705082400&v=beta&t=ZKsgS33zuo4WoHAIikNZqJeHi5HGHQw1Z9O67SOcgXY"
        },
        "Post_90": {
            "text": "This is one critical teaching I have learned from my last year working as an ML engineer freelancer/contractor 👇\n.\nAs a curious person, I tend to stick my nose in all I can get. But after five years of experience, this left me with a broad knowledge of various domains but without knowing to do a particular thing exceptionally well. I mean extremely well. Like, to be the best at doing that thing.\nUnfortunately, in the world of freelancing/contracting, you are thriving in the spectrum of being the best in doing one thing. This sounds quite niched, but hear out my plan.\n.\n👉 In the coming months, I plan to get very good at building recommendation systems following good MLOps practices. Therefore, I can quickly build, train, deploy, and monitor recommendation systems for future clients, which will let me build trust and, ultimately, a brand around this.\nThus, I can expand my client base around building recommendation systems. Slowly, I will get more real-life experience in building production-ready ML systems. Therefore, at that point, it will be straightforward to learn to solve other problems and expand my skill set.\nMy point is that in the beginning, it is better to focus all your energy on becoming extremely good at one thing. After, you can quickly build on top of that.\n.\nWhy recommendation systems?\nAfter some research, I saw recommendation systems are estimated to have an annual CAGR of >50% (which is huge). Also, intuitively makes a lot of sense, as almost all web applications will need personalization in the future.\nAfter all, we all love when our tech adapts to our needs, right?\nhashtag\n#\ncareer\nhashtag\n#\ncontracting\nhashtag\n#\nfreelancing\nhashtag\n#\nonemanbusiness\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/D4D10AQE0N8YLWnLe1A/image-shrink_800/0/1666853483144?e=1705082400&v=beta&t=VggF0SG6D_fOp_OVMOnlKaOFtVY0SeRg6RG_7QJgXhM"
        },
        "Post_91": {
            "text": "Often a good dashboard is all you need 🔥\nI compiled for you a list of 7 open-source dashboards that you have to know to level up your data game.\nhashtag\n#\ndata\nhashtag\n#\nml\nhashtag\n#\nai",
            "image": "https://media.licdn.com/dms/image/C4D10AQED1LQtWOK6vw/image-shrink_800/0/1666767048482?e=1705082400&v=beta&t=c21rmEBMahyp9FoDMKjjYY7ChfKesUB_liAcEV2Apkc"
        },
        "Post_92": {
            "text": "I recently tested WakaTime, a tool that monitors all my development activity. I must say I love it 🔥.\n.\nWakaTime automatically generates weekly charts for visualizing distributions for:\n- coding time/day\n- projects you have been working on\n- programming languages\n- IDEs\n- OS\n- workstations\n- trends in programming time (don't mind my 100% decrease, I just had a research week where I mostly read and designed stuff 😂)\nYou just have to set up an account on WakaTime and install their plugin in your favorite IDE, and you are done!\n.\nAs a data person, I love to look at charts. Thus, looking at my own developing activity makes it even better.\nCheck the link in the comments 👇\nhashtag\n#\nmonitoring\nhashtag\n#\ndeveloping\nhashtag\n#\nsoftware\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D10AQEUW2tsE4TmmQ/image-shrink_800/0/1666767050051?e=1705082400&v=beta&t=DhSCGX34NX21r21hLcQ6B2f9C3ICFC2Z9QiNvwg9BEQ"
        },
        "Post_93": {
            "text": "I don't see any problem with this solution. Do you? 😂\nhashtag\n#\nai\nhashtag\n#\nml\nhashtag\n#\nclustering",
            "image": "https://media.licdn.com/dms/image/C4D10AQHhzeVU55WNjQ/image-shrink_800/0/1666767049826?e=1705082400&v=beta&t=ApZU_9DBipG6rBaqCTpdv8xyikQqFKUrgsOrCnzZXQM"
        },
        "Post_94": {
            "text": "Black Mirror, is that you?\nhashtag\n#\nai\nhashtag\n#\nml",
            "image": "https://media.licdn.com/dms/image/C4D10AQEpuyTPZX7sVQ/image-shrink_800/0/1666680071415?e=1705082400&v=beta&t=MzDwR2gHAb7Ame0RWwEen5bcwQG6JTjAnjmwweDW4gQ"
        },
        "Post_95": {
            "text": "This AI paper honestly scares me a bit.\nHere's what researchers did:\n- Send people into an MRI\n- Show them an image\n- Measure their brain waves\n- Put their brain waves into an AI image model (Stable Diffusion)\nGuess what happened?\nThe image model was able to reconstruct an image from those brain waves. It's not perfect - almost like a memory of the image.\nIt's as if Stable Diffusion can read minds.\nWhat else can today's language and image models do that we're not expecting?\nbiorxiv: 2022.11.18.517004v2\nPS. Get today's top AI stories in a quick 3-minute digest. Join 18K+ other professionals staying smart on AI:\nhttp://bit.ly/3iKiI10\nhashtag\n#\nartificialintelligence\nhashtag\n#\nmachinelearning\nhashtag\n#\ntechnology",
            "image": "https://media.licdn.com/dms/image/C4D10AQELrotau8bZgA/image-shrink_1280/0/1666248335172?e=1705082400&v=beta&t=j0EWEz4ySJpHTPznWyGyi8syTebZ1wEZ43yjOjmekX0"
        },
        "Post_96": {
            "text": "If you are a perfectionist, you know there is always another function to change, another class to refactor, and another document to be prettified...\nYou know the saying: \"The perfectionist's mind is always in pain.\"\nBut how can we bypass this in the pragmatic world we live in?\nWell... There is another saying that states the following:\n80% of the work is done in 20% of the time.\nThe other 20% of the work is finished in the remaining 80% of the time.\nAs perfectionists, we are often stuck refining that 20% of the work when most of the job is done.\nThat is why I found it extremely helpful to define what 80% of the work would look like and stop there no matter what. Only after I validate my project, idea, prototype, etc., and see a need for further refinement will I continue the other 20% of the work.\nI am not saying to ignore the details. Ultimately, the details differentiate a decent product from a great one. This is a method to prioritize your work and ensure that what you do has real impact and value.\n.\nWhat do you think about this strategy? Would you use it?\nhashtag\n#\nproductivity\nhashtag\n#\nsystem\nhashtag\n#\nperfectionist",
            "image": "https://media.licdn.com/dms/image/C4D10AQGjHH_a-BGrnQ/image-shrink_800/0/1666248334949?e=1705082400&v=beta&t=7FFhm9jzQFAEUW23RUUVYFrBHrKHE534yh-zEHZ7KhM"
        },
        "Post_97": {
            "text": "Have you looked at job descriptions for any Data Scientist/Machine Learning/MLOps role lately?\nAfter reading multiple JD and discussing various positions, I realized that the requirements for most data positions are a mess...\nFor many companies, the boundaries between various roles are practically inexistent, which results in one size fits all people and unrealistic expectations.\nFor example, a company wants an ML engineer that has the following qualifications:\n- 5 years of experience in writing data pipelines\n- 7 years of experience in training models\n- 10 years of experience in deploying models\nBut, If you look closely, all three skills are part of different roles. I am not saying this isn't good. I love working in interdisciplinary roles. But sometimes, navigating the data world and understanding what to learn and improve is difficult.\nI believe that this confusion might be because of the ML/MLOps engineer's emerging roles or because some European companies realized that they are behind in the data-centric movement and are just starting to integrate data people into their teams. In the end, this field is moving so quickly that sometimes it is hard to keep track of what is happening.\nDisclaimer: I am not saying this happens 100% of the time. I just saw a general trend in this confusion and wanted to hear your opinion about it.\n.\nIt is incredibly satisfying to work on a novel field and contribute to its way to maturity, but sometimes things might get confusing. That is why I believe hearing other people's opinions are helpful.\nThus I would love to hear your thoughts on this topic.\nWhat do you think? How was your experience in finding a job that fits you well? Do you believe that companies have realistic expectations and know what they want for their new data people hires?\nhashtag\n#\njobs\nhashtag\n#\ncareer\nhashtag\n#\nlearning",
            "image": "https://media.licdn.com/dms/image/C4D10AQGu0LHIBXJ-vg/image-shrink_800/0/1666161018881?e=1705082400&v=beta&t=7AHHWs7llTFN93_OhoB9HwXChnpVIuj8uDK8Y876pCg"
        },
        "Post_98": {
            "text": "Have you ever wondered where you can quickly learn more about various software design patterns?\n.\nYou can read about dozens of software design patterns on Refactoring Guru:\nhttps://lnkd.in/dVUKymMT\nThey provide intuitive examples and code snippets to understand various concepts practically.\nAlso, they have support for all your beloved programming languages: Python, Java, C, TypeScript, etc.\nhashtag\n#\ndesignpatterns\nhashtag\n#\nsoftwareengineering\nhashtag\n#\narchitecture\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D10AQFkE0gWn0mbVg/image-shrink_800/0/1666077124300?e=1705082400&v=beta&t=InVMvu9XaP5whV2uLq4sPvKCNz_K40RHHpQvIww7hMY"
        },
        "Post_99": {
            "text": "This visualization could replace your OOP class 😂\nhashtag\n#\nsoftwareengineer\nhashtag\n#\noop",
            "image": "https://media.licdn.com/dms/image/C5610AQEQ52CL8R7cqw/image-shrink_800/0/1664435735000?e=1705082400&v=beta&t=nEZITBV3LL7ejok4b6o8bldWcAEk5-QjkCTw7T_wlRg"
        },
        "Post_100": {
            "text": "Visualization is an art form: pass by reference vs pass by value.\n↓\nCheck out\nhttps://AlphaSignal.ai\nto get a weekly summary of the top breakthroughs in Machine Learning.\n-\ncredit penjee",
            "image": "https://media.licdn.com/dms/image/C5610AQEXz-YTeDxwvQ/image-shrink_800/0/1664346623847?e=1705082400&v=beta&t=fHwqAAonLjCl8UxlZ06S8quApvvU9G5h4l8Lkjj5vDU"
        },
        "Post_101": {
            "text": "Have you ever wondered where to start learning MLOps?\n.\nYou can find lots of valuable information by checking out the MLOps organization site:\nhttps://ml-ops.org/\nAlso, you can find hundreds of essential references by following this GitHub repository:\nhttps://lnkd.in/dxknAmTK\nNow you have no excuse to start learning MLOps!\nhashtag\n#\nmlops\nhashtag\n#\ncontent\nhashtag\n#\nlearning\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E10AQGXJoIqehDqSw/image-shrink_800/0/1664263660845?e=1705082400&v=beta&t=pRmC3SJcHQGHyslduqUadLkX136YQ27zgqZgArMLcPk"
        },
        "Post_102": {
            "text": "Who fell for it raise your hand 😂😂😂",
            "image": "https://media.licdn.com/dms/image/C4E22AQF6te4xqmKLqw/feedshare-shrink_800/0/1664009611443?e=1707350400&v=beta&t=DK1uCGSJwya9t7en3C8eSLm7qC2eun_xsJ2im1N-p4M"
        },
        "Post_103": {
            "text": "This is so hilarious 🤭😂😂😂\nCredit\nCorey Quinn\nhashtag\n#\nAWS\nhashtag\n#\nawscloud\nhashtag\n#\nawscloudpractitioner\nhashtag\n#\nmachinelearning\nhashtag\n#\nnew\nhashtag\n#\nbrad",
            "image": "https://media.licdn.com/dms/image/C4E22AQEsH5BoNQUwmw/feedshare-shrink_800/0/1664009611520?e=1707350400&v=beta&t=4IF19D8c7j96tmq0fRBv7Am_UzXfm5eDacxmOBlfF8A"
        },
        "Post_104": {
            "text": "These are 3 characteristics showing that a feature store is a crucial component within any end-to-end ML production system.\n.\n#𝟏 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭\nFeature management is the ability to discover and share features across teams. This characteristic may save time and computation as multiple features can be shared across models. Also, it helps share knowledge across teams or projects.\n#𝟐 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐜𝐨𝐦𝐩𝐮𝐭𝐚𝐭𝐢𝐨𝐧\nFeature computation means you compute your features only once and store them for future uses. In this case, it acts like a data warehouse, and it may save a lot of time and computation, especially when computing specific features consume a lot of resources (usually the case).\n#𝟑 𝐅𝐞𝐚𝐭𝐮𝐫𝐞 𝐜𝐨𝐧𝐬𝐢𝐬𝐭𝐞𝐧𝐜𝐲\nFeature consistency refers to the problem of having two separate pipelines for the model between training and production (known as the training-serving skew issue). Using a feature store, you unify the logic of feature creation, using the feature store both in training and production.\n.\nI am curious to know what tools you use for your feature store solutions. Please Let me know if the comments below 👇\nhashtag\n#\nfeaturestore\nhashtag\n#\ndatawarehouse\nhashtag\n#\nmlops\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFKE41Cb2RxTA/feedshare-shrink_800/0/1664009611481?e=1707350400&v=beta&t=_q8ocLIGlMT32vm3NjjqmFEqg3Sota7sQXHVfWit2eA"
        },
        "Post_105": {
            "text": "𝐖𝐨𝐫𝐤𝐟𝐥𝐨𝐰 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭/𝐬𝐜𝐡𝐞𝐝𝐮𝐥𝐞𝐫𝐬 and 𝐨𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐢𝐨𝐧 𝐭𝐨𝐨𝐥𝐬 are two critical elements of any successful MLOps infrastructure. But what are they used for, and what are some vital differences between them?\n.\n#𝟏 𝐖𝐨𝐫𝐤𝐟𝐥𝐨𝐰 𝐦𝐚𝐧𝐚𝐠𝐞𝐦𝐞𝐧𝐭 𝐭𝐨𝐨𝐥𝐬 (𝐨𝐫 𝐬𝐜𝐡𝐞𝐝𝐮𝐥𝐞𝐫𝐬)\nWorkflow management tools are concerned with when to run jobs and what resources are needed to run those jobs.\nSchedulers are based on job-type abstractions: DAGs or priority queues. The dependencies between multiple jobs are defined within a YAML or Python file. For complex applications, this is essential because, for example, if job A fails, you don't want to execute job B. Another example would be that if job A fails, retry to run it 5 times until you give up.\nSchedulers are used for periodic jobs.\nPopular tools: Airflow, Argo, Prefect, Kubeflow, Metaflow\n#𝟐 𝐎𝐫𝐜𝐡𝐞𝐬𝐭𝐫𝐚𝐭𝐢𝐨𝐧 𝐭𝐨𝐨𝐥𝐬\nOrchestration tools concern where to get resources to run the jobs and how to scale the resources to run the jobs efficiently.\nOrchestration tools deal with lower-level attractions: machines, instances, or clusters. They manage the hardware resources on top of which you run your system.\nOrchestration tools are responsible for scaling the computational power size to efficiently support the given jobs (e.g., if the number of jobs is more significant than the number of instances, it will spin up more instances).\nOrchestration tools are used for long-running servers that serve clients' requests.\nPopular tools: Kubernetes (K8s)\n.\nThese tools are usually used interchangeably. Schedulers typically run on top of orchestration tools.\nhashtag\n#\ntools\nhashtag\n#\nschedulers\nhashtag\n#\norchestration\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQE0VhXfdWO0vQ/feedshare-shrink_800/0/1664009611111?e=1707350400&v=beta&t=CSibfshNVK-YCVmKcm2pVwVwEVxYxz0deWl3Gq_spuU"
        },
        "Post_106": {
            "text": "You don't need a feature store...\nSaid no one after he understood why using one is so essential for any ML system.\nHere is why 👇\nhashtag\n#\nmlops\nhashtag\n#\nml\nhashtag\n#\nfeaturestore",
            "image": "https://media.licdn.com/dms/image/C4E22AQFwQNEtNZfEoQ/feedshare-shrink_800/0/1664009611450?e=1707350400&v=beta&t=x-aNerQvGJAIqf8we19cSAgKZnrLIUeFQJUb_JD8Q28"
        },
        "Post_107": {
            "text": "Top 3 tools to keep your Python code professional without any additional effort 👇\n.\n#𝟏. 𝐁𝐥𝐚𝐜𝐤\nA tool for automatically formatting your code.\nCheck it out:\nhttps://lnkd.in/dR_RdjT9\n#𝟐. 𝐅𝐥𝐚𝐤𝐞𝟖\nA tool that checks and validates your coding style.\nCheck it out:\nhttps://lnkd.in/dBmyuECV\n#𝟑. 𝐑𝐞𝐟𝐮𝐫𝐛\nA tool for automatically refurbishing and modernizing your Python codebases.\nCheck it out:\nhttps://lnkd.in/d67J9hbt\n.\nAn important note is that you can automate the trigger of these tools using pre-commit. Thus, these tools will be triggered every time you run a commit.\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\ncodingstyle\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQEtU9tJZCedcQ/feedshare-shrink_2048_1536/0/1663822911250?e=1707350400&v=beta&t=G-LJmPj4io6STcp7ek--sMOzIzDyZnBh70OYgsyWtrs"
        },
        "Post_108": {
            "text": "I compiled a list of the top 10 metrics your ML monitoring system should track.\n.\n#𝟏 𝐎𝐩𝐞𝐫𝐚𝐭𝐢𝐨𝐧𝐚𝐥 𝐦𝐞𝐭𝐫𝐢𝐜𝐬\nUsed to monitor the health of your overall software system.\n➝ latency\n➝ throughput\n➝ CPU/GPU utilization\n➝ memory utilization\n➝ the number of requests your model receives in the last X minutes/hours/days\n➝ the number of successful requests\n#𝟐 𝐌𝐋-𝐬𝐩𝐞𝐜𝐢𝐟𝐢𝐜 𝐦𝐞𝐭𝐫𝐢𝐜𝐬\nUsed to monitor the performance of your ML system.\n➝ accuracy (extremely useful when you have feedback or natural labels from the user: click rate, upvote, downvote, purchases, bookmarks, views, etc.)\n➝ predictions (as predictions are low dimensional various statistics are easy to compute + the distribution of the prediction represents a proxy for the input distribution)\n➝ features (feature validation + two-sample tests for drift detection)\n➝ raw inputs (these are harder to monitor due to their scattered nature within the infrastructure)\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D10AQFF13WkaXvq9A/image-shrink_800/0/1663828711420?e=1705082400&v=beta&t=H45_IFogt0_nFieeIKL0x_wrMzUC2eQgWl3wxSFrSLM"
        },
        "Post_109": {
            "text": "I compiled a table with the top 8 most demanding domains in AI/ML. I want to understand in which field to specialize further as a freelancer. Thus I wanted to take a data-centric approach in making this decision. I used the market size as the 'demand' metric because where the money flows represents a strong signal of demand in that field.\nNote that the absolute values in $$$ might not be 100% accurate compared as they might be extracted from different years, such as 2020-2021-2022. The most interesting column is 𝐂𝐀𝐆𝐑 % which shows the potential growth in that field. If there is growth, there is new demand for engineers in that field.\nThe market size within the table consists only of the AI/ML part of that domain (i.e., the entire energy domain is way bigger than 3.82$ Billion). Also, some domains might cross each other, so the Global Machine Learning values won't add to the rest of the fields.\n.\nWhat do you think? Do you believe that this table is relevant? Did I miss any domains? What field in AI/ML do you think has the most potential?\nI am trying to figure out the answers to these questions myself, so feel free to start an open discussion in the comments below 👀.\nhashtag\n#\ncarrer\nhashtag\n#\nai\nhashtag\n#\ndemand\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E10AQGFK8tYf0y5KQ/image-shrink_800/0/1663655696508?e=1705082400&v=beta&t=SCdYV79zEKcki-8azv4OxjQtJNiR2s2fIEqKjjc8NtM"
        },
        "Post_110": {
            "text": "Everybody says you can improve your model by training it on more data. Is it true? How can we test this theory before spending money and effort collecting that new data?\n.\nGood news. The solution is quite simple.\nUsing the 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐂𝐮𝐫𝐯𝐞 graph, you can quickly see the performance evolution of the model with various amounts of data.\nIt is important to validate this because more data is not the answer to all your problems. 😂\n.\nThe main idea of the 𝐋𝐞𝐚𝐫𝐧𝐢𝐧𝐠 𝐂𝐮𝐫𝐯𝐞 method is simple:\n1. You create N subsets from your training dataset. Where the first subset is the smallest. You keep increasing the subset size at a given rate until you reach the full dataset size: 1 < 2 < 3 < ... < N.\n2. You train your model on subsample 1.\n3. You test your model on the test split (which is not divided; it is not good practice to touch your test split).\n4. Save the results.\n5. Repeat steps 2-4 on all your subsets.\n6. Plot the results.\n.\nUsing this mechanism, you can simulate if adding more data will boost your model's performance.\nAlso, you can quickly compare different models and evaluate which one needs more/fewer data to achieve your desired accuracy.\n.\nCheck out how easy it is to do this with Sklearn:\nhttps://lnkd.in/d358Dhhd\n.\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQGnqUTdbrlucQ/feedshare-shrink_800/0/1660716949072?e=1707350400&v=beta&t=VhCxp0eknc3WcP43-UQ9R8CYU_1PA6h7UO1SgRsH-5k"
        },
        "Post_111": {
            "text": "How you frame a\nhashtag\n#\nmachinelearning\nproblem is essential for your future self.\nIt can make your life easy or a living hell.\n💡 𝐄𝐱𝐚𝐦𝐩𝐥𝐞\nA user just clicked on our e-commerce site that sells tech. We want to welcome them and give them a personalized discount on a hand-picked item.\nHow do we solve this problem using\nhashtag\n#\nML\n?\nAt first sight, we might start building a multiclass classifier that takes input features about the user and predicts the probability that the user might like a particular item.\nBut this decision will make your future life a living hell if we consider it.\n.\n𝐖𝐡𝐲?\nFor every new item in the store, you need to retrain the model to adapt its output to the new inventory.\n.\n𝐒𝐨𝐥𝐮𝐭𝐢𝐨𝐧\nBuild a binary classifier that takes as input the user's features and information about a specific item. The model's output will reflect only the probability that the user might like the item you used as input.\nIn this scenario, we will have no issue when a new item is added to the store.\n.\n𝐖𝐡𝐲?\nBecause the output is not dependent on the number of items within the store.\nWe will predict the probability for every item and take the most significant out of them.\nIn conclusion, it might be worth taking a deep breath and thinking about the problem before starting to code your solution.\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQGGqouhjN4ANQ/feedshare-shrink_800/0/1659623203087?e=1707350400&v=beta&t=0GpCiltAwx5IgR6ybqn4Argz9ih3dpFwmsqbP5cBg8M"
        },
        "Post_112": {
            "text": "I just started watching an excellent  ML course on building real-world, end-to-end ML applications.\nIf you ever want to level up your ML knowledge and make it production-ready, I recommend this course by\nPau Labarta Bajo\n.\nHe is a DS and ML professional with 8+ years of experience building production-ready ML systems. Thus, you can get real-world guidance on how to build end-to-end ML applications.\nWithin the course, you will get clear explanations of how to build and design an end-to-end ML application.\nI think it's worth it to check it out.\nP.S. Also, you can follow him on LinkedIn and Twitter, where he often posts content about ML in production. I learned a lot from him.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nrealworld\nhashtag\n#\ntutorial\nhashtag\n#\ncourse",
            "image": "https://media.licdn.com/dms/image/C5622AQGYvQmmtnTGbg/feedshare-shrink_2048_1536/0/1654833749888?e=1707350400&v=beta&t=hfIcvbeNw9TNhdt3uCbI4SI9KIzyWCcLS70vSfG3Ces"
        },
        "Post_113": {
            "text": "I found the top 10 most in-demand skills in DS and ML after I analyzed 111 jobs. Using a data-centric approach, here is how I have done it 🦾\n.\nUsing LinkedIn Premium, I randomly sampled 111 positions across Europe and US. I saved them to\nTeal\n, which automatically extracted essential keywords from the job descriptions and generated the graph below.\nI believe this graph is beneficial in optimizing your resume/LinkedIn profile with the proper keywords. Because I sampled positions from various domains (so I won't be biased), these are more general terms. But along these keywords, you can also repeat the same process in your domain of interest (e.g., CV, NLP, recommendation systems, etc.) to build a killer profile.\nNote that you have to pay for LinkedIn Premium, but to use this feature from Teal is free of charge.\nLink to Teal in the comments 👇\nhashtag\n#\njobs\nhashtag\n#\nresume\nhashtag\n#\nskills\n-----\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQE6GrRqLwZyaw/feedshare-shrink_800/0/1654272075833?e=1707350400&v=beta&t=0Wd_ub71J9bj1JV7ahHpkxCdCDCHWUEuPb3B5cxFdYk"
        },
        "Post_114": {
            "text": "We'll send five of these to lucky people anywhere in the world 🌎\nWe want to celebrate with you - our book just turned 1 - they grow up so fast.\n👉 Simply comment below by adding (@) friends who should get the book. We randomly select your friends and contact them for their address to ship the books.\n📖 About the book:\n\"Why does my neural network not learn?\" by @Frank Hafner and I, will kickstart your friends' deep learning journey!\nWith best practices and extensive experience from applying deep learning in research and development, they will get their own neural networks to train.\n🚀 Check it out in more detail, and get your own copy here (make sure to search on your amazon marketplace, we are basically available everywhere) >>\nhttps://lnkd.in/exfkjY9t\n☕ And as always, feel free to follow here on LinkedIn.\nhashtag\n#\ndatascience\nhashtag\n#\npython\nhashtag\n#\ndeeplearning\nhashtag\n#\nGiveaway\nhashtag\n#\nbooks\nhashtag\n#\nkdp\nhashtag\n#\nmachinelearning",
            "image": "https://media.licdn.com/dms/image/C5622AQGxmGsXLqJorw/feedshare-shrink_800/0/1653666061964?e=1707350400&v=beta&t=asToqika68XG9Qp1jCil0OAFGCIkbtKDLzo7q1joJAw"
        },
        "Post_115": {
            "text": "These are 3 data distribution shifts you have to know when building real-world ML applications 👇\nhashtag\n#\nml\nhashtag\n#\ndata\nhashtag\n#\ndistributionshift\nhashtag\n#\nmonitoring",
            "image": "https://media.licdn.com/dms/image/C5622AQEpbH-nQXuj0A/feedshare-shrink_2048_1536/0/1650294001082?e=1707350400&v=beta&t=C_hNLMpiBA8mpDMeiXRbHEBIEDRdVh6Xf_3oc5B1sTU"
        },
        "Post_116": {
            "text": "Let's clarify the difference between online and batch prediction for deploying your ML models🔥\n.\nFirst, we must understand 𝐭𝐰𝐨 𝐟𝐞𝐚𝐭𝐮𝐫𝐞 𝐭𝐲𝐩𝐞𝐬 that your ML model can learn from.\n#𝟏 𝐁𝐚𝐭𝐜𝐡 𝐟𝐞𝐚𝐭𝐮𝐫𝐞𝐬\nBatch features are represented by the good old static datasets you are accustomed to. It represents historical data that can be stored in databases, data warehouses, etc.\nDon't confuse batch features with batch prediction (we will get to it immediately).\n#𝟐 𝐒𝐭𝐫𝐞𝐚𝐦𝐢𝐧𝐠 𝐟𝐞𝐚𝐭𝐮𝐫𝐞𝐬\nStreaming features are computed from real-time data sources. These are used when you have access to streaming infrastructure.\n.\n#𝟏 𝐁𝐚𝐭𝐜𝐡 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧\nBatch prediction uses batch features to periodically generate predictions that are stored in a database. When a client makes a prediction request to the system, based on some metadata, the system will directly look for the precomputed predictions in the database.  It is also known as an 𝐚𝐬𝐲𝐧𝐜𝐡𝐫𝐨𝐧𝐨𝐮𝐬 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧.\nBatch prediction is used when the model computes the predictions with high latency, and your application has to be in real-time. It is like you are caching all possible predictions in a database so you can instantly serve the result.\nThe most significant advantage of this method is also its biggest weakness. You need a database and computing power to generate predictions for all possible combinations (e.g., in a song recommendation system, you must compute all the recommendations for your users hourly), which might be costly and redundant. Imagine that you have 100k users and only 1k use your app daily. If you compute your predictions daily, 99k of predictions are redundantly computed.\n#𝟐 𝐎𝐧𝐥𝐢𝐧𝐞 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧\nOnline prediction can use both batch features and streaming features to make predictions. It is a more intuitive method as it is the usual way to do things. When a client makes a prediction request to the system, the prediction is computed by the model on demand. It is also known as a 𝐬𝐲𝐧𝐜𝐡𝐫𝐨𝐧𝐨𝐮𝐬 𝐩𝐫𝐞𝐝𝐢𝐜𝐭𝐢𝐨𝐧.\nOnline prediction is used when the system has to be real-time, and the model predictions can be calculated with low latency. Also, in many cases, you are forced to apply the online prediction pattern because you can't possibly compute all the combinations required for your users (e.g., an NLP translation app can't compute all the combinations between all the sentence options and languages).\n.\nBoth methods can be used for real-time predictions. Only the way the predictions are served is different.\nAlso, these two methods are complementary. For example, you can use batch prediction for your top most common requests and make an online prediction for the rest.\n.\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQHeDO1Xk9nlmg/feedshare-shrink_800/0/1650018577071?e=1707350400&v=beta&t=BOjHwhAueXJBGQNzso_7FJQ-4ckeAGve-TUqlbiog-A"
        },
        "Post_117": {
            "text": "As an ML engineer, you are still a software engineer specializing in building AI/ML systems. Therefore, you still have to write clean, maintainable, and scalable software.\nThus, you need to be aware of good software practices and patterns.\nHaving that in mind, I researched and compiled an article about 10 software patterns that fit well in any ML application.\nhashtag\n#\nml\nhashtag\n#\nsoftwarepatterns\nhashtag\n#\nai",
            "image": "https://media.licdn.com/dms/image/C4E22AQE0S9Y0dHuBAg/feedshare-shrink_800/0/1650039433740?e=1707350400&v=beta&t=TVev7A-xddnxc3RvTNQXxMgsKnNS7PMfktUeVLqpNyA"
        },
        "Post_118": {
            "text": "This is a method that usually solves the class imbalance issue and is generally omitted 👇🏻\n.\nStop complicating with oversampling or undersampling your data distribution.\nThe easiest way is to use loss weights to raise the error on your minority class and lower the error on other classes.\n.\nBut the most elegant solution is by using: Focal Loss.\nA loss function initially introduced in Computer Vision by Facebook Research that dynamically moves its learning attention to samples from which it needs to learn more.\nFrom my experience, this method is often omitted when training models with tabular data.\n.\nAdditional to the difference between the model prediction and the ground truth, the \"Focal Loss\" focuses on the confidence of the output:\nWhen the confidence in a prediction is high, the loss will be very low.\nWhen the confidence in a prediction is low, the loss will be very high.\nTo find out more, check out the paper:\nhttps://lnkd.in/dWyPbAD5\n.\n💡 Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFKb__mL4LRpg/feedshare-shrink_800/0/1643387997184?e=1707350400&v=beta&t=o6k9wWmj_nwS8vqF_bnipEmrzwNbvVbonaWso7Z9xCU"
        },
        "Post_119": {
            "text": "Code quality is what sets apart good software systems from bad ones.\nData quality distinguishes great AI systems from those that will never make it to production.\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQGCzw1nHfIPqw/feedshare-shrink_800/0/1627203653569?e=1707350400&v=beta&t=dDO4iLPcoMZN2JEabKZI-Oq2jCpUOyJEHisn9hPwzU8"
        },
        "Post_120": {
            "text": "Those are the 2 most common issues of data quality:\n𝟏. 𝐄𝐝𝐠𝐞 𝐜𝐚𝐬𝐞𝐬 𝐚𝐫𝐞 𝐫𝐚𝐫𝐞.\nOften due to random sampling. You can get significant performance by picking more difficult training data.\nWhat doesn't kill you makes you stronger.\nEdge cases are often environmental conditions (not only class imbalance).\n𝟐. 𝐒𝐚𝐦𝐩𝐥𝐞𝐬 𝐚𝐫𝐞 𝐦𝐢𝐬𝐥𝐚𝐛𝐞𝐥𝐞𝐝 𝐨𝐫 𝐦𝐢𝐬𝐬𝐢𝐧𝐠.\nThis makes sense because images seen 1,000 times are easier to label than seen only once.\nOne innovative method to find mislabeled samples is picking the ones in your trained model with the highest error.\nThis is known as sort-by-loss or sort-by-confidence, where your model and ground truth disagree.\n.\nA good strategy is to pick high-quality data, starting from difficult ones and finding more of the same.\nThis strategy is also known as a derivative of Active Learning.\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQHAxYVtTgvrqA/feedshare-shrink_800/0/1627811822843?e=1707350400&v=beta&t=daV-rIyzFXXLWwQihj1FxtKpHIRfDk047WJyU91vwio"
        },
        "Post_121": {
            "text": "Here is an excellent site to read and research the most pressing world problems. They also provide suggestions on how you can contribute to adding value to these fields.\nI stumbled upon this site as I was trying to find a field where to contribute and find real meaning. It helped me grasp the big picture of the world's pressing problems.\n.\nTheir list of most pressing world problems (sorted by importance):\n1. Risks from artificial intelligence\n2. Catastriohic pandemics\n3. Building effective altruism\n4. Global priorities research\n5. Nuclear war\n6. Epistemics and institutional decision-making\n7. Climate change\n.\nAs expected, as\nhashtag\n#\ndata\npeople, we can contribute to all of these. This is awesome because we can directly have a tangible impact on the future of the earth.\nIt is interesting to see that climate change is at the bottom of their list and risks from artificial intelligence at the very top. Is AI more dangerous than climate change? What do you think?\n-----\nYou can find the link to the site in the comments 👇\nhashtag\n#\nvalue\nhashtag\n#\nworldproblems\nhashtag\n#\nai\nhashtag\n#\nml",
            "image": "https://media.licdn.com/dms/image/C4D22AQG2Av4Wf2vX1Q/feedshare-shrink_800/0/1627591805899?e=1707350400&v=beta&t=QfYerJQEZBSdpoYJe00SIE-0X0ewSEhJegbTA4vl0jo"
        },
        "Post_122": {
            "text": "These are the most common ways to handle missing data before feeding it to your ML model.\nhashtag\n#\ndata\nhashtag\n#\nml\nhashtag\n#\nmissingdata",
            "image": "https://media.licdn.com/dms/image/C4D22AQF18UYOBfIqVw/feedshare-shrink_800/0/1624867441048?e=1707350400&v=beta&t=zJMtR4HRqs73TriBRhU19Hcgu-cJu9bBNQVREtg-Qx8"
        },
        "Post_123": {
            "text": "I found another use-case for the \"*\" symbol in Python 👇\n.\nAs you know, the \"*\" is used in Python for deconstructing iterable structures (and not for signaling pointers as in other languages, like C & C++😍).\nI recently learned that you could use the \"*\" deconstruction logic to take the first/last element from a list.\n.\nYou can achieve the same functionality with other methods like \"pop,\" but I found this syntax a lot cleaner as it is more verbose.\nIt is yelling to the reader: \"Hey, I just popped out the last element from the list into variable 'tail'.\"\nAs you can see in the image below 👇\n.\nAs a person who writes code, it is valuable for you and others who will read the code to write it as verbosely as possible. In this manner, the code is readable and documents itself, removing the need for useless comments (which are hard to maintain in the long run).\n.\n💡 My goal is to make\nhashtag\n#\nml\neasy and intuitive. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQFsplyh0YtRbg/feedshare-shrink_800/0/1624867463197?e=1707350400&v=beta&t=9mRiAec5SeCtNupL367J43Id9MVjQlrTRfVpKAYENZk"
        },
        "Post_124": {
            "text": "ML models are limited, dumb, and wrong.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\ndata",
            "image": "https://media.licdn.com/dms/image/C4D22AQFrKm0kTozXEA/feedshare-shrink_800/0/1624867463951?e=1707350400&v=beta&t=MLPDFt4I0_9ApTwIlrDtaq95vyUcy5q-rIlTLq2xBW4"
        },
        "Post_125": {
            "text": "New Year's Eve is another trip around the sun, nothing special if you put it that way. But personally, I find it a great reminder to reflect on your life. I firmly believe that you must understand yourself to add value to the world.\n.\nFrom my perspective, it is essential to periodically ask yourself questions such as:\n➝ What matters right now in my life?\n➝ Are my current systems and habits aligned with my long-term goals?\n➝ Do I have the same goals, or should I reevaluate them?\n➝ How can I make life simpler and more meaningful?\n➝ Is my life balanced for a happy and meaningful life?\n.\nAs human beings, we are constantly changing relative to our environment. Thus it is essential to reevaluate our desires continually. What we thought would make us happy at 20 years old might completely change at 25 years old, which is perfectly fine. We don't have to create our prison of desires.\nMy goal with this post is not to brag (I hate bragging) but to cast what I have in mind to the world.\n2022 was an intense year:\n➝ I finished my ML master's degree and DS nanodegree\n➝ I started my career as a freelancer\n➝ I moved in with my girlfriend\n➝ I started creating content on LinkedIn and Medium (as an introvert, this took me completely outside my comfort zone)\n➝ I learned to accept who I am and to follow my path.\nIn 2023 I plan to:\n➝ master the craft of bringing ML systems into production\n➝ to continue to improve, learn, and add value by making content on LinkedIn and Medium\n➝ to improve my freelancing skills: selling, communication, and negotiation\n➝ to learn to work with people better\n➝ to move to a tech hub in Europe and use ML to contribute to real-world such as the global warming\n➝ to take better care of my mind: learn to meditate more profoundly, be curious, grateful, and present, to learn to be more kind to myself and more open to people\n➝ to take better care of my body: I want to be more active by learning to play tennis, swim better, go to the sauna, and do cold baths\n.\nAt the end of the day, what else is there than living a life full of meaning and fulfillment?\n.\nThis was more of a personal and philosophical post, but I hope you liked it.\nHappy holidays and a new year full of prosperity and meaning  🎉🎉🎉.\nhashtag\n#\nlife\nhashtag\n#\nresolutions\nhashtag\n#\nwellbeing",
            "image": "https://media.licdn.com/dms/image/C4D22AQFbl5mv5FKfjA/feedshare-shrink_800/0/1624867444054?e=1707350400&v=beta&t=SeWExhpPicnFRn0fEF-4JXrmbIch1I9z2OhJWwZ3TH0"
        },
        "Post_126": {
            "text": "Have you ever felt overwhelmed by the monstrous number of tools you have to know to build an\nhashtag\n#\nMLOps\nsystem?\n.\nPersonally, I sure did. There are just too many...\nAs the field is only at the beginning, it is only natural for tools to pop all over the place.\nThough it is hard to keep track of them...\n.\nFortunately, here is a tool that summaries all of them and groups them relative to their functionality:\n➝ Experiment tracking\n➝ Data versioning\n➝ Pipeline orchestration\n➝ Runtime engine\n➝ Artifact tracking\n➝ Model registry\n➝ Model serving\n➝ Model monitoring\nCheck it out:\nhttps://mymlops.com/\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4D22AQFgD1PZn4NB6g/feedshare-shrink_800/0/1624867456703?e=1707350400&v=beta&t=y6BcO7W_poM0ir6sz9HzTMq9ge7A-Qw5tB8tjrKPlmA"
        },
        "Post_127": {
            "text": "ChatGPT's ability to \"solve\" leetcode/DSA exercises with pseudocode doesn't prove how awesome and advanced it is.\nIt proves how useless those kinds of problems are for hiring.",
            "image": "https://media.licdn.com/dms/image/C4E22AQHhJXdg9zbqfQ/feedshare-shrink_800/0/1624528753593?e=1707350400&v=beta&t=yG7uzE0FVIo3FJTkec2xLMCmo4WYAnfbkbrjYkvPZ4U"
        },
        "Post_128": {
            "text": "If users have the slightest possibility of inputting incorrect data, be sure they will.\n.\nThey could input text instead of numbers.\nThe images might be corrupted.\nThe numbers might exceed the supported range/interval.\nAnd the possibilities are endless.\n.\nThat is why data validation in a production system is essential.\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQEg3IKY0KqoVA/feedshare-shrink_2048_1536/0/1623660125369?e=1707350400&v=beta&t=X9G3K5KeLHz8rkuV72i3BqMnRSrBVxdYRJn-hP-nneE"
        },
        "Post_129": {
            "text": "I think\nhashtag\n#\nsampling\nis one of the most common words used in\nhashtag\n#\nmachinelearning\n. Why?\n.\nBecause we don't live in an ideal world where we are omnipresent and all-powerful, and thus we don't have access to all the data in the world or don't have the computing power to train a model with 100TB of data.\nTherefore, we use sampling to compile a representative subset of the whole population.\nWith an emphasis on \"representative.\"\n.\nIt is easy to sample a random subset, but making it representative is hard.\nIt is essential to pay attention to this detail. Otherwise, your model will be biased and prone to failure when you deploy it into production.\n.\nSome popular sampling methods are:\n- stratified sampling\n- weighted sampling\n- reservoir sampling\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFM2vMSrpMqHw/feedshare-shrink_2048_1536/0/1621427623180?e=1707350400&v=beta&t=yNIz6gHvclclrNhMSAeILoYO9-lfz5SU7R4pqK7BYSE"
        },
        "Post_130": {
            "text": "CSV vs. Parquet. Which one is better?\n.\nSuch questions do not have strict answers.\nLet's observe this question from a different perspective.\nWe should 𝐫𝐞𝐟𝐨𝐫𝐦𝐮𝐥𝐚𝐭𝐞 𝐭𝐡𝐞 𝐪𝐮𝐞𝐬𝐭𝐢𝐨𝐧 𝐚𝐬 𝐟𝐨𝐥𝐥𝐨𝐰𝐬:\n- row vs. column major\n- text vs. binary\n.\n𝐑𝐨𝐰 𝐯𝐬. 𝐂𝐨𝐥𝐮𝐦𝐧 𝐌𝐚𝐣𝐨𝐫:\nIt is a method of accessing the data. A row-major structure can easily read a row from a file, and a column-major file can quickly retrieve a column.\nAlso, it is faster to write row-major structures.\nCSV: row-major\nParquet: column-major\n.\n𝐓𝐞𝐱𝐭 𝐯𝐬. 𝐁𝐢𝐧𝐚𝐫𝐲:\nA text file is human-readable, while a binary file consumes less storage and is faster to read.\nCSV: text\nParquet: binary\n.\n🧐 There is no correct answer on which file type you choose. It all depends on your use case. This also applies to other persistence mechanisms.\n📃 But, for example, you could use a CSV file to log real-time data. Meanwhile, you can schedule a different process that regularly reads the CSV file, transforms it into a parquet file, and stores it in your data warehouse for long purpose use-cases.\nThrough this, we can leverage the best of both worlds.\n.\n💡 My focus is making\nhashtag\n#\nml\neasy and intuitive to understand. Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQHWPk1KX6LmNg/feedshare-shrink_800/0/1621353923984?e=1707350400&v=beta&t=i6CmUTnQQFmafD07zJdv2WzwWCH9oyTPfxJOYzvkIDM"
        },
        "Post_131": {
            "text": "Is active learning a game changer on how we label data?",
            "image": "https://media.licdn.com/dms/image/C5622AQElhwSF0A90sQ/feedshare-shrink_800/0/1571500709235?e=1707350400&v=beta&t=epxjuoC-EMwjU8NgC9rdchmXiG8oEydteyBUBtsXH2E"
        },
        "Post_132": {
            "text": "I find weak supervision labeling methods very intriguing.\n.\n𝗪𝗵𝗮𝘁 𝗶𝘀 𝘄𝗲𝗮𝗸 𝘀𝘂𝗽𝗲𝗿𝘃𝗶𝘀𝗶𝗼𝗻?\nIt is a method of labeling your data through a set of labeling functions (LF).\nThe LF represents a set of heuristics based on which you can programmatically generate ground truth for your data.\n.\n𝗙𝗼𝗿 𝗲𝘅𝗮𝗺𝗽𝗹𝗲 👇🏻\nYou want to create a model that predicts the type of LinkedIn posts.\nBased on the hashtags of some posts, you can create your desired labels.\nNow you can train a generalized model that can predict the type of a post, whether it has hashtags or not.\n.\n𝗪𝗵𝘆 𝗶𝘀 𝗶𝘁 𝘀𝗼 𝗽𝗼𝘄𝗲𝗿𝗳𝘂𝗹?\n➝ You can quickly scale your labeling infrastructure.\n➝ It is cheap (or at least cheaper than hand labeling).\n➝ The rules can quickly be shared among peers.\n.\n𝗕𝘂𝘁, 𝘂𝗻𝗳𝗼𝗿𝘁𝘂𝗻𝗮𝘁𝗲𝗹𝘆, 𝗻𝗼𝘁𝗵𝗶𝗻𝗴 𝗶𝘀 𝗽𝗲𝗿𝗳𝗲𝗰𝘁...\n➝ Using this method, your labels will most likely be noisy.\n➝ Sometimes, you cannot write the heuristics programmatically.\nBoth cases,  if they happen, could be a complete killer for your ML model.\n.\n💡 Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C5622AQGycMyQqF9LjA/feedshare-shrink_800/0/1561826703102?e=1707350400&v=beta&t=aVX2mE1iJw5z-Y0A7UX57JBSNvQxvpB9Kh3oB837hH4"
        },
        "Post_133": {
            "text": "Your model's performance heavily depends on the 𝗾𝘂𝗮𝗹𝗶𝘁𝘆 and 𝗾𝘂𝗮𝗻𝘁𝗶𝘁𝘆 of your training data and labels.\n.\nThe reality is that hand-labeling qualitative data is often complicated and expensive.\nSome major issues that might appear are:\n➝ data privacy: Some annotators might not be allowed to look at the data, or the data cannot leave a particular place.\n➝ label multiplicity: You use multiple sources and annotators, which might introduce a lot of noise.\n➝ data lineage: Your new batch of annotated data, which you thought would save your model, actually burnt it down. The real issue is that you haven't versioned your data batches, so you cannot differentiate between the good and bad samples.\n.\n💡 Follow me for daily insights about\nhashtag\n#\ndata\n,\nhashtag\n#\nml\n, and\nhashtag\n#\nmlops\n.",
            "image": "https://media.licdn.com/dms/image/C4E22AQFhwQYT4N8fdg/feedshare-shrink_800/0/1577744773434?e=1707350400&v=beta&t=J-V9Izcy3FTxkXwZhhpBXoDmQyGL09S17FO6U8ke1gs"
        },
        "Post_134": {
            "text": "I am excited to see the drastic performance improvement in the latest version of Python.\nAs you can see, Python 3.11 is ~35% faster than Python 3.10.\nThat is incredibly powerful to the Python community and for everything built with Python in the future.\nhashtag\n#\npython\nhashtag\n#\nml\nhashtag\n#\nmlops"
        },
        "Post_135": {
            "text": "I was pretty excited when I found out that on scikit-learn-contrib you can find all kinds of extensions to the scikit-learn library. It contains various tools that are compatible with the scikit-learn API.\nSome of the most popular extensions are imbalaced-learn (tools to handle imbalanced datasets), category_encoders (various encoders for your categorical variables), hdbscan (one of the best clustering algorithms), etc.\n➝ Check them out:\nhttps://lnkd.in/dPZmGZKv\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nsklearn"
        },
        "Post_136": {
            "text": "Check out the most common metrics you should know for\nhashtag\n#\nclustering\n.\n.\n.\n.\nFor unsupervised methods, such as clustering, we cannot use standard classification metrics because we don't have any ground truth.\n.\nThe most common metrics, where you don't require information about the ground truth labels, are:\n1. Silhouette score:\n- defined as the ratio between the mean distance of the intra-cluster and the mean distance to the nearest cluster\n- it ranges within [-1, 1], the higher, the better\n- tells us how well-assigned each point is:\n- '1' indicates that the sample is well-assigned to the cluster\n- '0' indicates that the sample is on the decision boundary\n- '-1' indicates that the sample is in the wrong cluster\n2. Calinski Harabaz index:\n- defined as the ratio between the within-cluster dispersion and the between-cluster dispersion\n- the higher, the better\n3. Davies-Bouldin Index:\n- defined as the average similarity measure of each cluster with its most similar cluster\n- it is helpful to decide on the number of clusters\n- it ranges within [0, +inf]. The smaller, the better\n4. Dunn's Index\n- defined as the minimum inter-cluster distance divided by the maximum cluster size\n- the higher, the better\n.\nThe metrics described above give a numerical interpretation of how well the clusters are separated. Because we don't have labels, it cannot determine the validity of the model's predictions.\nWe can use those methods to decide on the optimal number of clusters and to compare the performance of various unsupervised algorithms.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nclustering\nhashtag\n#\nmathematics"
        },
        "Post_137": {
            "text": "I recently found an intuitive way to understand the main families of clustering methods.\n.\n.\nYou can split all the clustering methods into:\n- 𝐟𝐥𝐚𝐭\n- 𝐡𝐢𝐞𝐫𝐚𝐫𝐜𝐡𝐢𝐜𝐚𝐥 (solves the resolution problem within your data)\nand into:\n- 𝐜𝐞𝐧𝐭𝐫𝐨𝐢𝐝-𝐛𝐚𝐬𝐞𝐝/𝐩𝐚𝐫𝐚𝐦𝐞𝐭𝐫𝐢𝐜 (it does NOT perform well when you don't know the shape of the data or the data has many dimensions)\n- 𝐝𝐞𝐧𝐬𝐢𝐭𝐲-𝐛𝐚𝐬𝐞𝐝/𝐧𝐨𝐧-𝐩𝐚𝐫𝐚𝐦𝐞𝐭𝐫𝐢𝐜 (robust to noise)\n.\nThose two types of families are orthogonal.\nTherefore, you can have hierarchical density-based algorithms.\nThose are one of the best-performing clustering algorithms (e.g., HDBSCAN).\n.\nAs a bonus:\n➝ Check out this excellent clustering cheatsheet from sklearn:\nhttps://lnkd.in/dXaSCiXG\n➝ Also, this video which explains what I wrote above in more detail:\nhttps://lnkd.in/dUDfuYxP\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nclustering\nhashtag\n#\nmathematics"
        },
        "Post_138": {
            "text": "Everybody says that the best way to learn something is by doing projects. Read below why this is not true.\n.\n.\nI am not against projects or pro courses/books.\nBut, if you learn only by putting it into practice, you won't be able to connect the dots in the long run.\n.\nWhy?\nBecause while you are developing something, you want concrete answers. Therefore, you probably won't take your time to focus on the fundamentals.\n.\nTo scale what you are learning, you have to master the fundamentals of the topic, and only after that should you add more complexity on top of it.\nOtherwise, you will memorize something, and in the long run, you will forget it.\n.\nI am not against project learning. I am pro projects, and I think it is a great source to understand why a piece of theory is useful.\nBut, I am trying to say that it is not enough.\n.\nSo, how can you get the best out of both worlds?\nWell, there should be a balance between practice and theory.\n.\nStart with a brief theoretical introduction to the topic you want to learn.\nThen, implement a project. Understand what is important and ask the right questions.\nIn the end, go again to that piece of theory and try to answer all your questions. Try to fill the gaps.\n.\nUnfortunately, learning is not linear.\nMy suggested learning path would be something like this:\ntheory -> practice -> theory -> practice -> ...\nAt every iteration, you will get a deeper understanding of the domain.\nhashtag\n#\nlearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nsoftware"
        },
        "Post_139": {
            "text": "Check out a great video that intuitively explains the math behind the convolution operation.\nSay no to complicated math formulas, jargon, and the desire to look smart.\nIt is all about intuition and the beauty behind math.\nThe video is by no other than 3Blue1Brown.\nHere is the link:\nhttps://lnkd.in/dnastedV\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\ndatascience"
        },
        "Post_140": {
            "text": "This is a good rule of thumb to judge the strength of the\nhashtag\n#\ncorrelation\nbetween two variables:\n.\n.\nStrong: 0.7 <= | c | <= 1.0\nModerate: 0.3 <= | c | < 0.7\nWeak: 0.0 <= | c | < 0.3\nc = correlation values, ranging between [-1, 1]\n.\nThis is only a heuristic. It might vary in different contexts.\nBut it is a good starting point to highlight the most critical variables of your specific problem.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\ncorrelation\nhashtag\n#\nmathematics"
        },
        "Post_141": {
            "text": "This is hypothesis testing in a nutshell.\n.\n.\nIt contains a null hypothesis (H0) and an alternative hypothesis (H1).\n.\nThe null hypothesis (H0) is true before you collect any data.\nH0 usually states there is no effect or that two groups are equal.\nH0 contains an equal sign of some kind - either =, ≤, or ≥.\nH1  is what we would like to prove to be true.\nH1 contains the opposition of the null - either ≠, >, or <.\nThe H0 and H1 are competing and non-overlapping hypotheses.\n.\nOne example of hypothesis testing.\nWe implemented a new design for our website. The end goal is to have more traffic. To measure the improvement of the new change, we can use hypothesis testing as follow:\nH0: avg_daily_visits_new ≤ avg_daily_visits_old\nH1: avg_daily_visits_new > avg_daily_visits_old\navg_daily_visits_x = the average number of daily visits on the website for design X\n.\nThere are two types of errors:\n𝐓𝐲𝐩𝐞 𝐈 𝐞𝐫𝐫𝐨𝐫: we believe the alternative to be true, but in reality, the null is true (denoted by the symbol α)\n𝐓𝐲𝐩𝐞 𝐈𝐈 𝐞𝐫𝐫𝐨𝐫: we believe the null to be true, but in reality, the alternative is true (denoted by the symbol β)\n.\nAnother example:\nWe want to test the parachutes before using them, which can either work or not.\nH0: the parachute doesn't open (safer to assume none of the parachutes work until enough data suggests otherwise)\nH1: The alternative is that the parachute opens, which makes sense as it is what we would like to prove.\nIn this case, a Type I error would be that we think the parachute will open (H1) when it doesn't work (H0). Such an error would result in killing somebody.\nTherefore, Type I errors are worst than Type II errors. The threshold α of accepting Type I error varies on different applications.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nhypothesistesting\nhashtag\n#\nmathematics"
        },
        "Post_142": {
            "text": "This AI robot wakes up and takes a deep breath\n↓\nCheck out\nhttps://AlphaSignal.ai\nto get a summary of top publications and breakthroughs in Machine Learning.\n-\nhashtag\n#\ndeeplearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nneuralnetworks\nhashtag\n#\nnlp\nhashtag\n#\nrobotics\nhashtag\n#\nai\nhashtag\n#\ndatascience"
        },
        "Post_143": {
            "text": "Let's understand what bootstrapping is all about.\n.\n.\nBootstrapping is a technique where we sample from a group with replacement (replacement=after we pick one item, we put it back, therefore, at the next event, there is an equal probability that we can pick it again).\n.\nWhy do we even care about this?\n.\nUsually, we have only one sample from our population, which is not enough to generate a sampling distribution (we need more samples to do that).\n.\nWe can use bootstrapping to simulate the creation of multiple samples. Therefore, we can easily simulate our sampling distribution.\n.\nIt is effortless to understand.\n1. We have our actual sample/observations.\n2. We create a new sample by sampling with replacement from our observations.\n3. We compute our desired statistic on the bootstrapped sample (e.g., mean).\n4. Repeat this many times (e.g., 10k times)\n5. We end up with 10k samples from which we can compute the sampling distribution, standard error, confidence intervals, etc.\n.\nThis technique is also used in well-known methods such as Random Forest and Stochastic Gradient Boosting.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nbootstrapping\nhashtag\n#\nmathematics"
        },
        "Post_144": {
            "text": "What is Concept Drift?\nAs data changes over time, so should your AI / ML model.\nA concept drift refers to a situation when your original target distribution has changed from your previous model.\nDownload our whitepaper for more:\nhttps://lnkd.in/etihQ8am\nIn the image below, you can observe an example of our monitoring in real-life. The teal and red bars, from the image below, represent the upper and lower bounds for the metrics (i.e. the intervals within which the data and model behave as expected). coreControl has out-of-the-box statistical tools to detect drift, such as KL divergence, KS statistics, tree methods, etc. After the monitoring system detects any value outside the allowed interval, an alarm is triggered, and the coreControl monitoring system will notify you that a specific drift is detected​​.\nhashtag\n#\nai\nhashtag\n#\nml\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nmlops\nhashtag\n#\ncoreai"
        },
        "Post_145": {
            "text": "What is the difference between the Law of Large Numbers and the Central Limit Theorems?\n.\n.\n1. 𝐋𝐚𝐰 𝐨𝐟 𝐋𝐚𝐫𝐠𝐞 𝐍𝐮𝐦𝐛𝐞𝐫𝐬: the larger the sample size, the closer our statistic gets to the parameter. The larger the sample size, the more significant the chance that the sample is a good representation of the population.\n2. 𝐓𝐡𝐞 𝐂𝐞𝐧𝐭𝐫𝐚𝐥 𝐋𝐢𝐦𝐢𝐭 𝐓𝐡𝐞𝐨𝐫𝐞𝐦: if our sample size is large enough, the sample mean will be normally distributed. That is why we use the normal distribution when comparing different samples from a population.\n.\nThose two theorems say that if the sample is large enough, we can estimate the statistics of the population (1) by leveraging normal distributions (2).\nBoth theorems are intuitive and important to know to master your\nhashtag\n#\ndata\nproperly.\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nmathematics"
        },
        "Post_146": {
            "text": "The only moment when coding looks cool to the outside world is when you do it quickly and look like a hacker.\nhashtag\n#\npython\nhashtag\n#\nprogramming\nhashtag\n#\nsoftwareengineering"
        },
        "Post_147": {
            "text": "What is the difference between the standard deviation and the standard error?\n.\n.\n➝ Standard Error:\n• The standard deviation of the statistics distribution\n• For example, we have five samples from a population with the sample mean equal to 3, 5, 7, 10, 50. The standard deviation of the mean of the samples is ~19.7, which is called the standard error.\n• The standard error decreases as we increase the sample size. This is true because as we have bigger samples, the mean of a sample is closer to the mean of the population, and therefore, it won't vary so much.\n.\n➝ Standard Deviation:\n• It is a general measure of spread that can be used for any set of points.\n• In this scenario, the standard error is a particular case of the standard deviation.\n• The standard error is just a naming convention for the standard deviation of the sample means (conventionally known as the statistical distribution of the given samples).\n.\n𝐍𝐨𝐭𝐞: Instead of the sample mean we could have used any other aggregate method that holds the Central Limit Theorem.\nhashtag\n#\nstandardeviation\nhashtag\n#\nstandarderror\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata"
        },
        "Post_148": {
            "text": "What is the importance of finding the best\nhashtag\n#\nsimilary\nmeasure for your\nhashtag\n#\nML\nalgorithm?\n.\n.\nThe similarity measure measures how much alike two data objects are.\nThis similarity score is the very basic building block in algorithms such as:\n- Recommendation engines,\n- Clustering,\n- Different classification problems, etc.\nIf the distance is small, the features have a high degree of similarity. At the same time, a considerable distance will be a low degree of similarity (or otherwise, the main idea is that we can numerically observe the 'equality' between two objects).\n.\nHere is a list of the most popular methods out there.\nThe smaller, the more significant the similarity between the two points (this is the case when we use distances as a similarity score):\n- Euclidean distance (geometric distance)\n- Manhattan distance (geometric distance)\n- Minkowski distance (a generalization of Euclidean & Manhattan)\n- Chebyshev distance (geometric distance)\n- Mahalanobis distance (distance between a point and a distribution)\nThe bigger, the more significant the similarity between the two points:\n- Cosine Similarity (the angle between two vectors)\n- Jaccard Similarity / Dice Score (used between sets)\n- Pearson/Spearman correlation (the trend between two vectors)\nhashtag\n#\nmachinelearning\nhashtag\n#\nstatistics\nhashtag\n#\ndata\nhashtag\n#\nsimilarity\nhashtag\n#\nmetrics\nhashtag\n#\nmathematics"
        },
        "Post_149": {
            "text": "How often do you feel lost trying to understand what is going on in your\nhashtag\n#\ndata\nhashtag\n#\npipeline\n?\n.\n.\nIn my case, I often can't tell 100% the specific output of every stage of a pipeline without using a debugger.\nThis is amplified by the fact that we are often using Python.\nUsually, in Python code (or another dynamic programming language), you always have all kinds of surprises, as the variables can hold anything without any compiling errors.\n.\nI recently stumbled into Mage AI, a tool that solves this problem.\n.\nIt allows you to quickly\nhashtag\n#\nbuild\n,\nhashtag\n#\norchestrate\n, and\nhashtag\n#\nvisualize\ndata pipelines.\nIt gives you an intuitive GUI from which you can manage your entire data pipeline.\nThe sweet spot about this tool is that it allows you to plot the output of every stage within your data pipeline without writing any code.\nAnd...\nYou can quickly hook unit tests to every stage of the pipeline.\nThis is awesome.\nNo more guessing around. Now you can quickly validate and visualize every step of your data flow.\n.\nIt is more than an orchestration tool.\nAs a bonus, with a few lines of code, you can easily integrate it with Airflow.\n.\nCheck out their GitHub:\nhttps://lnkd.in/dcaSqbmv\n.\n.\n👋 If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dxV7Dnqv"
        },
        "Post_150": {
            "text": "Data, Data, Data... AI\nhashtag\n#\nmlops\nhashtag\n#\ndata\nhashtag\n#\ndatacentricai\nhashtag\n#\naiinnovation\nhashtag\n#\nbi\nhashtag\n#\nai2bi\nhashtag\n#\nai"
        },
        "Post_151": {
            "text": "Failing is the fastest way to succeed.\nYou thought this was a motivational speech?\nMaybe.\n.\nBut this concept also applies in\nhashtag\n#\nML\nwhen you want to find the best solution.\nThe faster you fail in your experimentation process, the quicker you understand what you did wrong.\nTherefore, the quicker you can adapt and develop better models.\n.\nDon't be afraid to fail.\nFailing is the fastest way to move forward.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nmlops\nhashtag\n#\ntraining"
        },
        "Post_152": {
            "text": "An AI system was asked to regenerate Human Evolution… A project by Fabio Comparelli.\nAll imagery generated with the latest AI tools, midjourney, stable diffusion etc. Prob. the most interesting (and disturbing) movie output I’ve seen up to now with these specific technologies. As it’s able to transcend the mere visual trickery, which is fun and technically mind blowing, but often remains flat as a hat 😉\nAlso note that projects like these do involve a great level of effort and experimenting of the human artist, before you’d actually make it to an end result like this. All movie frames are created by AI though and reflect, through their datasets, existing perceptions of our past and a seemingly unavoidable future…\nI’ve added a link to Comparelli’s website down below."
        },
        "Post_153": {
            "text": "Group By vs. Window Functions. Two methods every\nhashtag\n#\nML\nengineer should know.\n.\n.\nGroup By aggregates multiple rows into a single row.\nWindow Functions aggregate multiple rows and output one value for each row.\nBoth are inter-rows operations.\n.\nGroup By performs inter-rows operations, but the information is compressed into a single data point (e.g., the average sales of a shop).\nWindow Functions perform inter-rows operations, but the number of rows stays the same (e.g., the average shop sales for the last three days).\nBoth operations are precious in doing your data preprocessing in the right way.\nhashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nsql"
        },
        "Post_154": {
            "text": "How can you think about your future self and colleagues when writing\nhashtag\n#\ngit\nhashtag\n#\ncommits\n?\n.\n.\n.\nYou may think writing git commits messages is trivial or unimportant, but that is not the case. The reader needs to understand what happened all over the development history.\n.\nTo fully leverage the message from a\nhashtag\n#\ngit\nhashtag\n#\ncommit\n, it should follow the following format:\n\"\"\"\n<type>[optional scope]: <description>\n[optional body]\n[optional footer(s)]\n\"\"\"\nChoose your favorite editor:\n→ git config --global core.editor \"nano\"\nStart writing:\n→ git commit\n.\nThe <type>: feat, fix, docs, style, refactor, text, etc.\nThe <scope>: a function, file, or module you worked on (optional).\nThe <description>:\n- Should be no greater than 50 characters.\n- Should begin with a capital letter and not end with a period.\n- Use an imperative tone: 'If applied, this commit will <commit description>.'\nThe <body>:\n- Separate the subject from the body with a blank line.\n- Detailed explanation and context of the new changes.\n- Use the body to explain the what and why of a git commit, not the how.\n- Optional - Add the body if the changes are complex enough.\nThe <footer>: reference issue tracker IDs (optional).\n.\nOk. So why all of this is important?\nBy respecting this format, you can:\n- Automatically generate CHANGELOGs.\n- Automatically compute a semantic version bump.\n- Communicate the changes' nature to teammates, the public, and other stakeholders.\n- Trigger build and publish processes.\n- Make it easier for people to contribute to your projects.\nI think that is pretty cool. You can do all of that just by respecting a few writing conventions.\nhashtag\n#\nmachinelearning\nhashtag\n#\nsoftware\nhashtag\n#\ncoding\nhashtag\n#\ngit\nhashtag\n#\ngoodpractices"
        },
        "Post_155": {
            "text": "For all the\nhashtag\n#\nSQL\nfans out there. Can you train\nhashtag\n#\nML\nmodels directly using\nhashtag\n#\nSQL\n?\n.\n.\n.\nIt is crazy, but it turns out you can!\nI recently learned about\nhashtag\n#\nMindsDB\n, which lets you train and use models directly from your\nhashtag\n#\nSQL\nqueries.\nUsing this tool, you can pour your data into the model and save its predictions using\nhashtag\n#\nSQL\ndirectly interacting with your database. No other data pipelines are required.\nIt is as easy as running:\n\"\"\"\nCREATE PREDICTOR mindsdb.customer_churn_predictor\nFROM files\n(SELECT * FROM churn)\nPREDICT Churn;\n\"\"\"\nUnder the hood, by default, they use a proprietary\nhashtag\n#\nAutoML\nengine to find the best solution for your data.\nYou can customize this behavior with your models and hyper-parameters, but internally they support many operations.\nFor example, when customizing, you can use your trained model from\nhashtag\n#\nMLFlow\nto make predictions directly into your database.\nThis tool can help you avoid writing unnecessary data and machine learning pipelines when your infrastructure heavily depends on SQL databases.\nYou can use this tool as SasS or self-hosted.\n.\nCheck out their docs:\nhttps://docs.mindsdb.com/\n.\n.\n👋 If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dxV7Dnqv"
        },
        "Post_156": {
            "text": "As a\nhashtag\n#\nML\nengineer, did you ever feel overwhelmed by everything you have to know?\n.\n.\nProbably, you are not the only one.\nI often have to relearn all kinds of concepts if I didn't use them for a long time.\nThe problem is that sometimes you can't find that priceless piece of information that will revive your\nhashtag\n#\nML\nmodel.\nSometimes, the sea of information, the internet, is just too daunting.\nRecently, I read a book that is meant, in such moments, to light up your day.\nIt is called \"Why Does My Neural Network Not Learn\" by Mark Schutera and Frank Hafner.\nIt includes best practices from applying\nhashtag\n#\ndeeplearning\nin\nhashtag\n#\nresearch\nand\nhashtag\n#\ndevelopment\n.\nBecause it is a compact book, I think it is excellent to quickly refresh your memory on specific concepts while developing your\nhashtag\n#\nML\nsystem.\nI think it is perfect for\nhashtag\n#\nML\npractitioners to use it as their daily knowledge index.\n☕  I completely resonated with this quote:\n\"What applause is for the musician, reviews are for the writer, and coffee for the engineer.\"\n👋 If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_157": {
            "text": "Have you ever wondered how hard it is to switch from\nhashtag\n#\nPandas\n+\nhashtag\n#\nSklearn\nto\nhashtag\n#\nSpark\n?\n.\n.\nWell... Not that hard.\nThere are a few things to understand while using\nhashtag\n#\nSpark\n, like pure functions and lazy initialization.\nIt is a different ecosystem, but within\nhashtag\n#\nPySpark\n, we have access to Spark DataFrames & Spark ML interfaces.\nThe guys from Spark tried to keep their interface as close as possible to Pandas and Sklearn (at least for PySpark).\nYou can see that for yourself in the following article, where I created an end-to-end tutorial for churn prediction using only\nhashtag\n#\nSpark\n.\nThe tutorial will take you from how you can load a data source to how to do feature engineering and run models with cross-validation.\n.\n🎉 I want to thank\nTowards AI\nfor considering this article in their \"Top 3 Best Articles\" of their weekly newsletter.\nTheir newsletter is a great way to stay in touch with the latest updates from the AI world:\nhttps://lnkd.in/gG4upkCD\n.\n→ Check out the article:\nhttps://lnkd.in/grQdudF5\n.\n👋 If you want to read weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_158": {
            "text": "This is how you can quickly write context managers (resources accessed by the \"with\" keyword)  in\nhashtag\n#\nPython\n.\n.\n.\nThe most known way to write 𝐜𝐨𝐧𝐭𝐞𝐱𝐭 𝐦𝐚𝐧𝐚𝐠𝐞𝐫𝐬 in\nhashtag\n#\nPython\nis using 𝐜𝐥𝐚𝐬𝐬𝐞𝐬 that implement the __𝐞𝐧𝐭𝐞𝐫__() and __𝐞𝐱𝐢𝐭__() methods.\nI recently stumbled into a lighter way to achieve the same behavior.\nYou can use the @𝐜𝐨𝐧𝐭𝐞𝐱𝐭𝐦𝐚𝐧𝐚𝐠𝐞𝐫 𝐝𝐞𝐜𝐨𝐫𝐚𝐭𝐨𝐫 over any 𝐟𝐮𝐧𝐜𝐭𝐢𝐨𝐧, as shown in the pinned image.\n➙ Besides decorating the function, there are four things that you have to consider within your function:\n1. Acquire your resource within the function.\n2. Use a \"try...except...finally\" block.\n3. Return your resource using the \"yield\" keyword within the \"try\" block (not by using \"return\").\n4. Release your resource within the \"finally\" block.\n➙ When you call the function with the \"with\" keyword,\nhashtag\n#\nPython\nwill do the following:\n1. Execute all the code until the \"yield\" statement.\n2. Return your resource.\n3. When the \"with\" block ends, it will execute the code starting from the \"yield\" statement until the end of the function.\nAnd that is all that you need to know!\n➙ You may ask, why does this even matter?\nWell is just syntactic sugar!\nThis method is better than classes when you don't have a lot of logic and want to keep things clean while using the \"with\" statement.\nOtherwise, if your logic is pretty concise, your code would be over 50% boilerplate code, using the standard syntax that uses classes.\nAlso, from my perspective, the logic flow is easier to follow within a single function.\n👋 If you want to read my weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_159": {
            "text": "Fully autonomous cars are still not quite ready for scale.\n… but by the time I retire, it’s highly likely I’ll be able to get in my car in the evening, tell it where to go, read a bit, watch a movie, go to sleep… and then wake up the next morning at my destination.\nI find this fascinating.\nVolkswagen just released this thought-provoking concept design. They call is ‘Gen Travel’. It envisions what future travel might look like once fully autonomous fully electric technology is ready.\nI particularly love the seats that turn into beds, the ambient lighting, and the 360 degree views.\n… and imagine all the hours we will be able to free up.\nHard not to be excited about the possibilities ahead.\nhashtag\n#\ntechnology\nhashtag\n#\ninnovation\nhashtag\n#\nmobility\nhashtag\n#\nautomotive"
        },
        "Post_160": {
            "text": "From Web 2.0 to Web3 🚀\nThe main distinction between Web 2.0 and Web 3 is that Web 3.0 is built on decentralization ⛓\nFurthermore, users will own their content and have complete control over using the internet 🔑\nThe possibilities for this new technology to completely change how users, developers, and brands all interact are significant 📊\n“I've become so numb I can't feel you there\nBecome so tired So much more aware I'm becoming this…” 🎶\n☀️🍫🍌😴"
        },
        "Post_161": {
            "text": "Yet another post of an\nhashtag\n#\nAI\ngenerating some incredible images.\nI generated a random image with the stable diffusion model, open-sourced by\nstability.ai\n.\nWhat they did is incredible!\nI would personally put this image on my wall.\nI guess this is amazing and scary at the same time.\n👋 If you want to read my weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_162": {
            "text": "How do you write an excellent\nhashtag\n#\nREADME\nfile that catches people's attention?\n.\n.\nYou know the saying, \"The cover sells the book.\"\nThis is also true for public repositories.\n\"The README sells the code.\"\nEvery README, to look professional, should cover at least the following:\n1.  What your project does:\nA concise, single-paragraph describing your project + A screenshot or even an animated GIF.\n2.  How to install it:\nA code block in your README that shows step-by-step what to type in the shell to set up the whole project (From my experience, this is the most crucial step.).\n3.  Example usage:\nAdd a few snapshots that will show the user how to use the core of your work.\n4.  How to set up the dev environment:\nDescribe how to install the development dependencies and how to run the tests.\n5.  How to contribute:\nA description of the development process.\n6.  Change log\nShare the changes to the project + Give credit to contributors publicly.\n7.  License, Author Information, and Acknowledgements\nWrite a statement about the project's license, the contact information of the author, and acknowledgments to contributors or other 3rd party vendors.\nOther possible sections for your\nhashtag\n#\nML\nproject:\n- Data (describe the datasets you used and how to download them)\n- Results (if you trained any models, show your evaluation results in a table)\n👋 If you want to read my weekly insights into\nhashtag\n#\nmachinelearning\n,\nhashtag\n#\nsoftware\n, and\nhashtag\n#\nmlops\n, follow me on LinkedIn and Medium:\nhttps://lnkd.in/dvB39TmV"
        },
        "Post_163": {
            "text": "Excited that I have the chance to work on coreControl 🎉.\nhashtag\n#\nml\nhashtag\n#\nmlops\nhashtag\n#\nai"
        },
        "Post_164": {
            "text": "From research to production and back for continuous improvement - Training and maintaining AI Models requires managing several MLOps functions:\n▪\tExperiment Management / KPIs\n▪\tData Versioning\n▪\tGithub version\n▪\tModel Serving\n▪\tModel Monitoring\n▪\tData Monitoring\n▪\tFinOps and Infrastructure\nRead more:\nhttps://lnkd.in/etihQ8am\nhashtag\n#\nmlops\nhashtag\n#\nfinops\nhashtag\n#\nversioncontrol\nhashtag\n#\naiops\nhashtag\n#\nai\nhashtag\n#\ncontinouslearning\nhashtag\n#\nmodelsearch\nhashtag\n#\nmonitoring"
        },
        "Post_165": {
            "text": "If you are interested in\nhashtag\n#\nai\n&\nhashtag\n#\nscience\nand psychology & the metaphysical, you need to check out this podcast.\nIt is a great discussion that takes into consideration both planes.\nCheers to\nLex Fridman\nfor his unique content and Jordan Peterson for his out-of-the-box concepts."
        },
        "Post_166": {
            "text": "Here's my conversation with Jordan Peterson about Nietzsche, Dostoevsky, Putin, the search for meaning, the corrupting effects of fame & power, and advice on how to think and how to live. This conversation was intense, challenging, and fascinating."
        },
        "Post_167": {
            "text": "😿 You don't have enough labeled data?\n🤖 Check out this Python package that will change your ML game.\n𝐦𝐨𝐝𝐀𝐋 is a Python tool/package for 𝐚𝐜𝐭𝐢𝐯𝐞 𝐥𝐞𝐚𝐫𝐧𝐢𝐧𝐠.\nBased on previous data samples, the package offers the developer out-of-the-box strategies to 𝐥𝐚𝐛𝐞𝐥 𝐮𝐧𝐬𝐞𝐞𝐧 𝐝𝐚𝐭𝐚 in a semi-supervised fashion.\nThe package is built around 𝐭𝐰𝐨 𝐦𝐚𝐢𝐧 𝐜𝐨𝐧𝐜𝐞𝐩𝐭𝐬:\n- an estimator (e.g., RandomForestClassifier)\n- a query strategy\n\"\"\"\nlearner = ActiveLearner(\nestimator=RandomForestClassifier(),\nquery_strategy=uncertainty_sampling\n)\n\"\"\"\nAn ActiveLearner class is a wrapper over a given estimator. The 𝐞𝐬𝐬𝐞𝐧𝐭𝐢𝐚𝐥 method it exposes is 𝐪𝐮𝐞𝐫𝐲(𝐗).\nYou can quickly 𝐥𝐚𝐛𝐞𝐥 your 𝐧𝐞𝐰 𝐢𝐧𝐜𝐨𝐦𝐢𝐧𝐠 𝐝𝐚𝐭𝐚 using the given query strategy. The package supports out of the box, the following processes (but you can implement your strategy):\n- max uncertainty sampling\n- max margin sampling\n- entropy sampling\nOne use-case where the tool can easily be implemented and adds value is 𝐏𝐨𝐨𝐥-𝐛𝐚𝐬𝐞𝐝 𝐬𝐚𝐦𝐩𝐥𝐢𝐧𝐠.\n📝 The 𝐞𝐱𝐚𝐦𝐩𝐥𝐞 from modAL states the following:\nWe split the good old iris dataset into a 𝐭𝐫𝐚𝐢𝐧𝐢𝐧𝐠 𝐬𝐞𝐭 𝐋 with only three random examples and into an 𝐮𝐧𝐥𝐚𝐛𝐞𝐥𝐞𝐝 𝐬𝐞𝐭 𝐕 (we drop the targets for the sake of the example).\nWe used a simple KNeighborsClassifier with three neighbors.\nInitially, the model was 𝐭𝐫𝐚𝐢𝐧𝐞𝐝 𝐨𝐧𝐥𝐲 𝐨𝐧 𝐭𝐡𝐫𝐞𝐞 𝐬𝐚𝐦𝐩𝐥𝐞𝐬 and performed poorly.\nAfter, 𝐢𝐭𝐞𝐫𝐚𝐭𝐢𝐯𝐞𝐥𝐲, we 𝐚𝐝𝐝𝐞𝐝 𝟐𝟎 𝐬𝐚𝐦𝐩𝐥𝐞𝐬, one by one. We can see in the graph below that only after four samples were 𝐥𝐚𝐛𝐞𝐥𝐞𝐝 𝐛𝐲 𝐦𝐨𝐝𝐀𝐋 did the 𝐚𝐜𝐜𝐮𝐫𝐚𝐜𝐲 𝐠𝐫𝐨𝐰 𝐫𝐚𝐩𝐢𝐝𝐥𝐲.\nThe performance is fascinating.\nIt has support for Sklearn, Keras and Pytorch.\nI strongly consider using this package in my current projects.\n❓What is your opinion about it? Would you trust such a method for your semi-supervised use cases?\n🐍 Python Package:\nhttps://lnkd.in/d9MeJ9X2\n✅ Pool-based sampling detailed example:\nhttps://lnkd.in/dXpD3czr"
        },
        "Post_168": {
            "text": "👀 This Is what you need to know to build an MLOps end-to-end architecture:\n🦾 I wrote a Medium article in which I presented 7 simple steps to consider when designing your MLOps infrastructure to ensure scalability, shareability, and reusability.\n1. ML development\n2. Training operationalization\n3. Continuous training\n4. Model deployment\n5. Prediction serving\n6. Continuous monitoring\n7. Data & model management\n📢 If you enjoyed this article and want to read more similar content, please support me by following me on Medium -\nhttps://lnkd.in/dvB39TmV\nhashtag\n#\nmachinelearning\nhashtag\n#\nmlops\nhashtag\n#\ndata\nhashtag\n#\nsoftwarearchitecture"
        },
        "Post_169": {
            "text": "Is climate change a myth or reality?\nThis is an animation that shows the temperature change by country from the years 1880-2021.\nI saw some discussions around whether this size of sample data is reasonable for us to have an assumption of climate change. Compare with the age of the earth, the data we have only covered less than 0.0000038% of the time, however, the first reliable mercury thermometer has been invented just around 300 years ago.\nWe always say that let data speak for itself, and in an ideal condition, we collected sufficient related unbiased data to make a correct conclusion/ prediction.\nThe reality is that most of the time, we just use the data we can get as much as possible  (with quality control), form a ‘biased’ understanding, then make the ‘best’ business decision we could.\nUsually, we just try to use the imperfect dataset to make a decision that benefits the current situation the most. Nothing is perfect in this process. But I guess this is also the charm of data science that we can always do better.\nAnimation by Antti Lipponen, source is NASA GISTEMP.\nFor ML/ AI/ Data Science learning materials, please check my previous posts.\nI share my learning journey into Data Science with my amazing LinkedIn friends, please follow me and let's grow together!\nAlex Wang\nhashtag\n#\ndatascience\nhashtag\n#\ndataanalysis"
        },
        "Post_170": {
            "text": "Smallest chef in the world makes your food in front of you. Would you like to go?\nImmersive dining experience from Le Petit Chef utilising 3D mapping and projection technology. 35 locations across the world\nFive-course meal will set you back £50-110 depending on location.\nMore info:\nhttps://lepetitchef.com/\n>>> We need more innovation everywhere <<<\nWeekly innovation newsletter sign up:\nhttps://lnkd.in/gkMMwdws\nhashtag\n#\nfood\nhashtag\n#\nimmersiveexperience\nhashtag\n#\ninnovation"
        },
        "Post_171": {
            "text": "Check out this Python syntactic sugar method that could benefit you in producing cleaner code.\n👇\n📃 In the standard fashion, we must loop through the whole list when we want to find an object. When we see it, we break out of the loop. To keep the state of the query, we create a flag. At the end of the for loop, if we do not find the query object, we perform a specific action.\n📃 You can compress this logic into a single \"else\" statement using the Python syntactic sugar. We wrote the for loop in the standard way. If we find the query object and hit the break statement, we won't execute the \"else\" block. Otherwise, we will complete the \"else\" block if we don't reach the break statement.\n✅ I think this is a fantastic way to write fewer lines of code. Therefore, less code to manage.\n❌ But, I also think your code can quickly get obscure and hard to read if you abuse this method.\n❔ What is your opinion about this syntactic sugar method?\nhashtag\n#\npython\nhashtag\n#\ncleancode\nhashtag\n#\nprogramming\nhashtag\n#\nlearn"
        },
        "Post_172": {
            "text": "Agari biodegradable bottles  🌱\nWe need more of such innovations. What do you think?\nInvented by product designer Ari Jonsonn\nhashtag\n#\ninnovation\nhashtag\n#\ntech\nhashtag\n#\ngreen\nhashtag\n#\nsustainable\nhashtag\n#\ngreentech"
        },
        "Post_173": {
            "text": "👀 I have recently read a blog on Udacity about how to actively find jobs that you are passionate about, aka your dream job.\n📃 They present a method called the Visionary Approach. It consists of four main steps:\n1. Choose your target company (domain, size, etc.);\n2. Research as much as possible about it (both the company and the people that work at it). Build a targeted network;\n3. Learn where you can fit into that companies plan and build/learn something valuable to them;\n4. Show off your new expertise.\n✨ To me was very insightful.\n❔What is your opinion about this approach?\nLink to the blog post:\nhttps://lnkd.in/dHNqCzpM\nhashtag\n#\nwork\nhashtag\n#\njob\nhashtag\n#\nreading\nhashtag\n#\nselfgrowth\nhashtag\n#\ninsights"
        },
        "Post_174": {
            "text": "Moving from BI to AI can be easy with CoreAI.\nWe build AI pipeline and give your organization and data analysts a head start, as well as becoming independent in generating more accurate and up to date models later.\nIf you want to stay up-to-date with us, follow our Page.\nThis is a short clip showing how:\nhttps://lnkd.in/erxh8BKg"
        },
        "Post_175": {
            "text": "Machine Learning in action 🤣\nhashtag\n#\nHappyWeekend\nCredit: @alvinfoo"
        },
        "Post_176": {
            "text": "What are you waiting for?\nThat's the fundamental question\nhashtag\n#\nWarrenBuffett\nposes in this amazing clip. And the honest answer for many aspiring business owners is fear of the unknown.\n\"I just don't know where to begin.\"\nWell, I have some exciting news today that goes right to the heart of this dilemma!\nI'll be the host and an executive producer of a new unscripted series on CNBC called “Business Hunters” that will guide everyday Americans through the journey of buying their own small business. There's simply never been a show like this before.   We will pull back the curtain on how to find the right business within budget, breakdown the numbers, and negotiate a deal – all while revealing the joy and sacrifices in making the transition from employee to employer. To navigate the process from beginning to end, I'll be drawing upon the expertise of business broker and entrepreneur, Mayumi Muller.\nComing on the heels of the greatest mass exodus of American workers in modern history, “Business Hunters” is being developed by award-winning producer and Chairman of Worldwide Television at MGM, Mark Burnett and Evolution Media. (See link to press release in the comments)\nOn a personal note, I hope this show will serve as a bridge from thought to action for millions of people around the world who are cradling a dream but are somehow stuck. I know how hard it is to come from nothing, muster the courage to quit the grind, leave your fears behind, and bet on yourself.  I believe “Business Hunters” will empower a whole new generation of aspiring business owners by demystifying this opaque process.\nI'm so grateful and honored that Mark Burnett and Barry Poznick at MGM, and Mark Hoffman and Denise Contis at CNBC have entrusted me to host this life-changing show.\nWatch out for “Business Hunters” coming this Fall!\nhashtag\n#\nBusinessHunters\nhashtag\n#\nCNBC\nhashtag\n#\nentrepreneurship"
        },
        "Post_177": {
            "text": "It’s time we change the conversation 💛"
        },
        "Post_178": {
            "text": "An alternative way to imagine PCA:\nMapping from one plane to another is basically matrix regression. This might help you create an intuition of PCA.\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning\ncredit:\nhttps://lnkd.in/giEeva_B\nby Karl Rohe"
        },
        "Post_179": {
            "text": "Someone just sent $300,000,000 of\nhashtag\n#\nBitcoin\nin a single transaction.\nTransaction fee: $0.12\nNo government, bank, or third party could have stopped this if they wanted to.\nThis is the power of Bitcoin."
        },
        "Post_180": {
            "text": "Have you ever wondered how GitHub Copilot 🤖 is performing? Then check out this article where I share my coding experience with it.\nI think it is a great addition to my developing tool stack 🦾.\nhashtag\n#\npython\nhashtag\n#\ndeveloper\nhashtag\n#\nsoftwareengineer\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence\nhashtag\n#\npycharm"
        },
        "Post_181": {
            "text": "An excellent illustration of how Convolutional Neural Networks (CNN) work\nFrom one picture, you end up with four understandable images. The reduction in resolution is a fundamental step in CNN to accelerate processing time\nCredit: Kensuke Koike\nhashtag\n#\nartificialintelligence"
        },
        "Post_182": {
            "text": "The data-Centric approach is taking over the AI development world!\nOne of its key elements is MLOPS.\nIn this blog post, we shared some of our experiences of how important it is!\nNot only for enterprises but small, medium, and startup companies."
        },
        "Post_183": {
            "text": "How cool is the live wallpaper - yes or no?"
        },
        "Post_184": {
            "text": "La carte de visite du futur !\nPar 川田十夢 :\nbit.ly/37nbxGx"
        },
        "Post_185": {
            "text": "Our brains are wired to compare. From the Predictably Irrational videobook by\nLIT Videobooks\nWatch more at\nhttps://lnkd.in/dwpdUDPy"
        },
        "Post_186": {
            "text": "Back in my day...\n.\n.\n.\n.\n.\n.\n.\nOriginal: Mundher Alshabi"
        },
        "Post_187": {
            "text": "💡 A sealed class cannot be inherited. Any attempt to do so will result in a compile error.\n👉 You can still attach additional behavior to immutable classes via extension methods, but you're limited to their public members in your implementation (unless both the class and the extension live in the same assembly, which gives you access to internal members as well).\nhashtag\n#\ndotnet\nhashtag\n#\ncsharp"
        },
        "Post_188": {
            "text": "I started using GitHub Copilot a few days ago.\nIt is a great addition to your IDE for your daily coding hours.\nI tested it by writing some Pandas code in Python. After using it for a few hours it picked up on my coding style. It is a lot more than just an autocompletion tool. I just typed a few letters and it suggested the whole line of code, with logic and syntax sugar. Of course, the great part is that its suggestions are also correct. You can accept the autocomplete with the tab key.\nSay goodbye to copy-paste from StackOverflow and hello to the tab key.\nhashtag\n#\npython\nhashtag\n#\npandas\nhashtag\n#\ncoding\nhashtag\n#\ngithubcopilot"
        },
        "Post_189": {
            "text": "LED fan holographic projection\nAmazing technology"
        },
        "Post_190": {
            "text": "We spend too much time thinking about the black lines, and not enough on the green ones.\nIt’s never too late to make a career pivot or learn something new.\nWhat’s the secret?\n✅ Learn about “new paths” - follow people in new fields, watch videos, read more\n✅ Surround yourself with people who encourage and empower you\n✅ Believe in yourself, even when people say otherwise\nFollow me on Instagram for more 👉\nhttps://lnkd.in/e2jEHwk\nImage source:\nTim Urban"
        },
        "Post_191": {
            "text": "Check out this Medium article about a short introduction to Yacht. A Python framework based on Machine Learning for automating your orders, as buy and hold investors, into the stock market.\nhashtag\n#\ntradingbot\nhashtag\n#\nreinforcementlearning\nhashtag\n#\ndeeplearning\nhashtag\n#\nmachinelearning\nhashtag\n#\nfinance"
        },
        "Post_192": {
            "text": "Penguins at 5x speed. I have no idea why, but at this late hour, I find this both hilarious and mesmerizing. Earth life is weird and beautiful."
        },
        "Post_193": {
            "text": "Came across this wonderful video posted by\nThi Hien Nguyen\n.\nA perspective is just a partial interpretation of reality. Not only in life, it is very much applicable in Data Science too 😃\nWhenever I come across stats being used with *absolute* certainty, I always remind myself that Statistics is not Arithmetic. 2 + 2 is not always 4.\nInterpreting data is much more nuanced. Data aggregations or visualizations have to be interpreted with care.\nAlways think: what is hiding in plain sight.\nCheck out the resources in the first comment.\n👉 For more content on Machine Learning for Developers, subscribe to\nhashtag\n#\nML4Devs\nnewsletter.\nhashtag\n#\nBigData\nhashtag\n#\nDataEngineering\nhashtag\n#\nDataScience\nhashtag\n#\nMachineLearning\nhashtag\n#\nMLOps\nhashtag\n#\nCloudComputing\nhashtag\n#\nMicroservices"
        },
        "Post_194": {
            "text": "hashtag\n#\nmachinelearning\nhashtag\n#\ndata\nhashtag\n#\nfinance"
        },
        "Post_195": {
            "text": "Check out my Medium article about how to make time series stationary without losing memory."
        },
        "Post_196": {
            "text": "Fractionally Differentiated Features to Preserve Memory in Stationary Time Series\nHow to Properly Preprocess Data for Machine Learning Trading Bots\nby\nPaul Iusztin\n🔵\nhttps://lnkd.in/e5JgTD-2\nAI is everywhere, but the question is 🟠 how much do you love it?\nhttps://lnkd.in/dd3EKnfv\nhashtag\n#\nart\nhashtag\n#\nmachinelearning\nhashtag\n#\ndeeplearning\nhashtag\n#\nartificialintelligence\nhashtag\n#\ndatascience\nhashtag\n#\niiot\nhashtag\n#\ndevops\nhashtag\n#\ndata\nhashtag\n#\nMLsoGood\nhashtag\n#\ncode\nhashtag\n#\npython\nhashtag\n#\nbigdata\nhashtag\n#\nMLart\nhashtag\n#\nalgorithm\nhashtag\n#\nprogrammer\nhashtag\n#\npytorch\nhashtag\n#\nDataScientist\nhashtag\n#\nAnalytics\nhashtag\n#\nAI\nhashtag\n#\nVR\nhashtag\n#\niot\nhashtag\n#\nTechCult\nhashtag\n#\nDigitalart\nhashtag\n#\nDigitalArtMarket\nhashtag\n#\nlearning\nhashtag\n#\nml\nhashtag\n#\ntrading"
        },
        "Post_197": {
            "text": "The carbon is pumped deep underground, where it turns into rock.\nLearn more about this incredible new plant:\nhttp://ow.ly/e8rj50HwPcR\nhashtag\n#\ndavosagenda"
        },
        "Post_198": {
            "text": "We are working with the zero-code smart contract generation platform MyWish to enable anyone to write their own Elrond smart contracts without any programming skills. The platform offers a number of predefined smart contracts templates that allow you to create your own token, host your own token sale and airdrop, perhaps even get married on the blockchain.\nhashtag\n#\nmission10\nDay 20.\nhttps://lnkd.in/e9KafJ_5"
        },
        "Post_199": {
            "text": "“Keep focused on the step in front of you. Nothing else.”\n~Bear Grylls"
        },
        "Post_200": {
            "text": "Anyone can learn to be a great leader – it’s a practice and the first requirement is you have to want to be one. Our team invites you into our virtual classroom to unpack the concepts in \"Leaders Eat Last.\" Join us:\nhttps://bit.ly/3ecNb2s"
        },
        "Post_201": {
            "text": "Amazing! Researchers from\nUniversity of California, San Francisco\nused\nhashtag\n#\nAI\n+\nNVIDIA\nGPUs to give a paralyzed man the ability to communicate by translating his brain signals into computer-generated writing. Read more:\nhttps://nvda.ws/3exANKm"
        },
        "Post_202": {
            "text": "Amazing innovation\nI am ready to hit the road - would you?"
        },
        "Post_203": {
            "text": "Surgical robots are coming.\nUK robotics start-up CMR Surgical just raised $600m, the largest funding round in MedTech history. 🔥\nTheir mission?\nTo make keyhole surgery universally accessible and affordable through next-gen surgical robots.\nDemand for robotic surgery is growing rapidly, and CMR partners with hospitals around the world to provide surgeons with unprecedented precision and accuracy through robot assistants.\nThey launched their first system, Versius (seen below), back in 2019, and their pioneering team in Cambridge already counts 700 people. This new funding will help them scale globally.\nHuge congrats to the whole team at CMR. 👏\nhashtag\n#\ntechnology\nhashtag\n#\ninnovation\nhashtag\n#\nmedtech"
        },
        "Post_204": {
            "text": "The Pena Palace in Sintra, Portugal 🏰\n📷: DoublekickOnesnare"
        },
        "Post_205": {
            "text": "A powerful picture, You Change yourself and nobody will do this for you💪\nhashtag\n#\njerinlovescreatures\nPC 👉 FB"
        },
        "Post_206": {
            "text": "Ego is a drug. Use in moderation, if at all."
        },
        "Post_207": {
            "text": "World of Warcraft and Fortnite have been played for over 141 billion hours. Maybe Earth is an online video game played by an alien civilization, each human controlled by an alien gamer. Population of Earth increases to account for the growing popularity of the game."
        },
        "Post_208": {
            "text": "Companies\n-2019 You NEED to be in office.\n-2020 Please work from home.\n-2021 You NEED to come back to office.\nGreat Companies\n-2019 You CAN work from home 1-2 days a week.\n-2020 Please work from home.\n-2021 work from where it WORKS for you.\nFlexibility to work from home is going to be the key difference in the coming years.\nAfter 2020, if companies are not able to trust their employees, they will NEVER be.\nGreat work cultures are built on TRUST.\nAgree?\nhashtag\n#\nworkfromhome\nhashtag\n#\ncareer\nhashtag\n#\nculture"
        },
        "Post_209": {
            "text": "I’ve never seen a set of Machine Learning Cheat Sheets look so cool and clearly explained. You need to keep those handy!\nThanks to\nAqeel Anwar\n, who’s keeping these doc versions updated, for creating them.\nSubjects include:\n🔹Bias Variance Trade-off\n🔹Imbalanced Data in Classification\n🔹Principal Component Analysis\n🔹Bayes’ Theorem and Classifier\n🔹Regression Analysis\n🔹Regularization in ML\n🔹Convolutional Neural Network\n🔹Famous CNNs\n🔹Ensemble Methods in Machine Learning\n🔹STAR Method for Behavioral Interview prep\n🔹How to Answer Behavioral Questions\nHope you enjoy!\nhashtag\n#\ndatascience\nhashtag\n#\nmachinelearning\nhashtag\n#\nartificialintelligence\nClick\nhashtag\n#\nlinkedangle\nand follow for more content\nFollow\nGreg Coquillo"
        },
        "Post_210": {
            "text": "Your brain works differently when you take breaks 🧠⚠️\nMicrosoft researched the impact of back-to-back video call meetings on our stress levels. The cool blues show consistent levels with breaks - the warmer reds show increasing stress levels with no breaks 🚨\nThe takeaways 👇🏽\n1️⃣ Breaks between meetings allow the brain to “reset,” reducing a cumulative buildup of stress across meetings.\n2️⃣ Back-to-back meetings can decrease your ability to focus and engage.\n3️⃣ Transitioning between meetings can be a source of high stress.\nMy best ideas come when I'm not in meetings or in an office. I've taken up DJ-ing and gymming every day to get away from my laptop and honestly, it's when I have my best ideas.\nHave you found your stress levels increased during the last year of virtual meetings? and how do you spend your 'breaks'? 🤯"
        },
        "Post_211": {
            "text": "Source:\nhttps://lnkd.in/ejFRgYA"
        },
        "Post_212": {
            "text": "Check out part two in this series on leveraging\nMathWorks\nto support workflows on NVIDIA DRIVE Sim, powered by\nNVIDIA Omniverse\n:\nhttps://nvda.ws/3tJd7rf"
        },
        "Post_213": {
            "text": "Electromyography has the potential to revolutionize how we interact with computers.\nImagine typing without a keyboard, controlling visual interfaces without a mouse, and navigating AR/VR environments much more intuitively.\nThe signals in our wrists are so clear that EMG can detect finger motion of just a millimeter.\nThis video highlights some of the groundbreaking research by the incredible CTRL-Labs team who joined us last year. 🤩\nMind-blowing potential.\nhashtag\n#\ninnovation\nhashtag\n#\ntechnology\nhashtag\n#\nfacebook"
        },
        "Post_214": {
            "text": "Come to see how you can be a part of a place of innovation, with projects like:\n➡️\nlek3.co\n, charging electric cars anywhere. Quick and easy.\n➡️\n2park.io\n, smart parking at your fingertips and\n➡️ SafeFleet, innovating the telematics industry\nWe're waiting for you! :)\n➡️ ➡️ ➡️\nhttps://lnkd.in/g-sEMiW\nhashtag\n#\niot\nhashtag\n#\ndesign\nhashtag\n#\nfuture\nhashtag\n#\nhr\nhashtag\n#\nteam\nhashtag\n#\nsmartcities\nhashtag\n#\nprojects\nhashtag\n#\nelectriccars\nhashtag\n#\ntelematics\nhashtag\n#\nparks\nhashtag\n#\nfuturism\nhashtag\n#\ncharging\nhashtag\n#\nsoftware\nhashtag\n#\nprojecting\nhashtag\n#\nsafefleet\nhashtag\n#\nrecruting\nhashtag\n#\nlek3\nhashtag\n#\nlek3co"
        }
    }
}