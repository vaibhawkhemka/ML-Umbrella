{
  "_id": {
    "$oid": ""
  },
  "originalName": "Journal_2Column_ACM.pdf",
  "path": "",
  "user": {
    "$oid": ""
  },
  "processed": "",
  "pinecone": "",
  "deleted": "",
  "fileHash": "",
  "figures_contents": "",
  "contents": [
    {
      "text": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models  HILA CHEFER\u2217, Tel-Aviv University, Israel YUVAL ALALUF\u2217, Tel-Aviv University, Israel YAEL VINKER, Tel-Aviv University, Israel LIOR WOLF, Tel-Aviv University, Israel DANIEL COHEN-OR, Tel-Aviv University, Israel  +Attend-  and-Excite  Stable   Diffusion  \u201cA painting of an  elephant with glasses\u201d  \u201cA playful kitten chasing a  butterfly in a wildflower meadow\u201d \u201cA horse and a dog\u201d  Fig. 1. Given a pre-trained text-to-image diffusion model (for example Stable Diffusion [Rombach et al 2022]) our method, Attend-and-Excite, guides the generative model to modify the cross-attention values during the image synthesis process to generate images that more faithfully depict the input text prompt. Stable Diffusion alone (top row) struggles to generate multiple objects (for example a horse and a dog). However, by incorporating Attend-and-Excite (bottom row) to strengthen the subject tokens (marked in blue), we achieve images that are more semantically faithful with respect to the input text prompts.  Recent text-to-image generative models have demonstrated an unparalleled ability to generate diverse and creative imagery guided by a target text prompt. While revolutionary, current state-of-the-art diffusion models may still fail in generating images that fully convey the semantics in the given text prompt. We analyze the publicly available Stable Diffusion model and assess the existence of catastrophic neglect, where the model fails to generate one or more of the subjects from the input prompt. Moreover, we find that in some cases the model also fails to correctly bind attributes (for example colors) to their corresponding subjects. To help mitigate these failure cases, we introduce the concept of Generative Semantic Nursing (GSN), where we seek to intervene in the generative process on the fly during inference time to  \u2217Denotes equal contribution.  Authors\u2019 addresses: Hila Chefer, Tel-Aviv University, Israel, hilach70@gmail[dot]com; Yuval Alaluf, Tel-Aviv University, Israel, yuvalalaluf@gmail[dot]com; Yael Vinker, Tel-Aviv University, Israel, yael[dot]vinker@mail[dot]huji[dot]ac[dot]il; Lior Wolf, Tel-Aviv University, Israel, liorwolf@gmail[dot]com; Daniel Cohen-Or, Tel-Aviv University, Israel, cohenor@gmail. com.  Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm[dot]org. \u00a9 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM. 0730-0301/2023/8-ART148 $15[dot]00 https://doi[dot]org/10[dot]1145/3592116  improve the faithfulness of the generated images. Using an attention-based formulation of GSN, dubbed Attend-and-Excite, we guide the model to refine the cross-attention units to attend to all subject tokens in the text prompt and strengthen \u2014 or excite \u2014 their activations, encouraging the model to generate all subjects described in the text prompt. We compare our approach to alternative approaches and demonstrate that it conveys the desired concepts more faithfully across a range of text prompts. Code is available at our project page: https://attendandexcite[dot]github[dot]io/Attend-and-Excite/[dot]  CCS Concepts: \u2022 Computing methodologies \u2192Computer graphics; Image processing.  Additional Key Words and Phrases: Image Generation, Diffusion Models  ACM Reference Format: Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-Excite: Attention-Based Semantic Guidance for Text-to- Image Diffusion Models. ACM Trans. Graph. 42, 4, Article 148 (August 2023), 10 pages. https://doi[dot]org/10[dot]1145/3592116  1 INTRODUCTION  Recent advancements in text-based image generation [Balaji et al 2022; Gafni et al 2022; Ramesh et al 2022; Rombach et al 2022; Saharia et al 2022] have demonstrated an unprecedented ability to generate diverse and creative imagery provided a free-form text prompt. However, it has been shown [Feng et al 2022; Wang et al  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 1
    },
    {
      "text": "148:2 \u2022 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or  \u201cA yellow bowl and a blue cat\u201d \u201cA yellow bow and a brown bench\u201d  Stable Diffusion  SD with  A&E  Catastrophic Neglect Incorrect Attribute Binding  Fig. 2. Failure cases of Stable Diffusion (SD) [Rombach et al 2022]. In the top row, we show examples of two failure settings: catastrophic neglect (left) and incorrect attribute binding (right). In the bottom row, we show images obtained when applying Attend-and-Excite over SD using the same seeds.  2022] that images produced by such models do not always faithfully reflect the semantic meaning of the target prompt. We observe two key semantic issues in state-of-the-art text-based image generation models: (i) \u201ccatastrophic neglect\u201d, where one or more of the subjects of the prompt are not generated; and (ii) in- correct \u201cattribute binding\u201d, where the model binds attributes to the wrong subjects or fails to bind them entirely. Examples of cases where the aforementioned issues arise can be found in the top row of Figure 2, which depicts images generated by the state-of-the-art Stable Diffusion model [Rombach et al 2022]. In the left column, we provide an example of catastrophic neglect where the model fails to generate the blue cat, choosing to focus solely on generating the bowl. In the right column, we demonstrate incorrect attribute binding where the color \u201cyellow\u201d is incorrectly binded to the bench. To mitigate these semantic issues, we introduce the concept of \u201cGenerative Semantic Nursing\u201d (GSN). In the GSN process, one slightly shifts the latent code at each timestep of the denoising process such that the latent is encouraged to better consider the semantic information passed from the input text prompt. We propose a form of GSN dubbed Attend-and-Excite, which lever- ages the powerful cross-attention maps of a pre-trained diffusion model. The attention maps define a probability distribution over the text tokens for each image patch, which determines the dominant tokens in the patch. We observe that this text-image interaction is susceptible to neglect. Although each patch can attend freely to all text tokens, there is no mechanism to ensure that all tokens are at- tended to by some patch in the image. In cases where a subject token is not attended to, the corresponding subject will not be manifested in the output image. Thus, intuitively, in order for a subject to be present in the gen- erated image, the model should assign at least one image patch to the subject\u2019s token. Attend-and-Excite embodies this intuition by demanding that each subject token is dominant in some patch in the image. We carefully guide the latent at each denoising timestep and encourage the model to attend to all subject tokens and strengthen \u2014 or excite \u2014 their activations. Importantly, our approach is applied on the fly during inference time and requires no additional training or fine-tuning. We instead choose to preserve the strong semantics  already learned by the pre-trained generative model and text en- coder. Example generations with our approach applied over Stable Diffusion are shown in the bottom row of Figure 1. As shall be demonstrated, although Attend-and-Excite explicitly tackles only the issue of catastrophic neglect, our solution implicitly encourages correct bindings between attributes and their subjects. This can be attributed to the connection between the two issues of catastrophic neglect and attribute binding. The embedding of the text, obtained by a pre-trained text encoder, links information between each subject and its corresponding attributes. For example, in the prompt \u201ca yellow bowl and a blue cat\u201d, the token \u201ccat\u201d receives information from the token \u201cblue\u201d during the text encoding process. Therefore, mitigating catastrophic neglect over the cat should ideally result in enhancing the color attribute (that is allowing for correct binding between \u201ccat\u201d and \u201cblue\u201d). We demonstrate Attend-and-Excite\u2019s superiority in generating semantically-faithful images over Stable Diffusion and alternative methods that explore similar semantic issues. We additionally an- alyze the cross-attention maps realized with and without Attend- and-Excite and demonstrate the importance of applying our method to mitigate catastrophic neglect, while enabling the use of cross- attention as a form of explanation for the generated content.  2 RELATED WORK  Early works studied text-guided image synthesis in the context of GANs [Tao et al 2022; Xu et al 2018; Ye et al 2021; Zhang et al 2021; Zhu et al 2019]. More recently, impressive results were achieved with large-scale auto-regressive models [Ramesh et al 2021; Yu et al 2022] and diffusion models [Nichol et al 2021; Ramesh et al 2022; Rombach et al 2022; Saharia et al 2022]. Yet, generating im- ages that faithfully align with the input prompt is often difficult. To enforce heavier reliance on the text, classifier-free guidance [Ho and Salimans 2022; Nichol et al 2021; Saharia et al 2022] allows extrapolating text-driven gradients to better guide the generation by strengthening the reliance on the text. However, even when employ- ing this technique, extensive prompt engineering is often required to achieve the expected result [Liu and Chilton 2022; Marcus et al 2022; Wang et al 2022; Witteveen and Andrews 2022]. To provide users with more control over the synthesis process, several works employ a segmentation map or spatial condition- ing [Avrahami et al 2022b; Gafni et al 2022; Zhao et al 2019]. In the context of image editing, while most methods are generally limited to global edits [Chefer et al 2022a; Crowson et al 2022; Gal et al 2022b; Kwon and Ye 2022], several works introduce a user-provided mask to specify the region that should be altered [Avrahami et al 2022a; Bau et al 2021; Couairon et al 2022; Nichol et al 2021]. Another related line of work aims to introduce specific concepts to a pre-trained text-to-image model by learning to map a set of images to a \u201cword\u201d in the embedding space of the model [Gal et al 2022a; Kumari et al 2022; Ruiz et al 2022]. Several works have also explored providing users with more control over the synthesis process solely through the use of the input text prompt [Brooks et al 2022; Hertz et al 2022; Kawar et al 2022; Valevski et al 2022]. Recently, two works have explored the semantic flaws of text-to- image models. First, Liu et al [2022] propose Composable Diffusion  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 2
    },
    {
      "text": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models \u2022 148:3  models where an image is generated by composing multiple out- puts of a pre-trained diffusion model. Each output is tasked with capturing different image components which are then joined using compositional operators to attain a unified image. Yet, we observe that this method often struggles in achieving realistic compositions of multiple objects (see Section 5). Moreover, the approach is limited to operating over conjunctions and negations of subjects. Feng et al [2022] propose StructureDiffusion which employs con- sistency trees or scene graphs to split the prompt into several noun phrases. An attention map is computed for each noun phrase and the output of the cross-attention unit is the average of all attention operations. In contrast, our Attend-and-Excite technique directly optimizes the noised latent, allowing us to synthesize images that vary significantly from those produced by Stable Diffusion. We find that results obtained by StructureDiffusion often resemble those produced by Stable Diffusion, falling short of achieving meaningful modifications that amend the semantic faults (see Section 5). It should be noted that there are additional semantic issues in text-based image synthesis, for example object relations and compositions. Addressing such issues may require additional models to determine the object relations [Ashual and Wolf 2019; Johnson et al 2018]. However, this deviates from the scope of this work where we focus on inference-time guidance of a pre-trained generative model.  3 PRELIMINARIES  Latent Diffusion Models. We apply our method over the state-of- the-art Stable Diffusion model (SD) [Rombach et al 2022]. Instead of operating in the image space, SD operates in the latent space of an autoencoder. First, an encoder E is trained to map a given image \ud835\udc65\u2208X into a spatial latent code \ud835\udc67= E(\ud835\udc65). A decoder D is then tasked with reconstructing the input image such that D(E(\ud835\udc65)) \u2248\ud835\udc65. Given the trained autoencoder, a denoising diffusion probabilistic model (DDPM) [Ho et al 2020; Sohl-Dickstein et al 2015] operates over the learned latent space to produce a denoised version of an input latent \ud835\udc67\ud835\udc61at each timestep \ud835\udc61. During the denoising process, the diffusion model can be conditioned on an additional input vector. In Stable Diffusion, this additional input is typically a text encoding produced by a pre-trained CLIP text encoder [Radford et al 2021]. Given a conditioning prompt \ud835\udc66, we denote the conditioning vector by \ud835\udc50(\ud835\udc66). The DDPM model \ud835\udf00\ud835\udf03is trained to minimize the loss,  L = E\ud835\udc67\u223cE(\ud835\udc65),\ud835\udc66,\ud835\udf00\u223cN(0,1),\ud835\udc61 \u0002 ||\ud835\udf00\u2212\ud835\udf00\ud835\udf03(\ud835\udc67\ud835\udc61,\ud835\udc61,\ud835\udc50(\ud835\udc66))||2 2 \u0003 . (1)  In words, at each timestep \ud835\udc61, the denoising network \ud835\udf00\ud835\udf03is tasked with correctly removing the noise \ud835\udf00added to the latent code \ud835\udc67, given the noised latent \ud835\udc67\ud835\udc61, timestep \ud835\udc61, and conditioning encoding \ud835\udc50(\ud835\udc66). Here, \ud835\udf00\ud835\udf03is a UNet network [Ronneberger et al 2015] consisting of self-attention and cross-attention layers, discussed below. At inference, a latent \ud835\udc67\ud835\udc47is sampled from N (0, 1) and is iteratively denoised to produce a latent\ud835\udc670 using the DDPM. The denoised latent is then passed to the decoder to obtain the image \ud835\udc65\u2032 = D(\ud835\udc670).  Text-Conditioning Via Cross-Attention. Text guidance in Stable Diffusion is performed using the cross-attention mechanism. The denoising UNet network consists of self-attention layers followed by cross-attention layers at resolutions of 64, 32, 16, and 8.  DDPM Process  \u201dA lion with   a crown\u201d  UNet UNet  . .  .  Zt Zt-1  With   Attend-and-Excite  Cross Attention  Q  N  2 P  K t A  =QxK  2 P  N (timestep t)  reshape  Loss Computation  (tokens \u201clion\u201d ,\u201ccrown\u201d)  A5  t A1  t A2  t A3  t A4  t  Final Cross- Attention Maps  (timestep t=0)  (PxP)  max  \u2013 = 1  2 L G(A2) t  L Z  \u237a\u2207 - t  = Z t  : Z Update  t  ) 5 ,L 2 Loss: L= max(L  SD Generated   Image  A         lion with       a        crown  Gaussian  Smoothing  max   patch  A2  t A5  t G(A2   t) G(A5  t)  max  \u2013 = 1  5 L G(A5) t  A5  t A1  t A2  t A3  t A4  t  A        lion with        a       crown  . .  .  With   Attend-and-Excite  \u2019  Fig. 3. Overview of Attend-and-Excite. Given a prompt (for example \u201cA lion with a crown\u201d), we extract the subject tokens (lion, crown), and their corresponding attention maps (\ud835\udc342 \ud835\udc61,\ud835\udc345 \ud835\udc61). We apply a Gaussian kernel on each attention map to obtain smoothed attention maps that consider the neighboring patches. Our optimization enhances the maximal activation for the most neglected token at timestep \ud835\udc61and updates the latent code \ud835\udc67\ud835\udc61accordingly. The final cross-attention maps at \ud835\udc61= 0 are illustrated in the final row.  Denote by \ud835\udc43the spatial dimension of the intermediate feature map (that is \ud835\udc43\u2208{64, 32, 16, 8}), and by \ud835\udc41the number of text tokens in the prompt. An attention map \ud835\udc34\ud835\udc61\u2208R\ud835\udc43\u00d7\ud835\udc43\u00d7\ud835\udc41is calculated over linear projections of the intermediate features (\ud835\udc44) and text embed- ding (\ud835\udc3e), as illustrated in the second row of Figure 3. \ud835\udc34\ud835\udc61defines a distribution over the text tokens for each spatial patch (\ud835\udc56, \ud835\udc57). Specif- ically, \ud835\udc34\ud835\udc61[\ud835\udc56, \ud835\udc57,\ud835\udc5b] denotes the probability assigned to token \ud835\udc5bfor the (\ud835\udc56, \ud835\udc57)-th spatial patch of the intermediate feature map. Intuitively, this probability indicates the amount of information that will be passed from token \ud835\udc5bto patch (\ud835\udc56, \ud835\udc57). Note that the maximum value of each of the \ud835\udc43\u00d7 \ud835\udc43cells is 1. We operate over the 16 \u00d7 16 attention units since they have been shown to contain the most semantic information [Hertz et al 2022].  4 ATTEND-AND-EXCITE  At the core of our method is the idea of generative semantic nursing, where we gradually shift the noised latent code at each timestep \ud835\udc61 toward a more semantically-faithful generation. At each denoising step \ud835\udc61, we consider the attention maps of the subject tokens in the prompt P. Intuitively, for a subject to be present in the synthesized image, it should have a high influence on some patch in the image. As such, we define a loss objective that attempts to maximize the  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 3
    },
    {
      "text": "148:4 \u2022 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or  Algorithm 1 A Single Denoising Step using Attend-and-Excite  Input: A text prompt P, a set of subject token indices S, a timestep \ud835\udc61, a set of iterations for refinement {\ud835\udc611, . . . ,\ud835\udc61\ud835\udc58}, a set of thresholds {\ud835\udc471, . . . ,\ud835\udc47\ud835\udc58}, and a trained Stable Diffusion model \ud835\udc46\ud835\udc37. Output: A noised latent \ud835\udc67\ud835\udc61\u22121 for the next timestep  1: _,\ud835\udc34\ud835\udc61\u2190\ud835\udc46\ud835\udc37(\ud835\udc67\ud835\udc61, P,\ud835\udc61)  2: \ud835\udc34\ud835\udc61\u2190Softmax(\ud835\udc34\ud835\udc61\u2212\u27e8\ud835\udc60\ud835\udc5c\ud835\udc61\u27e9)  3: for \ud835\udc60\u2208S do  4: \ud835\udc34\ud835\udc60 \ud835\udc61\u2190\ud835\udc34\ud835\udc61[:, :,\ud835\udc60]  5: \ud835\udc34\ud835\udc60 \ud835\udc61\u2190Gaussian(\ud835\udc34\ud835\udc60 \ud835\udc61)  6: L\ud835\udc60\u21901 \u2212max(\ud835\udc34\ud835\udc60 \ud835\udc61)  7: end for  8: L \u2190max\ud835\udc60(L\ud835\udc60)  9: \ud835\udc67\u2032 \ud835\udc61\u2190\ud835\udc67\ud835\udc61\u2212\ud835\udefc\ud835\udc61\u00b7 \u2207\ud835\udc67\ud835\udc61L  10: if \ud835\udc61\u2208{\ud835\udc611, . . . ,\ud835\udc61\ud835\udc58} then \u22b2If performing iterative refinement at \ud835\udc61  11: if L > 1 \u2212\ud835\udc47\ud835\udc61then  12: \ud835\udc67\ud835\udc61\u2190\ud835\udc67\u2032 \ud835\udc61  13: Go to Step 1  14: end if  15: end if 16: \ud835\udc67\ud835\udc61\u22121, _ \u2190\ud835\udc46\ud835\udc37(\ud835\udc67\u2032 \ud835\udc61, P,\ud835\udc61)  17: Return \ud835\udc67\ud835\udc61\u22121  attention values for each subject token. We then update the noised latent at time \ud835\udc61according to the gradient of the computed loss. This encourages the latent at the next timestep to better incorporate all subject tokens in its representation. This manipulation occurs on the fly during inference (that is no additional training is performed). In the next sections, we discuss each of the steps presented in Al- gorithm 1 for a single denoising timestep \ud835\udc61as illustrated in Figure 3.  Extracting the Cross-Attention Maps. Given the input text prompt P, we consider the set of all subject tokens (for example nouns) S = {\ud835\udc601, ..,\ud835\udc60\ud835\udc58} present in P. Our objective is to extract a spatial attention map for each token \ud835\udc60\u2208S, indicating the influence of the token \ud835\udc60on each image patch. Given the noised latent \ud835\udc67\ud835\udc61at the current timestep, we perform a forward pass through the pre-trained UNet network using \ud835\udc67\ud835\udc61and P (Step 1 in Algorithm 1). We then consider the resulting cross- attention map obtained after averaging all 16 \u00d7 16 attention layers and heads. The resulting aggregated map \ud835\udc34\ud835\udc61contains \ud835\udc41spatial attention maps, one for each of the tokens of P, that is \ud835\udc34\ud835\udc61\u2208R16\u00d716\u00d7\ud835\udc41. The pre-trained CLIP text encoder prepends a specialized token \u27e8\ud835\udc60\ud835\udc5c\ud835\udc61\u27e9to P indicating the start of the text. We note that Stable Diffu- sion learns to consistently assign a high attention value to the \u27e8\ud835\udc60\ud835\udc5c\ud835\udc61\u27e9 token in the token distribution defined in \ud835\udc34\ud835\udc61. Since we are interested in enhancing the actual prompt tokens, we re-weigh the attention values by ignoring the attention of \u27e8\ud835\udc60\ud835\udc5c\ud835\udc61\u27e9and performing a Softmax operation on the remaining tokens (Step 2 in Algorithm 1). After the Softmax operation, the (\ud835\udc56, \ud835\udc57)-th entry of the resulting matrix \ud835\udc34\ud835\udc61 indicates the probability of each of the textual tokens being present in the corresponding image patch. We then extract the 16 \u00d7 16 normalized attention map for each subject token \ud835\udc60(Step 4).  Obtaining Smooth Attention Maps. Observe that the attention values \ud835\udc34\ud835\udc60 \ud835\udc61calculated above may not fully reflect whether an object  Cat Frog Cat Frog  Stable Diffusion (SD)  \u201cA cat and a frog\u201d  Stable Diffusion w/ Attend-and-Excite  Fig. 4. Visualization of the cross-attention maps for each subject token with and without Attend-and-Excite over Stable Diffusion.  is generated in the resulting image. Specifically, a single patch with a high attention value could stem from partial information being passed from the token \ud835\udc60. This may occur when the model does not generate the full subject, but rather a patch that resembles some part of the subject, for example a silhouette that resembles an animal\u2019s body part. See the supplementary materials for such failure cases. To avoid such adversarial solutions, we apply a Gaussian filter over \ud835\udc34\ud835\udc60 \ud835\udc61in Step 5 of Algorithm 1. After doing so, the attention value of the maximally-activated patch is dependent on its neighboring patches since, after this operation, each patch is a linear combination of its neighboring patches in the original map.  Performing On the Fly Optimization. Intuitively, successfully gen- erated subjects should have an image patch that significantly attends to their corresponding token. Our optimization objective embodies this intuition directly. For each subject token in \ud835\udc46, our optimization encourages the existence of at least one patch of \ud835\udc34\ud835\udc60 \ud835\udc61with a high activation value. Therefore, we define the loss quantifying this desired behavior as  L = max \ud835\udc60\u2208\ud835\udc46L\ud835\udc60 where L\ud835\udc60= 1 \u2212max(\ud835\udc34\ud835\udc60 \ud835\udc61). (2)  That is, the loss attempts to strengthen the activations of the most neglected subject token at the current timestep \ud835\udc61. It should be noted that different timesteps may strengthen different tokens, encourag- ing all neglected subject tokens to be strengthened at some timestep. Having computed our loss L, we shift the current latent \ud835\udc67\ud835\udc61by  \ud835\udc67\u2032 \ud835\udc61\u2190\ud835\udc67\ud835\udc61\u2212\ud835\udefc\ud835\udc61\u00b7 \u2207\ud835\udc67\ud835\udc61L, (3)  where \ud835\udefc\ud835\udc61is a scalar defining the step size of the gradient update. Finally, we perform another forward pass through \ud835\udc46\ud835\udc37using \ud835\udc67\u2032 \ud835\udc61to calculate \ud835\udc67\ud835\udc61\u22121 for the next denoising step (Step 16 of Algorithm 1). The above update process is repeated for a subset of the timesteps\ud835\udc61= \ud835\udc47,\ud835\udc47\u22121, . . . ,\ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51where we set\ud835\udc47= 50, following Stable Diffusion, and \ud835\udc61\ud835\udc52\ud835\udc5b\ud835\udc51= 25. This is based on the observation that the final timesteps do not alter the spatial locations of objects in the generated image.  Iterative Latent Refinement. So far, we have made a single latent update at each denoising timestep. However, if the attention values of a token do not reach a certain value in the early denoising stages, the corresponding object will not be generated. Therefore, we iter- atively update \ud835\udc67\ud835\udc61until a pre-defined minimum attention value is achieved for all subject tokens. Yet, many updates of \ud835\udc67\ud835\udc61may lead to the latent becoming out-of-distribution, resulting in incoherent  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 4
    },
    {
      "text": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models \u2022 148:5  \u201cA cat and a dog\u201d \u201cA turtle and a yellow bowl\u201d \u201cA frog and a pink bench\u201d \u201cA red bench and a yellow clock\u201d  Stable Diffusion Composable Diffusion StructureDiffusion Stable Diffusion with Attend-and-Excite  Fig. 5. Qualitative Comparison using prompts from our dataset. For each prompt, we show four images generated by each of the four considered methods where we use the same set of seeds across all approaches. The subject tokens optimized by Attend-and-Excite are highlighted in blue.  images. As such, this refinement is performed gradually across a small subset of timesteps. Specifically, we demand that each subject token reaches a maxi- mum attention value of at least 0[dot]8. To do so gradually, we perform the iterative updates at various denoising steps (Steps 10-15 in Al- gorithm 1). We set the iterations to \ud835\udc611 = 0,\ud835\udc612 = 10, and \ud835\udc613 = 20 with minimum required attention values of \ud835\udc471 = 0[dot]05,\ud835\udc472 = 0[dot]5, and \ud835\udc473 = 0[dot]8. This gradual refinement prevents \ud835\udc67\ud835\udc61from becoming out-of-distribution while encouraging more faithful generations.  Obtaining Explainable Image Generators. The extent to which at- tention can be used as an explanation has been widely explored [Ab- nar and Zuidema 2020; Chefer et al 2021, 2022b]. In the context of  text-based image generation, the cross-attention maps have been considered a natural explanation for the model [Hertz et al 2022]. However, a direct result of catastrophic neglect is that the atten- tion map corresponding to the neglected subject no longer faithfully represents the subject\u2019s localization in the generated image, as can be seen in the left column of Figure 4. While the cross-attention map for the cat is correctly localized, the map corresponding to the frog highlights irrelevant regions since a frog is not present. Thus, the cross-attention maps do not constitute viable explanations, as they are misleading and inaccurate. Conversely, as can be seen on the right of Figure 4, by mitigating neglect using Attend-and-Excite,  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 5
    },
    {
      "text": "148:6 \u2022 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or  \u201cA grizzly bear catching a salmon in a crystal clear  river surrounded by a forest\u201d  \u201cA pod of dolphins leaping out of the water in an ocean with a ship on the background\u201d \u201cA Picasso painting in a garden\u201d \u201cA cat and a dog  reading in the library\u201d  Stable Diffusion Stable Diffusion with Attend-and-Excite  Fig. 6. Additional comparisons with Stable Diffusion using prompts describing complex scenes and multiple subject tokens. For each prompt, we show four generated images where we use the same set of seeds for both approaches. The subject tokens optimized by Attend-and-Excite are highlighted in blue.  both the cat and the frog are accurately localized in the attention maps, and the maps can now be considered a faithful explanation.  5 RESULTS  Evaluation Setup. As there are currently no openly-available datasets that analyze semantic issues in text-based image generation, we construct a new benchmark to evaluate all methods. To analyze the existence of catastrophic neglect, we construct prompts contain- ing two subjects. Additionally, to test correct attribute binding, the prompts should contain a variety of attributes matched to the sub- ject tokens. Specifically, we consider three types of text prompts: (i) \u201ca [animalA] and a [animalB]\u201d, (ii) \u201ca [animal] and a [color][object]\u201d, and (iii) \u201ca [colorA][objectA] and a [colorB ][objectB]\u201d. To compose the prompts, we consider 12 animals and 12 object items with 11 colors, detailed in the supplementary materials. For each prompt containing a subject-color pair, we randomly select a color for the subject. This results in 66 Animal-Animal and Object-Object pairs and 144 Animal-Object pairs. For each prompt, we then generate 64 images using 64 random seeds applied across all methods. For ease of evaluation, our prompts are constructed of conjunc- tions and color attributes. Yet, our method is not limited only to such cases and can be applied to a range of prompts with any number and type of subjects and attributes (see Figures 6 and 7 and the supplementary materials).  5[dot]1 Qualitative Comparisons  In Figure 5, we present results using prompts from our dataset. As can be seen, Composable Diffusion [Liu et al 2022] tends to generate images containing a mixture of the subjects. For example, for \u201cA cat  StructureDiffusion Attend-and-Excite  \u201cA cat is perched upon a stone bench that sits on a wooden patio\u201d  \u201cA red sphere and a blue cube\u201d  \u201cRice with red sauce with eggs over the top and orange slices on the side\u201d  \u201cTwo elephants walking by a green wall with tan palm trees painted on it\u201d  Fig. 7. Comparison with prompts appearing in Feng et al [2022]. For each prompt, we apply the same set of random seeds across the two methods.  and a dog\u201d, the images tend to mix the cat\u2019s body with the dog\u2019s face and vice versa. This can similarly be seen in the prompt \u201cA frog and a pink bench\u201d where the images may contain a frog in the shape of a bench. For StructureDiffusion [Feng et al 2022], the generated images tend to be very similar to those of Stable Diffusion, indicating that the approach fails to adequately address the semantic issues since it heavily relies on the inaccurate semantics captured by Stable Diffusion. Further, in the second and last column, the alternative methods either fail to generate all subjects or fail to correctly bind colors to each subject (for example a blue bowl instead of a yellow bowl and  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 6
    },
    {
      "text": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models \u2022 148:7  \u0003\u0010\u0019\u0011\u0015\u001a\u0003\b  \u0010\u0014\u0017\u0011\u0013\u0013\b  \u0003\u0010\u001a\u0011\u001a\u0018\u0003\b  \u0010\u0014\u0018\u0011\u0015\u001c\b  \u0010\u0014\u0017\u0011\u0018\u001b\b  \u0003\u0010\u0019\u0011\u0019\u0013\u0003\b  )XOO\u00033URPSW\u00036LPLODULW\\ \u0003\u0003\u00030LQ\u0011\u00032EMHFW\u00036LPLODULW\\  \u0013\u0011\u0014\u001b  \u0013\u0011\u0015  \u0013\u0011\u0015\u0015  \u0013\u0011\u0015\u0017  \u0013\u0011\u0015\u0019  \u0013\u0011\u0015\u001b  \u0013\u0011\u0016  \u0013\u0011\u0016\u0015  \u0013\u0011\u0016\u0017  $QLPDO\u0010$QLPDO  $YHUDJH\u0003&/,3\u00036LPLODULW\\  8SSHU\u0003%RXQG  \u0010\u0016\u0011\u0019\u0018\b  \u0010\u001a\u0011\u0015\u0018\b  \u0010\u0017\u0011\u001a\u001b\b  \u0010\u001b\u0011\u001b\u0013\b  \u0010\u0017\u0011\u001c\u0013\b  \u0010\u0017\u0011\u001b\u001c\b  )XOO\u00033URPSW\u00036LPLODULW\\ \u0003\u0003\u00030LQ\u0011\u00032EMHFW\u00036LPLODULW\\  \u0013\u0011\u0015\u0015  \u0013\u0011\u0015\u0017  \u0013\u0011\u0015\u0019  \u0013\u0011\u0015\u001b  \u0013\u0011\u0016  \u0013\u0011\u0016\u0015  \u0013\u0011\u0016\u0017  \u0013\u0011\u0016\u0019  $QLPDO\u00102EMHFW  $YHUDJH\u0003&/,3\u00036LPLODULW\\  8SSHU\u0003%RXQG  \u0003\u0010\u0019\u0011\u001c\u0017\b  \u0010\u0014\u0016\u0011\u0013\u0013\b  \u0003\u0010\u001a\u0011\u001c\u0014\b  \u0010\u0014\u0016\u0011\u0017\u0019\b  \u0003\u0010\u0016\u0011\u0013\u001c\b  \u0003\u0010\u0014\u0011\u001a\u0018\b  )XOO\u00033URPSW\u00036LPLODULW\\ \u0003\u0003\u00030LQ\u0011\u00032EMHFW\u00036LPLODULW\\  \u0013\u0011\u0015\u0015  \u0013\u0011\u0015\u0017  \u0013\u0011\u0015\u0019  \u0013\u0011\u0015\u001b  \u0013\u0011\u0016  \u0013\u0011\u0016\u0015  \u0013\u0011\u0016\u0017  \u0013\u0011\u0016\u0019  \u0013\u0011\u0016\u001b 2EMHFW\u00102EMHFW  $YHUDJH\u0003&/,3\u00036LPLODULW\\  8SSHU\u0003%RXQG  -3[dot]65%  -7[dot]25%  -4[dot]78%  -8[dot]80%  -4[dot]90%  -4[dot]89%  Full Prompt Similarity    Min. Object Similarity  0[dot]18  0[dot]2  0[dot]22  0[dot]24  0[dot]26  0[dot]28  0[dot]3  0[dot]32  0[dot]34  Attend-and-Excite StableDiffusion StructureDiffusion ComposableDiffusion  Animal-Object  Average CLIP Similarity  Upper Bound  Fig. 8. Average CLIP image-text similarities between the text prompts and the images generated by each method, split by subset. The Full Prompt Similarity indicates the image-text similarity when considering the full text prompt while Minimum Object Similarity represents the average CLIP similarity for the most neglected subject. Note, the Upper Bound (the maximal-expected similarity) is applicable only to the Minimum Object Similarity.  a red clock instead of a yellow clock). In contrast, Attend-and-Excite is able to synthesize images that more faithfully contain all subjects with correctly binded colors. Although we explicitly tackle only the issue of neglect, we are able to implicitly improve attribute bindings between colors and subjects (for example the red bench and yellow clock). Additionally, we provide examples of complex prompts in Figure 6 and in the supplementary materials, including prompts with three or more subjects, complex attributes, and interactions between subjects. As can be seen, Attend-and-Excite is able to mitigate neglect while generating images that correspond to the input prompt, and the interactions between the subjects. For example, for the prompt \u201cA grizzly bear catching a salmon in a crystal clear river surrounded by a forest\u201d Attend-and-Excite mitigates the neglect over the salmon while generating images in which the bear catches the salmon, as specified in the prompt. Finally, Attend-and-Excite can also be used to correct global properties such as a background subject as shown with the \u201cgarden\u201d in the third column. In Figure 7 we consider prompts from the StructureDiffusion paper with more than two subjects or complex attributes (for example \u201cstone bench\u201d, \u201cwooden patio\u201d). As can be observed, StructureDiffusion fails to mitigate both semantic issues. For example, in the second row, StructureDiffusion generates a sphere or cube-like object but fails to generate both. In the third row, it fails to correctly bind attributes such as the red sauce to the rice. Conversely, Attend-and-Excite generates semantically accurate images in both cases. In the supplementary materials, we provide additional qualitative and quantitative results on complex prompts, as well as an ablation study and additional comparisons to image editing techniques.  5[dot]2 Quantitative Analysis  We quantify the performance of each method using CLIP-space distances along two fronts. First, we evaluate image-text similarities between the generated images and each text prompt. Second, several works [Liang et al 2022; Sheynin et al 2022] have analyzed the exis- tence of a modality gap between CLIP\u2019s image and text embeddings. To overcome this gap, we consider an additional text-only metric.  Text-Image Similarities. For each prompt, we compute the average CLIP cosine similarity between the text prompt and the correspond- ing set of 64 generated images. We denote this as the Full Prompt Similarity. Yet, considering the full text may not accurately reflect  Table 1. Average CLIP text-text similarities between the text prompts and captions generated by BLIP over the generated images.  Method Animal-Animal Animal-Object Object-Object  Stable Diffusion 0[dot]767 (-5[dot]08%) 0[dot]793 (-4[dot]74%) 0[dot]765 (-5[dot]89%) Composable Diffusion 0[dot]692 (-16[dot]47%) 0[dot]769 (-7[dot]94%) 0[dot]759 (-6[dot]85%) StructureDiffusion 0[dot]761 (-5[dot]91%) 0[dot]781 (-6[dot]31%) 0[dot]762 (-6[dot]49%) Attend-and-Excite 0[dot]806 0[dot]830 0[dot]811  the existence of neglect. It has been observed [Paiss et al 2022] that CLIP\u2019s similarities resemble a bag-of-words behavior where a high score can be achieved even if the image does not fully correspond to the semantic meaning of the prompt. For example, an image of a cat may obtain a high similarity to \u201ca cat and a dog\u201d even though a dog is not present. In such cases, considering only the full-text similarity will not capture the existence of neglect. As such, we evaluate the CLIP similarity for the most neglected subject independently of the full text. To this end, we split the prompt into two sub-prompts, each containing a single subject (for example \u201ca cat\u201d, \u201ca dog\u201d). We then compute the CLIP similarity between each sub-prompt and each generated image. Given the two scores for each image, we are interested in maximizing the smaller of the two as this would correspond to minimizing neglect. We average the smaller of the two scores across all seeds and prompts and denote this as the Minimum Object Similarity. To provide intuition for the scale of the best-achievable Minimum Similarity, we compute an Upper Bound. For each subject, we collect 50 images from classification and detection datasets [Banerjee 2022; Lin et al 2014] and the internet. We then compute the average CLIP similarity between the collected images and the subject prompt (for example \u201ca cat\u201d). To obtain the bound for each subset, we average the scores of all subjects in the set. Figure 8 presents the results of the CLIP text-image metrics for all three subsets (Animal-Animal, Animal-Object, Object-Object). Observe that Attend-and-Excite outperforms all baselines across all subsets and for both metrics. Additionally, we provide the relative de- crease in similarity (in percentage) compared to Attend-and-Excite. Notice that StructureDiffusion obtains scores similar to those of Stable Diffusion (albeit slightly lower). Attend-and-Excite signifi- cantly improves the Minimum Object Similarity in comparison to both by a gap of at least 7% across all test cases, indicating that our method substantially improves the issue of neglect. For some  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 7
    },
    {
      "text": "148:8 \u2022 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or  Table 2. User study conducted with 65 respondents. We randomly select 10 prompts from each subset and apply the same 4 randomly-selected seeds to all methods. Users are asked to select the set of images that best corresponds to the input prompt. Results are averaged across all prompts in the subset.  Method Animal-Animal Animal-Object Object-Object  Stable Diffusion 2[dot]32% 13[dot]92% 5[dot]71% Composable Diffusion 0% 1[dot]69% 9[dot]82% StructureDiffusion 6[dot]98% 6[dot]75% 7[dot]31% Attend-and-Excite 90[dot]70% 77[dot]64% 77[dot]16%  subsets, Composable Diffusion achieves results closest to those ob- tained by Attend-and-Excite. This can be attributed to a deficiency in the image-based metric where a high score can be achieved even when only a portion of a subject is present. As mentioned, Com- posable Diffusion often generates an object that is a mixture of the subjects in the input text. In such cases, the similarity to both sub- jects could be high, even though they are not generated separately. For example, an image featuring a car shaped like a bird may obtain a high similarity for both \u201ca bird\u201d and \u201ca car\u201d since the shape corre- sponds to \u201ca bird\u201d while the object itself is a car. We refer the reader to the supplementary materials for examples of such behavior. To overcome this limitation, we explore a text-based metric below.  Text-Text Similarities. Given the 64 generated images for a given input prompt, we generate matching image captions using a pre- trained BLIP image-captioning model [Li et al 2022]. We then com- pute the average CLIP similarity between the prompt and all cap- tions. This process is repeated for each subset and the results are averaged across the prompts in the subset. The choice of CLIP to compute the text-text similarity arises from the strong semantic prior of CLIP. We are less concerned with the exact phrasing and order of subjects in the captions. Instead, our focus is on capturing all subjects and attributes in the original prompt. We present the text-text similarity results in Table 1. As shown, Attend-and-Excite outperforms all alternative methods across each of our three subsets by at least 4[dot]7%. Additionally, observe that ComposableDiffusion is the lowest-performing approach when con- sidering the text-text similarity metrics, indicating that the text-text metric captures the subject-mixing behavior discussed above.  User Study. Finally, we perform a user study to analyze the fi- delity of the generated images. For each of the three evaluation subsets, we randomly sample 10 prompts and generate images with each approach using the same 4 randomly-selected seeds. For each prompt, we ask the respondents to select which set of images best reflects the prompt. The final score for each approach is calculated as the number of times respondents selected the approach averaged across all the prompts in the set (for example a score of 90% indicates that 90% of responses preferred the approach over all others). The study results are shown in Table 2. Attend-and-Excite re- ceived the highest percentage of votes across all subsets, with 90[dot]70% of responses preferring our method in the Animal-Animal subset, 77[dot]64% for the Animal-Object category, and 77[dot]16% for Object-Object. When evaluating each prompt individually, Attend-and-Excite is always preferred over the baselines by a majority of respondents. Even our lowest performing prompt received 59[dot]09% of votes (with  \u201cA dog and a bear\u201d \u201cAn elephant with a sombrero\u201d  Fig. 9. Limitations. Left: Out-of-distribution results due to the limited ex- pressive power of Stable Diffusion. Right: When the subject combination is not natural (\u201celephant\u201d, \u201csombrero\u201d), the results may be less realistic.  SD and StructureDiffusion tied for second, each receiving 16% of votes). This substantiates the effectiveness of Attend-and-Excite in alleviating semantic issues in text-based image generation.  6 LIMITATIONS  While our method offers increased fidelity with respect to the given prompt, there are several limitations to consider. First, our method is limited by the expressive power of the generative model since we do not apply additional training. In cases where the prompt resides outside the distribution of the textual descriptions the model learned, our method could lead to latents that are out of distribution, resulting in images that do not correspond to the text prompt. Second, when synthesizing subjects that naturally do not appear together, the generated images may be less realistic (for example paintings). We attribute this to the fact that such combinations tend to reside outside the distribution that Stable Diffusion has learned for real images. Examples for these limitations are shown in Figure 9. Finally, while we tackle two core semantic issues, the path to achieving semantically-accurate generation is still long, and there exist additional challenges to be addressed such as complex object compositions (for example \u201criding on\u201d, \u201cin front of\u201d, \u201cbeneath\u201d). Additionally, while we have not explored applying Attend-and-Excite over a nega- tion (that is \u201cnot\u201d), this could potentially be achieved by demanding a low attention value for the subject.  7 CONCLUSIONS  Can a diffusion process be corrected once it takes a wrong turn? In this work, we introduce the concept of Generative Semantic Nursing (GSN), which refers to a careful manipulation of latents during the denoising process of a pre-trained text-to-image diffusion model. We then present Attend-and-Excite, a specific form of GSN that en- courages all subject tokens in the text to be attended to by some image patch. We demonstrate that by applying this intuitive opti- mization, we are able to alleviate two core semantic issues on the fly, thus correcting the generator after it has taken a wrong turn. Similar to extrapolating text-driven gradients in classifier-free guidance, our approach aims to strengthen the text conditioning along the image generation process. While we explore the notion of GSN for mitigating semantic issues of text-conditioned generation, we believe GSN can potentially be applied to any image editing and generation task by defining an appropriate loss objective. More- over, this guidance need not be through text and does not require conditioning at all, but is defined only by the task itself.  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 8
    },
    {
      "text": "Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models \u2022 148:9  ACKNOWLEDGMENTS  We thank Daniel Livshen, Elad Richardson, Matan Cohen, Or Patash- nik, Rinon Gal, and Yotam Nitzan for their early feedback and in- sightful discussions. The first author is supported by the Council for Higher Education in Israel. This work was supported in part by the Israel Science Foundation under Grant No. 2366/16 and Grant No. 2492/20.  REFERENCES  Samira Abnar and Willem Zuidema. 2020. Quantifying Attention Flow in Transformers. ArXiv abs/2005[dot]00928 (2020).  Oron Ashual and Lior Wolf. 2019. Specifying object attributes and relations in interactive scene generation. In Proceedings of the IEEE/CVF international conference on computer vision. 4561\u20134569.  Omri Avrahami, Ohad Fried, and Dani Lischinski. 2022a. Blended Latent Diffusion. arXiv preprint arXiv:2206[dot]02779 (2022).  Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. 2022b. SpaText: Spatio-Textual Representa- tion for Controllable Image Generation. arXiv preprint arXiv:2211[dot]14305 (2022).  Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, and Ming-Yu Liu. 2022. eDiff-I: Text-to-Image Diffusion Models with an Ensemble of Expert Denoisers. ArXiv abs/2211[dot]01324 (2022).  Sourav Banerjee. 2022. Animal Image Dataset. https://www[dot]kaggle[dot]com/datasets/ iamsouravbanerjee/animal-image-dataset-90-different-animals.  David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian, Aude Oliva, and Antonio Torralba. 2021. Paint by word. arXiv preprint arXiv:2103[dot]10951 (2021).  Tim Brooks, Aleksander Holynski, and Alexei A Efros. 2022. InstructPix2Pix: Learning to Follow Image Editing Instructions. arXiv preprint arXiv:2211[dot]09800 (2022).  Hila Chefer, Sagie Benaim, Roni Paiss, and Lior Wolf. 2022a. Image-Based CLIP-Guided Essence Transfer. (2022).  Hila Chefer, Shir Gur, and Lior Wolf. 2021. Generic Attention-Model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 397\u2013406.  Hila Chefer, Idan Schwartz, and Lior Wolf. 2022b. Optimizing Relevance Maps of Vision Transformers Improves Robustness. In Thirty-Sixth Conference on Neural Information Processing Systems. https://openreview[dot]net/forum?id=upuYKQiyxa_  Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and Matthieu Cord. 2022. DiffEdit: Diffusion-based semantic image editing with mask guidance.  Katherine Crowson, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. 2022. Vqgan-clip: Open domain image generation and editing with natural language guidance. In European Conference on Computer Vision. Springer, 88\u2013105.  Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. 2022. Training-Free Structured Diffusion Guidance for Compositional Text-to-Image Synthesis. arXiv preprint arXiv:2212[dot]05032 (2022).  Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taig- man. 2022. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203[dot]13131 (2022).  Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022a. An image is worth one word: Personalizing text-to- image generation using textual inversion. arXiv preprint arXiv:2208[dot]01618 (2022).  Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022b. Stylegan-nada: Clip-guided domain adaptation of image genera- tors. ACM Transactions on Graphics (TOG) 41, 4 (2022), 1\u201313.  Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208[dot]01626 (2022).  Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.  Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv preprint arXiv:2207[dot]12598 (2022).  Justin Johnson, Agrim Gupta, and Li Fei-Fei. 2018. Image generation from scene graphs. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1219\u20131228.  Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. 2022. Imagic: Text-based real image editing with diffusion models. arXiv preprint arXiv:2210[dot]09276 (2022).  Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. 2022. Multi-Concept Customization of Text-to-Image Diffusion. arXiv preprint arXiv:2212[dot]04488 (2022).  Gihyun Kwon and Jong Chul Ye. 2022. Clipstyler: Image style transfer with a single text condition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18062\u201318071.  Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and genera- tion. arXiv preprint arXiv:2201[dot]12086 (2022).  Weixin Liang, Yuhui Zhang, Yongchan Kwon, Serena Yeung, and James Zou. 2022. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. arXiv preprint arXiv:2203[dot]02053 (2022).  Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In European conference on computer vision. Springer, 740\u2013755.  Nan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. 2022. Compositional Visual Generation with Composable Diffusion Models. arXiv preprint arXiv:2206[dot]01714 (2022).  Vivian Liu and Lydia B Chilton. 2022. Design Guidelines for Prompt Engineering Text- to-Image Generative Models. In CHI Conference on Human Factors in Computing Systems. 1\u201323.  Gary Marcus, Ernest Davis, and Scott Aaronson. 2022. A very preliminary analysis of DALL-E 2. arXiv preprint arXiv:2204[dot]13807 (2022).  Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112[dot]10741 (2021).  Roni Paiss, Hila Chefer, and Lior Wolf. 2022. No token left behind: Explainability-aided image classification and generation. In European Conference on Computer Vision. Springer, 334\u2013350.  Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al 2021. Learning transferable visual models from natural language supervision. In Interna- tional Conference on Machine Learning. PMLR, 8748\u20138763.  Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical Text-Conditional Image Generation with CLIP Latents. ArXiv (2022). https://arxiv[dot]org/abs/2204[dot]06125  Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Rad- ford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning. PMLR, 8821\u20138831.  Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10684\u201310695.  Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention. Springer, 234\u2013241.  Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models for subject- driven generation. arXiv preprint arXiv:2208[dot]12242 (2022).  Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L. Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Seyedeh Sara Mahdavi, Raphael Gontijo Lopes, Tim Salimans, Jonathan Ho, David J. Fleet, and Mohammad Norouzi. 2022. Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding. ArXiv abs/2205[dot]11487 (2022).  Shelly Sheynin, Oron Ashual, Adam Polyak, Uriel Singer, Oran Gafni, Eliya Nach- mani, and Yaniv Taigman. 2022. KNN-Diffusion: Image Generation via Large-Scale Retrieval. ArXiv abs/2204[dot]02849 (2022).  Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning. PMLR, 2256\u20132265.  Ming Tao, Hao Tang, Fei Wu, Xiao-Yuan Jing, Bing-Kun Bao, and Changsheng Xu. 2022. DF-GAN: A Simple and Effective Baseline for Text-to-Image Synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 16515\u201316525.  Dani Valevski, Matan Kalman, Yossi Matias, and Yaniv Leviathan. 2022. Unitune: Text- driven image editing by fine tuning an image generation model on a single image. arXiv preprint arXiv:2210[dot]09477 (2022).  Zijie J Wang, Evan Montoya, David Munechika, Haoyang Yang, Benjamin Hoover, and Duen Horng Chau. 2022. DiffusionDB: A Large-scale Prompt Gallery Dataset for Text-to-Image Generative Models. arXiv preprint arXiv:2210[dot]14896 (2022).  Sam Witteveen and Martin Andrews. 2022. Investigating Prompt Engineering in Diffusion Models. arXiv preprint arXiv:2211[dot]15462 (2022).  Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition. 1316\u20131324.  Hui Ye, Xiulong Yang, Martin Takac, Rajshekhar Sunderraman, and Shihao Ji. 2021. Improving text-to-image synthesis using contrastive learning. arXiv preprint arXiv:2107[dot]02423 (2021).  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 9
    },
    {
      "text": "148:10 \u2022 Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or  Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al 2022. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206[dot]10789 (2022).  Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and Yinfei Yang. 2021. Cross- modal contrastive learning for text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 833\u2013842.  Bo Zhao, Lili Meng, Weidong Yin, and Leonid Sigal. 2019. Image Generation from Layout. In CVPR.  Minfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. 2019. Dm-gan: Dynamic memory generative adversarial networks for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 5802\u20135810.  ACM Trans. Graph, Vol. 42, No. 4, Article 148. Publication date: August 2023.",
      "page_number": 10
    }
  ]
}