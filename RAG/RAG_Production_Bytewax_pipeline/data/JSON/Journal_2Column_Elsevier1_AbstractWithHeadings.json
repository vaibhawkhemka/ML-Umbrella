{
  "_id": {
    "$oid": ""
  },
  "originalName": "Journal_2Column_Elsevier1_AbstractWithHeadings.pdf",
  "path": "",
  "user": {
    "$oid": ""
  },
  "processed": "",
  "pinecone": "",
  "deleted": "",
  "fileHash": "",
  "figures_contents": "",
  "contents": [
    {
      "text": "Evaluating the Performance of ChatGPT in Ophthalmology  An Analysis of Its Successes and Shortcomings  Fares Antaki, MD, CM,1,2,3,4 Samir Touma, MD, CM,1,2,3 Daniel Milad, MD,1,2,3 Jonathan El-Khoury, MD,1,2,3  Renaud Duval, MD1,2  Purpose: Foundation models are a novel type of arti\ufb01cial intelligence algorithms, in which models are pre- trained at scale on unannotated data and \ufb01ne-tuned for a myriad of downstream tasks, such as generating text. This study assessed the accuracy of ChatGPT, a large language model (LLM), in the ophthalmology question- answering space. Design: Evaluation of diagnostic test or technology. Participants: ChatGPT is a publicly available LLM. Methods: We tested 2 versions of ChatGPT (January 9 \u201clegacy\u201d and ChatGPT Plus) on 2 popular multiple choice question banks commonly used to prepare for the high-stakes Ophthalmic Knowledge Assessment Program (OKAP) examination. We generated two 260-question simulated exams from the Basic and Clinical Science Course (BCSC) Self-Assessment Program and the OphthoQuestions online question bank. We carried out logistic regression to determine the effect of the examination section, cognitive level, and dif\ufb01culty index on answer accuracy. We also performed a post hoc analysis using Tukey\u2019s test to decide if there were meaningful differences between the tested subspecialties. Main Outcome Measures: We reported the accuracy of ChatGPT for each examination section in percentage correct by comparing ChatGPT\u2019s outputs with the answer key provided by the question banks. We presented logistic regression results with a likelihood ratio (LR) chi-square. We considered differences between examination sections statistically signi\ufb01cant at a P value of < 0[dot]05. Results: The legacy model achieved 55[dot]8% accuracy on the BCSC set and 42[dot]7% on the OphthoQuestions set. With ChatGPT Plus, accuracy increased to 59[dot]4% \u0001 0[dot]6% and 49[dot]2% \u0001 1[dot]0%, respectively. Accuracy improved with easier questions when controlling for the examination section and cognitive level. Logistic regression analysis of the legacy model showed that the examination section (LR, 27[dot]57; P \u00bc 0[dot]006) followed by question dif\ufb01culty (LR, 24[dot]05; P < 0[dot]001) were most predictive of ChatGPT\u2019s answer accuracy. Although the legacy model performed best in general medicine and worst in neuro-ophthalmology (P < 0[dot]001) and ocular pathology (P \u00bc 0[dot]029), similar post hoc \ufb01ndings were not seen with ChatGPT Plus, suggesting more consistent results across examination sections. Conclusion: ChatGPT has encouraging performance on a simulated OKAP examination. Specializing LLMs through domain-speci\ufb01c pretraining may be necessary to improve their performance in ophthalmic subspecialties. Financial Disclosure(s): Proprietary or commercial disclosure may be found in the Footnotes and Disclo- sures at the end of this article. Ophthalmology Science 2023;3:100324 \u00aa 2023 by the American Academy of Ophthalmology. This is an open access article under the CC BY-NC-ND license (http://creativecommons[dot]org/ licenses/by-nc-nd/4[dot]0/).  Supplemental material available at www[dot]ophthalmologyscience[dot]org[dot]  Since 2015, signi\ufb01cant progress has been made in the application of arti\ufb01cial intelligence (AI) and deep learning (DL) in medicine, particularly in ophthalmology.1 Deep learning has been widely used for image recognition using various types of ophthalmic data, such as fundus photographs and OCT, and has shown strong results in detecting a wide range of diseases.2,3 More recently, there has been growing interest in using DL for natural  language processing in ophthalmology, which involves using AI to understand and interact with human language.4  Natural language processing has received considerable media attention in the past months due to the release of large DL models called foundation models.5 Foundation models represent a novel paradigm for building AI systems, whereby models are pretrained at scale on vast amounts of unannotated multimodal data in a self-supervised  1 \u00aa 2023 by the American Academy of Ophthalmology This is an open access article under the CC BY-NC-ND license (http://creativecommons[dot]org/licenses/by-nc-nd/4[dot]0/)[dot] Published by Elsevier Inc.  https://doi[dot]org/10[dot]1016/j[dot]xops[dot]2023[dot]100324  ISSN 2666-9145/23",
      "page_number": 1
    },
    {
      "text": "manner. They are subsequently \ufb01ne-tuned for a myriad of downstream tasks through a process called transfer learning.6,7 The incredible scale of foundation models, which can now contain billions of parameters, has been made possible by advances in computer hardware coupled with the transformer model architecture and the availability of vast amounts of training data.6 A prominent example of such models is Generative Pretrained Transformer 3 (GPT-3), a large language model (LLM) that generates human-like text. It was trained on a massive data set of text (> 400 billion words) from the internet, including books, articles, and websites.8  There has been recent interest in evaluating the capabil- ities of LLMs for understanding and generating natural language in medicine.9,10 The medical domain can pose a signi\ufb01cant challenge for LLMs because clinical reasoning often requires years of training and hands-on experience to master. In 2022, Singhal et al9 demonstrated the capabilities of Pathways Language Model, a 540-billion parameter LLM, by testing it on multiple choice questions from the United States Medical Licensing Examination (USMLE) with an impressive 67[dot]6% accuracy. More recently, Kung et al11 evaluated the performance of ChatGPT, a generic LLM developed by OpenAI that is based on the GPT-3 series and optimized for dialogue, us- ing multiple choice questions also from the USMLE. They found that ChatGPT achieved overall accuracy of > 50% in most of their experiments and also provided insightful ex- planations to support its answer choices. To our knowledge, the performance of LLMs has not yet been examined in the ophthalmology question-answering space. In this study, we evaluated the performance of ChatGPT in ophthalmology by using 2 popular board preparation question banks: the American Academy of Ophthalmology\u2019s Basic and Clinical Science Course (BCSC) Self-Assessment Program and the OphthoQuestions online question bank. These resources have been shown to be effective in studying for board examinations and have been linked to improved performance on the standardized Ophthalmic Knowledge Assessment Program (OKAP) examination, which is taken annually by ophthalmology residents in the United States and Canada.12,13  Methods  ChatGPT Is an LLM  ChatGPT (OpenAI) is a \ufb01ne-tuned LLM based on a model from the GPT-3[dot]5 series called \u201cgpt-3[dot]5-turbo\u201d14 Generative Pretrained Transformer 3 has a transformer architecture and was trained using billions of text data obtained from writings on the internet. This process is done by training the model to minimize the difference between the predicted word and the actual word in the training data set. Once the model is trained, it can be used to generate new text by providing it with a prompt and allowing it to predict the next word. The model then uses this predicted word as the context for the next prediction, and this process is repeated until a complete sentence or paragraph is generated.8  ChatGPT goes beyond just predicting the next word because it is optimized for dialogue and was trained using human feedback.  This allows it to understand and respond to human expectations when answering questions.15  ChatGPT January 9 (Legacy Model) and ChatGPT Plus  We tested 2 versions of ChatGPT. In the initial experiment, we used the free research preview that was released on January 9, 2023, which we will subsequently refer to as the \u201clegacy model\u201d While conducting further experiments, OpenAI unveiled a newly upgraded model on January 30, 2023, that boasted \u201cenhanced factuality and mathematical capabilities\u201d Shortly thereafter, ChatGPT Plus was introduced as a subscription-based service, offering faster responses and priority access.16 Because previous models of ChatGPT were made inaccessible as new ones were released, we used ChatGPT Plus for subsequent experiments to ensure the stability of results.  Repeatability of ChatGPT Performance  We were only able to run a single experiment with the legacy model of ChatGPT before it was made unavailable by OpenAI. Using the reliable ChatGPT Plus, we conducted multiple experi- ments to measure the variability and establish the reproducibility of our results. We anticipated that the responses provided by ChatGPT would exhibit some variability across different runs because of the probabilistic nature of LLMs. We repeated the ex- periments 3 times for each of the BCSC and OphthoQuestions sets by manually composing the prompts and extracting responses from the ChatGPT website.  BCSC and OphthoQuestions  In January 2023, we generated a test set of 260 questions from the BCSC Self-Assessment Program and 260 questions from Oph- thoQuestions through personal subscription accounts. Permission was obtained from the American Academy of Ophthalmology for use of the underlying BCSC Self-Assessment program materials. Those questions are not publicly accessible, thereby excluding the possibility of prior indexing in any search engine (like Google) or in the ChatGPT training data set. For the BCSC and Oph- thoQuestions test sets, we randomly generated 260 questions out of a pool of 4458 and 4539 potential questions, respectively. During the process, any questions that included visual information, such as clinical, radiologic, or graphical images, were removed and replaced because ChatGPT does not currently support such data. We generated 20 random questions based on the 13 sections of the OKAP examination: update on general medicine, fundamentals and principles of ophthalmology, clinical optics and vision reha- bilitation, ophthalmic pathology and intraocular tumors, neuro- ophthalmology, pediatric ophthalmology and strabismus, oculofa- cial plastic and orbital surgery, external disease and cornea, uveitis and ocular in\ufb02ammation, glaucoma, lens and cataract, retina and vitreous, and refractive surgery.  Question Format and Encoding  We aimed to replicate an OKAP examination and therefore maintained the standard multiple choice format with 1 correct answer and 3 incorrect options (distractors). We employed a zero- shot approach for the lead-in prompt, using the prompt \u201cPlease select the correct answer and provide an explanation\u201d followed by the question and answer options, without providing any examples.9  Although more challenging for ChatGPT,8 we chose this technique because it is the closest to human test-taking. A new session was started in ChatGPT for each question to reduce memory retention bias.  Ophthalmology Science Volume 3, Number 4, December 2023  2",
      "page_number": 2
    },
    {
      "text": "Level of Cognition and Question Dif\ufb01culty  Because the BCSC and OphthoQuestions questions were not labeled for dif\ufb01culty, we labeled them according to the cognitive level and calculated a dif\ufb01culty index.17 We did this to analyze ChatGPT\u2019s performance based on not only the subject but also the type of question and level of dif\ufb01culty. Despite having no control over the distribution of cognitive level and question dif\ufb01culty in each of the randomly generated test sets, we elected not to balance them manually to prevent cherry-picking, thereby avoiding bias in the experiment results. We used a simpli\ufb01ed scoring system of low and high cognitive level, instead of the 3-tier system proposed in the OKAP User\u2019s Guide.18 This was done because we found it dif\ufb01cult to distinguish between level 2 and level 3 questions, and we wanted to avoid making assumptions about the intended goal of the questions. Low-cognitive-level questions tested recall of facts and concepts, such as identifying the gene implicated in a known condition. High-cognitive-level questions tested the ability to interpret data, make calculations and manage patients, like in common clinical optics exercises (for example cylinder transpositions) or to select the best treatment for speci\ufb01c cancers in unique clinical contexts (for example the optimal treatment for metastatic sebaceous cell carcinoma of the eyelid). The dif\ufb01culty index represented the percentage of in- dividuals who correctly answered a question, as reported by BCSC and OphthoQuestions platforms for each question. Questions with a higher dif\ufb01culty index are considered easier. The questions were categorized into 3 levels of dif\ufb01culty: dif\ufb01cult (< 30%), moderate (\u0003 30% and < 70%), and easy (\u0003 70%).19  Statistical Analysis  Accuracy was determined by comparing ChatGPT\u2019s answer with the answer key provided by the question banks. The legacy model\u2019s accuracy was determined from a single run, whereas the means and standard deviations of the ChatGPT Plus model were derived from data collected over 3 runs. The degree of repeatability for those runs was assessed within each examination section using a k measure for M raters.20 We used logistic regression (all the input variables were entered simultaneously) to examine the effect of the examination section, cognitive level, and dif\ufb01culty index on ChatGPT\u2019s answer accuracy. We then performed a post hoc analysis using Tukey\u2019s test to determine if there were signi\ufb01cant differences in accuracy between examination sections while controlling for question dif\ufb01culty and cognitive level. By controlling for those factors, we were able to isolate the effect of the examination section on accuracy and determine if there were any meaningful differences between the tested topics. For the ChatGPT Plus model analyses, we combined the results of the 3 runs. Therefore, to account for correlated values, we used a generalized estimating equation model with an exchangeable correlation matrix and a binomial distribution using a logit link.  Results  The Testing Sets Demonstrated Similar Dif\ufb01culty and Cognitive Levels  The BCSC and OphthoQuestions training sets had a similar level of dif\ufb01culty (P \u00bc 0[dot]154) and mostly included easy and moderate questions in a very similar distribution (P \u00bc 0[dot]102), as illustrated in Figure 1 and Table 1. Likewise, the questions\u2019 cognitive levels were comparable between the 2 test sets (P \u00bc 0[dot]425). Those similarities allowed us to combine the testing sets during further analyses.  ChatGPT Had a Modest Overall Performance Initially but It Improved After a Model Update  In the initial experiment, the legacy model achieved an accuracy of 55[dot]8% on the BCSC set and 42[dot]7% on the OphthoQuestions set. However, with the improved ChatGPT Plus, the accuracy increased to 59[dot]4% \u0001 0[dot]6% on the BCSC set and 49[dot]2% \u0001 1[dot]0% on the OphthoQuestions set. Table S2 (available at www[dot]ophthalmologyscience[dot]org) and Figure 2 show the variations in performance between the ChatGPT models and the BCSC and OphthoQuestions sets for the same examination section. Taking the data sets together, the legacy model performed well in general medicine (75%), fundamentals (60%), and cornea (60%), but not as well in neuro-ophthalmology (25%), glaucoma (37[dot]5%), and pediatrics and strabismus (42[dot]5%). The updated ChatGPT Plus model consistently excelled in its strongest subjects: fundamentals (68[dot]3% \u0001 6[dot]1%), general medicine (65[dot]8% \u0001 7[dot]4%), and cornea (65[dot]0% \u0001 3[dot]2%). However, its weakest subject remained neuro- ophthalmology (40[dot]0% \u0001 9[dot]5%) in addition to oculo- plastics (40[dot]8% \u0001 9[dot]2%) and clinical optics (45[dot]8% \u0001 10[dot]2%).  ChatGPT Plus Answers Were Consistent across Runs with Substantial to Almost Perfect Repeatability  The k values were 0[dot]769 (95% con\ufb01dence interval [CI], 0[dot]699e0[dot]839) for the BCSC set and 0[dot]798 (95% CI, 0[dot]728e0[dot]868) for the OphthoQuestions set. When these sets were combined, the resulting k value was 0[dot]786 (95% CI, 0[dot]736e0[dot]835). These \ufb01ndings indicate that the CIs fell within the range of substantial to almost perfect repeat- ability.21 The mean overall accuracies for each of the runs are shown in Table S2, and they had a maximum difference of 1[dot]9%.  ChatGPT\u2019s Accuracy Depends on the Examination Section, Cognitive Level, and Question Dif\ufb01culty  Our initial experiments on the legacy model showed that the examination section (likelihood ratio [LR], 27[dot]57; P \u00bc 0[dot]006) followed by question dif\ufb01culty (LR, 24[dot]05; P < 0[dot]001) were most predictive of answer accuracy (Tables 3, S4, available at www[dot]ophthalmologyscience[dot]org)[dot] The initial experiments also showed that while controlling for question dif\ufb01culty and cognitive level, there were signi\ufb01cant differences in the legacy model\u2019s performance between general medicine and each of glaucoma (P \u00bc 0[dot]002), neuro-ophthalmology (P < 0[dot]001), and ophthalmic pathology and intraocular tumors (P \u00bc 0[dot]029) (Fig S3, available at www[dot]ophthalmologyscience[dot]org)[dot] Similarly, we found that accuracy improved with increased dif\ufb01culty index (easier questions) even when controlling for the examination section and cognitive level. Figures S4 and S5 (available at www[dot]ophthal mologyscience.org) provide the results of the post hoc analysis for each of the testing sets.  Antaki et al \u0004 Performance of ChatGPT in Ophthalmology  3",
      "page_number": 3
    },
    {
      "text": "With the improved ChatGPT Plus model, question dif- \ufb01culty (LR, 32[dot]30; P < 0[dot]001), followed by examination section (LR, 23[dot]40; P \u00bc 0[dot]024), and cognitive level (LR, 5[dot]60; P \u00bc 0[dot]018) were all predictive of ChatGPT Plus\u2019s answer accuracy (Tables 3, S5 available at www[dot]ophthal mologyscience.org). Although there was a signi\ufb01cant global effect of examination section on ChatGPT Plus\u2019s performance, our analysis of pairwise differences, which accounted for multiple comparisons, did not reveal any statistically signi\ufb01cant differences in performance across the examination sections (Fig S6, available at  www[dot]ophthalmologyscience[dot]org)[dot] This contrasts with the results obtained from the legacy model, making it a noteworthy \ufb01nding. Similar to the legacy model, accuracy  improved with easier questions even when controlling the examination section and cognitive level. The results of the post hoc analysis for each testing set can be found in Figures S7 and S8 (available at www[dot]ophthal mologyscience.org).  Discussion  In the past months, there has been signi\ufb01cant interest in examining the utility of LLMs in medicine.5 Despite having encouraging impacts in various industries, it is important to thoroughly evaluate their performance and biases before determining their clinical usefulness.4,22 In this study, we  Figure 1. Alluvial diagram illustrating the distribution of questions across examination sections, cognitive level, and question dif\ufb01culty. Despite having been generated at random, the Basic and Clinical Science Course (BCSC) and OphthoQuestions test sets have a similar distribution of questions with high and low cognitive levels and similar dif\ufb01culty.  Table 1. Baseline Characteristics of the Testing Sets  BCSC (n [ 260) OphthoQuestions (n [ 260) P Value*  Dif\ufb01culty index Mean 0[dot]69 0[dot]719 0[dot]154 Median 0[dot]73 0[dot]750 Interquartile range 0[dot]28 0[dot]260 Range 0[dot]00e0[dot]94 0[dot]21e0[dot]96 Dif\ufb01culty category 0[dot]102 Easy 146 (56[dot]2%) 151 (58[dot]1%) Moderate 105 (40[dot]4%) 107 (41[dot]2%) Dif\ufb01cult 9 (3[dot]5%) 2 (0[dot]8%) Cognitive level 0[dot]425 High 106 (40[dot]7%) 115 (44[dot]2%) Low 154 (59[dot]3%) 145 (55[dot]8%)  BCSC \u00bc Basic and Clinical Science Course. *ManneWhitney U nonparametric test (dif\ufb01culty index) and chi-square test (dif\ufb01culty category and cognitive level)  Ophthalmology Science Volume 3, Number 4, December 2023  4",
      "page_number": 4
    },
    {
      "text": "provide evidence on the performance of ChatGPT, a nonedomain-speci\ufb01c LLM, in responding to questions similar to those found on the OKAP examination. During experimentation, we observed notable improve- ment in ChatGPT\u2019s performance as the model was updated. The most powerful model (ChatGPT Plus) achieved an ac- curacy of 59[dot]4% on the simulated OKAP examination using the BCSC testing set and 49[dot]2% on the OphthoQuestions testing set. To put the results into perspective, we  aggregated historical human performance data in Table S6 (available at www[dot]ophthalmologyscience[dot]org)[dot] The data indicate that, on average, humans score 74% on the BCSC question bank; and in the last 3 years, the group of ophthalmology residents who completed their training in 2022 obtained an average score of 63% on OphthoQuestions. Despite being slightly out of range of human performance, we believe that this outcome is noteworthy and promising within ophthalmology, as our results approach ChatGPT\u2019s performance on the USMLE despite being a much more specialized examination.11 Our \ufb01ndings are also encouraging as ChatGPT\u2019s accuracy in ophthalmology is similar to the typical accuracy seen in general medical question answering by state-of-the-art LLMs, typically w 40% to 50%, as reported in publica- tions from 2022[dot]9  We found that the accuracy of the legacy model mostly depended on the examination section, even when controlling for question dif\ufb01culty and cognitive level. This effect was less pronounced in the updated version of ChatGPT. ChatGPT consistently excelled in general medicine, funda- mentals, and cornea. The model\u2019s high performance in these areas might be attributed to the vast amount of training data and resources available on the internet for those topics. In contrast, the legacy model performed poorest in neuro- ophthalmology as well as ophthalmic pathology and intra- ocular tumors. Those are highly specialized domains that are  Figure 2. Bar plot of the accuracy of ChatGPT across examination sections and ChatGPT models for the Basic and Clinical Science Course (BCSC) and OphthoQuestions testing sets. The ChatGPT Plus model accuracy is shown with error bars representing the standard deviation from the 3 experimental runs.  Table 3. Comparing the Likelihood Ratios for Examination Sec- tion, Cognitive Level, and Dif\ufb01culty Index for the Legacy and ChatGPT Plus Models (Testing Sets Combined)  Effects LR Chisq Df Pr(>Chisq)  Model: legacy Section 27[dot]57 12 0[dot]006*  Cognitive level 3[dot]54 1 0[dot]06 Dif\ufb01culty index 24[dot]05 1 < 0[dot]001*  Model: ChatGPT Plus Section 23[dot]40 12 0[dot]024*  Cognitive level 5[dot]60 1 0[dot]018*  Dif\ufb01culty index 32[dot]30 1 < 0[dot]001*  BCSC \u00bc Basic and Clinical Science Course; LR \u00bc likelihood ratio. *Statistically signi\ufb01cant at the 0[dot]05 level  Antaki et al \u0004 Performance of ChatGPT in Ophthalmology  5",
      "page_number": 5
    },
    {
      "text": "considered challenging even within the ophthalmology community. For example, up to 40% of patients referred to a neuro-ophthalmology subspecialty service are mis- diagnosed,23 and similar referral patterns are observed in ocular oncology.24 The updated ChatGPT Plus model continued to perform poorly in neuro-ophthalmology, but its performance in pathology and intraocular tumors improved. Understanding why ChatGPT makes mistakes is impor- tant. We found that question dif\ufb01culty was predictive of ChatGPT\u2019s accuracy, even when controlling for the exam- ination section and cognitive level. ChatGPT was more accurate when a higher percentage of human peers obtained the right answer for a speci\ufb01c question. This discovery is comforting as it suggests that ChatGPT\u2019s responses align, to a certain degree, with the collective understanding of ophthalmology trainees. In parallel, Kung et al11 showed that the accuracy of ChatGPT is heavily in\ufb02uenced by concordance and insight, indicating that inaccurate responses are caused by a lack of training information for the USMLE. We plan to perform a similar qualitative analysis to identify areas for improvement in the ophthalmology space. Incorporating ChatGPT with other specialized foundation models that are trained using domain-speci\ufb01c sources (such as EyeWiki) might be required to improve its accuracy. Despite its encouraging performance, the imminent implementation of ChatGPT in ophthalmology may be limited because it does not have the capability to process images. This is a signi\ufb01cant limitation because ophthal- mology is a \ufb01eld that heavily relies on visual examination and imaging to diagnose, treat, and monitor patients. Large lan- guage model, such as ChatGPT, may need to incorporate other transformer models that can handle multiple types of data, such as the Contrastive Language-Image Pretraining model,25 which can classify images and generate a text description that ChatGPT can then use to respond to a question. Although this approach shows potential, it is limited by its reliance on a large amount of imageetext pairs from the internet (in the case of the Contrastive Language-Image Pretraining model) that are not speci\ufb01c to our domain. These data may not be suf\ufb01cient to accurately distinguish subtle and speci\ufb01c differences relevant to medi- cine and ophthalmology.26 For instance, the Contrastive Language-Image Pretraining model may not be able to accurately caption a \u201csuperior\u201d retinal detachment that would need a pneumatic retinopexy, as opposed to an \u201cinferior\u201d retinal detachment that might require a scleral buckle.  Although we could not obtain repeat experiments on the legacy model, we conducted the ChatGPT Plus experi- ments thrice to ensure the consistency of our \ufb01ndings. This process proved to be extremely labor-intensive. We believe that the availability of an application programming inter- face for ChatGPT may facilitate more thorough validation of this technology in the future and potentially alleviate the labor-intensive nature of the process. Generally, we found that ChatGPT Plus provided highly consistent and repeat- able results, but some variations occurred. We expected that because ChatGPT is a probabilistic model that works by predicting the likelihood of a particular sequence of words appearing in a language. The model calculates the probability of each possible next word given the previous words in the sequence, and the probability distribution of each next word is based on statistical patterns learned from its training data. Until recently, ChatGPT could not be made more deterministic, but the latest application pro- gramming interface release now permits such modi\ufb01ca- tions. By adjusting the \u201ctemperature\u201d setting, you can either decrease it to maintain the model\u2019s emphasis on the prompt\u2019s intention (more deterministic) or increase it to allow it to digress (more probabilistic). Determining the appropriate temperature for each case use and each clinical context may be necessary as we experiment further with those models. As the performance of ChatGPT improves (perhaps through prompting strategies and through updates by OpenAI), it will be important to work collectively toward building safeguards for our patients.4 Those will include protecting vulnerable populations from biases and evaluating the potential harm or risk of acting on the answers provided by LLMs, such as ChatGPT. This will be particularly important for high-level decision- making questions that may be challenging to train for because of inconclusive training data on the internet, re\ufb02ecting the variability in research data as well as global practice patterns. We are excited about the potential of ChatGPT in ophthalmology, but we remain cautious when considering the potential clinical applications of this technology.  Acknowledgments  The authors thank Mr. Charles-\u00c9douard Gigu\u00e8re, statistician at the Institut Universitaire en Sant\u00e9 Mentale de Montr\u00e9al, for his assis- tance in the statistical analysis. We thank the American Academy of Ophthalmology for generously granting us permission to use the underlying BCSC Self-Assessment Program materials.  Footnotes and Disclosures  Originally received: February 3, 2023. Final revision: April 21, 2023. Accepted: April 25, 2023. Available online: May 5, 2023. Manuscript number XOPS-D-23-00026R2.  1 Department of Ophthalmology, Universit\u00e9 de Montr\u00e9al, Montr\u00e9al, Quebec, Canada.  2 Centre Universitaire d\u2019Ophtalmologie (CUO), H\u00f4pital Maisonneuve- Rosemont, CIUSSS de l\u2019Est-de-l\u2019\u00cele-de-Montr\u00e9al, Montr\u00e9al, Quebec, Canada.  3 Department of Ophthalmology, Centre Hospitalier de l\u2019Universit\u00e9 de Montr\u00e9al (CHUM), Montr\u00e9al, Quebec, Canada.  4 The CHUM School of Arti\ufb01cial Intelligence in Healthcare (SAIH), Centre Hospitalier de l\u2019Universit\u00e9 de Montr\u00e9al (CHUM), Montr\u00e9al, Quebec, Canada.  Disclosure(s):  All authors have completed and submitted the ICMJE disclosures form.  Ophthalmology Science Volume 3, Number 4, December 2023  6",
      "page_number": 6
    },
    {
      "text": "The authors have made the following disclosures: F.A: Grants e Bayer.  S.T: Grants e Bayer.  The other authors have no proprietary or commercial interest in any ma- terials discussed in this article.  HUMAN SUBJECTS: No human subjects were included in this study.  No animal subjects were used in this study.  Author Contributions:  Conception and design: Antaki, Touma, Milad, El-Khoury, Duval  Analysis and interpretation: Antaki, Duval  Data collection: Antaki, Touma, Milad, El-Khoury  Obtained funding: N/A Overall responsibility: Antaki, Touma, Milad, El-Khoury, Duval  Abbreviations and Acronyms: AI \u00bc arti\ufb01cial intelligence; BCSC \u00bc Basic and Clinical Science Course; CI \u00bc con\ufb01dence interval; DL \u00bc deep learning; GPT \u00bc Generative Pre- trained Transformer; LLM \u00bc large language model; LR \u00bc likelihood ratio; OKAP \u00bc Ophthalmic Knowledge Assessment Program; USMLE \u00bc United States Medical Licensing Examination.  Keywords: Arti\ufb01cial intelligence, ChatGPT, Generative Pretrained Transformer, Medical education, Ophthalmology.  Correspondence: Renaud Duval, MD, Centre Universitaire d\u2019Ophtalmologie (CUO), H\u00f4pital Maisonneuve-Rosemont, 5415 Boulevard de l\u2019Assomption, Montr\u00e9al, Qu\u00e9bec, Canada, H1T 2M4. E-mail: renaud[dot]duval@gmail[dot]com.  References  1. Ting DSW, Pasquale LR, Peng L, et al Arti\ufb01cial intelligence and deep learning in ophthalmology. Br J Ophthalmol. 2019;103:167e175. 2. Schmidt-Erfurth U, Sadeghipour A, Gerendas BS, et al Arti- \ufb01cial intelligence in retina. Prog Retin Eye Res. 2018;67:1e29. 3. Antaki F, Coussa RG, Kahwati G, et al Accuracy of auto- mated machine learning in classifying retinal pathologies from ultra-wide\ufb01eld pseudocolour fundus images. Br J Ophthalmol. 2023;107:90e95. 4. Nath S, Marie A, Ellershaw S, et al New meaning for NLP: the trials and tribulations of natural language processing with GPT- 3 in ophthalmology. Br J Ophthalmol. 2022;106:889e892. 5. Topol E. When M.D. is a Machine Doctor. Available at:  https://erictopol[dot]substack[dot]com/p/when-md-is-a-machine-doc- tor. Accessed January 20, 2023. 6. Bommasani R, Hudson DA, Adeli E, et al On the opportu- nities and risks of foundation models. arXiv preprint arXiv: 210807258. 2021. 7. Wiggins WF, Tejani AS. On the opportunities and risks of foundation models for natural language processing in radi- ology. Radiol Artif Intell. 2022;4:e220119. 8. Brown T, Mann B, Ryder N, et al Language models are few-shot learners. Adv Neural Inf Process Syst. 2020;33:1877e1901. 9. Singhal K, Azizi S, Tu T, et al Large language models encode clinical knowledge. arXiv preprint arXiv:221213138. 2022. 10. Li\u00e9vin V, Hother CE, Winther O. Can large language models reason about medical questions? arXiv preprint arXiv: 220708143. 2022. 11. Kung TH, Cheatham M, Medinilla A, et al Performance of ChatGPT on USMLE: potential for AI-assisted medical edu- cation using large language models. PLOS Digit Health. 2023;2:e0000198. 12. Zafar S, Wang X, Srikumaran D, et al Resident and program characteristics that impact performance on the Ophthalmic Knowledge Assessment Program (OKAP). BMC Med Educ. 2019;19:190. 13. Lee AG, Oetting TA, Blomquist PH, et al A multicenter analysis of the ophthalmic knowledge assessment program and American Board of Ophthalmology written qualifying exam- ination performance. Ophthalmology. 2012;119:1949e1953.  14. OpenAI. ChatGPT: Optimizing Language Models for Dia- logue. Available at: https://openai[dot]com/blog/chatgpt/[dot] Pub- lished 2022. Accessed January 20, 2023. 15. Ouyang L, Wu J, Jiang X, et al Training language models to follow instructions with human feedback. arXiv preprint arXiv:220302155. 2022. 16. OpenAI, Introducing ChatGPT Plus. Available at: https:// openai.com/blog/chatgpt-plus. Accessed March 2, 2023. 17. Taib F, Yusoff MSB. Dif\ufb01culty index, discrimination index, sensitivity and speci\ufb01city of long case and multiple choice questions to predict medical students\u2019 examination perfor- mance. J Taibah Univ Med Sci. 2014;9:110e114. 18. Americal Academy of Ophthalmology, OKAP Exam. Avail- able at: https://www[dot]aao[dot]org/okap-exam[dot] Published 2022. Accessed January 21, 2023. 19. Hingorjo MR, Jaleel F. Analysis of one-best MCQs: the dif- \ufb01culty index, discrimination index and distractor ef\ufb01ciency. J Pak Med Assoc. 2012;62:142e147. 20. Fleiss JL. Measuring nominal scale agreement among many raters. Psychol Bull. 1971;76:378e382. 21. Landis JR, Koch GG. The measurement of observer agreement for categorical data. Biometrics. 1977;33:159e174. 22. Korngiebel DM, Mooney SD. Considering the possibil- ities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery. NPJ Digit Med. 2021;4:93. 23. Stunkel L, Mackay DD, Bruce BB, et al Referral patterns in neuro-ophthalmology. J Neuroophthalmol. 2020;40: 485e493. 24. Law C, Krema H, Simpson ER. Referral patterns of intraocular tumour patients to a dedicated Canadian ocular oncology department. Can J Ophthalmol. 2012;47:254e261. 25. Radford A, Kim JW, Hallacy C, et al Learning Transferable Visual Models From Natural Language Supervision. Pro- ceedings of the 38th International Conference on Machine Learning; 2021; Proceedings of Machine Learning Research. arxiv preprint. https://doi[dot]org/10[dot]48550/arXiv[dot]2103[dot]00020[dot] 26. Wang Z, Wu Z, Agarwal D, Sun J. Medclip: contrastive learning from unpaired medical images and text. arXiv pre- print arXiv:221010163. 2022.  Antaki et al \u0004 Performance of ChatGPT in Ophthalmology  7",
      "page_number": 7
    }
  ]
}